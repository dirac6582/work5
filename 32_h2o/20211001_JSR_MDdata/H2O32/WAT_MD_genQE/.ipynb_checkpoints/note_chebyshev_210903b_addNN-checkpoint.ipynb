{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r -f input.acpype\n",
    "!ls ./ |  grep -v -E 'RFmodels|inputs|note_chebyshev_210903b_addNN.ipynb' | xargs rm -rf \n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "poly = pd.read_csv(\"./inputs/propanediol.csv\")\n",
    "#poly = poly[1:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OCCCO</td>\n",
       "      <td>PRD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Smiles Name\n",
       "0  OCCCO  PRD"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_atoms=26\n",
    "density = 0.005\n",
    "!. /home/yamazaki/usr/local/gromacs/bin/GMXRC\n",
    "gromacs_home = \"/home/yamazaki/usr/local/gromacs/bin/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/rdkit/Chem/PandasTools.py\", line 130, in <module>\n",
      "    if 'display.width' in pd.core.config._registered_options:\n",
      "AttributeError: module 'pandas.core' has no attribute 'config'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from rdkit.Chem import PandasTools\n",
    "\n",
    "#化学構造のsmilesリストを準備する\n",
    "smiles_list_polymer = poly[\"Smiles\"].to_list()\n",
    "\n",
    "\n",
    "# 化合物のラベルを作成\n",
    "label_list = poly[\"Name\"].to_list()\n",
    "\n",
    "# molオブジェクトのリストを作成\n",
    "mols_list_polymer = [Chem.MolFromSmiles(smile) for smile in smiles_list_polymer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1200x200 at 0x7F4732316DA0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Draw.MolsToGridImage(mols_list_polymer,\n",
    "                           molsPerRow=4, #一列に配置する分子の数\n",
    "                           subImgSize=(300,200),\n",
    "                           legends=label_list #化合物の下に表示するラベル\n",
    "                           )\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaff_bcc(smiles):\n",
    "    #GAFF2/AM1-BCCをアサインする\n",
    "    import sys,os,os.path\n",
    "    os.environ['SMILES']=str(smiles)\n",
    "    !echo ${SMILES} > input.smi\n",
    "    !obabel -ismi input.smi -O input.mol2 --gen3D --conformer --nconf 5000 --weighted \n",
    "    !babel -imol2 input.mol2 -oxyz input.xyz\n",
    "    from ase.io import read, write\n",
    "    inp1 = read('input.xyz')\n",
    "    !acpype -i input.mol2 -c bcc -n 0 -m 1 -a gaff2 -f -o gmx -k \"qm_theory='AM1',grms_tol=0.05,scfconv=1.d-10,ndiis_attempts=700, \"\n",
    "\n",
    "    import shutil\n",
    "    src = './input.acpype/input_GMX.gro'\n",
    "    copy = './input1.gro'\n",
    "    shutil.copyfile(src,copy)\n",
    "    src = './input.acpype/input_GMX.itp'\n",
    "    copy = './input1.itp'\n",
    "    shutil.copyfile(src,copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 molecule converted\n",
      "1 molecule converted\n",
      "13 info messages 6 audit log messages \n",
      "=========================================================================================\n",
      "| ACPYPE: AnteChamber PYthon Parser interfacE v. 2020-07-25T09:06:13CEST (c) 2021 AWSdS |\n",
      "=========================================================================================\n",
      "==> ... charge set to 0\n",
      "==> Executing Antechamber...\n",
      "==> * Antechamber OK *\n",
      "==> * Parmchk OK *\n",
      "==> Executing Tleap...\n",
      "++++++++++start_quote+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Checking 'UNL'....\n",
      "Checking parameters for unit 'UNL'.\n",
      "Checking for bond parameters.\n",
      "Checking for angle parameters.\n",
      "Unit is OK.\n",
      "++++++++++end_quote+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "==> * Tleap OK *\n",
      "==> Removing temporary files...\n",
      "==> Writing GROMACS files\n",
      "\n",
      "==> Writing GMX dihedrals for GMX 4.5 and higher.\n",
      "\n",
      "==> Overwriting pickle file input.pkl\n",
      "Total time of execution: less than a second\n"
     ]
    }
   ],
   "source": [
    "gaff_bcc(smiles_list_polymer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make mdp file for energy minimization\n",
    "def make_mdp_em():\n",
    "\n",
    "    mdp_file = \"em.mdp\"\n",
    "\n",
    "    lines = [\n",
    "    \"; VARIOUS PREPROCESSING OPTIONS\",\n",
    "    \";title                    = Yo\",\n",
    "    \";cpp                      = /usr/bin/cpp\",\n",
    "    \"include                  =\", \n",
    "    \"define                   =\", \n",
    "    \"    \",\n",
    "    \"; RUN CONTROL PARAMETERS\",\n",
    "    \"integrator               = steep\",\n",
    "    \"nsteps                   = 1000000\",\n",
    "    \"emtol                    = 10\",\n",
    "    \"emstep                   = 0.1\",\n",
    "    \"nstlist                  = 1\",\n",
    "    \"cutoff-scheme            = verlet\",\n",
    "    \"vdw-type                 = cut-off\",\n",
    "    \"rlist                    = 0.5\",\n",
    "    \"rvdw                     = 0.5\",\n",
    "    \"rcoulomb                 = 0.5\",\n",
    "    ]\n",
    "\n",
    "    with open(mdp_file, mode='w') as f:\n",
    "        f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make mdp file for NVT run\n",
    "def make_mdp_nvt(temp,steps,dt,cutoff):\n",
    "\n",
    "    temperature      = temp\n",
    "    simulation_steps = steps \n",
    "    time_step        = dt/1000.0  # ps\n",
    "    cutoff_radius    = cutoff/10.0\n",
    "    \n",
    "    mdp_file = \"run.mdp\"\n",
    "\n",
    "    lines = [\n",
    "    \"; VARIOUS PREPROCESSING OPTIONS\",\n",
    "    \";title                    = Yo\",\n",
    "    \";cpp                      = /usr/bin/cpp\",\n",
    "    \"include                  =\", \n",
    "    \"define                   =\", \n",
    "    \"    \",\n",
    "    \"; RUN CONTROL PARAMETERS\",\n",
    "    \"constraints              = none\",\n",
    "    \"integrator               = md\",\n",
    "    \"nsteps                   = {}\".format(simulation_steps),\n",
    "    \"dt                       = {}\".format(time_step),\n",
    "    \"nstlist                  = 1\",\n",
    "    \"rlist                    = {}\".format(cutoff_radius),\n",
    "    \"rvdw                     = {}\".format(cutoff_radius),\n",
    "    \"rcoulomb                 = {}\".format(cutoff_radius),\n",
    "    \"coulombtype              = cut-off\",\n",
    "    \"cutoff-scheme            = verlet\",\n",
    "    \"vdw-type                 = cut-off\",        \n",
    "    \"tc-grps                  = system\",\n",
    "    \"tau-t                    = 0.1\",\n",
    "    \"gen-vel                  = yes\",\n",
    "    \"gen-temp                 = {}\".format(temperature),\n",
    "    \"ref-t                    = {}\".format(temperature),\n",
    "    \"Pcoupl                   = no\",\n",
    "    \"nstenergy                = 5000\",\n",
    "    \"nstxout                  = 5000\",  \n",
    "    ]\n",
    "\n",
    "    with open(mdp_file, mode='w') as f:\n",
    "        f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_cell_gromacs(dt,eq_cutoff,eq_temp,eq_steps):\n",
    "\n",
    "    import pandas as pd\n",
    "    \n",
    "    import time \n",
    "    init_time = time.time()\n",
    "    \n",
    "    dt = dt\n",
    "\n",
    "    import MDAnalysis as mda\n",
    "    from nglview.datafiles import PDB, XTC\n",
    "\n",
    "    #混合溶液を作成\n",
    "    import mdapackmol\n",
    "    import numpy as np\n",
    "    from ase import units\n",
    "    import shutil\n",
    "\n",
    "    # load individual molecule files\n",
    "    mol1 = mda.Universe('input1.gro')\n",
    "    #num_mols1 = 30\n",
    "    total_mol = int(max_atoms/(mol1.atoms.n_atoms))\n",
    "    num_mols1 = total_mol\n",
    "    mw_mol1 = np.sum(mol1.atoms.masses)\n",
    "    total_weight = num_mols1 * mw_mol1 \n",
    "    \n",
    "    # Determine side length of a box with the density of mixture \n",
    "    #L = 12.0 # Ang. unit \n",
    "    d = density / 1e24 # Density in g/Ang3 \n",
    "    volume = (total_weight / units.mol) / d\n",
    "    L = volume**(1.0/3.0)\n",
    "    \n",
    "    system = mdapackmol.packmol(\n",
    "    [ mdapackmol.PackmolStructure(\n",
    "    mol1, number=num_mols1,\n",
    "    instructions=[\"inside box \"+str(0)+\"  \"+str(0)+\"  \"+str(0)+ \"  \"+str(L)+\"  \"+str(L)+\"  \"+str(L)]),])\n",
    "\n",
    "    system.atoms.write('mixture.gro')\n",
    "\n",
    "    import os \n",
    "    os.environ['GMX_MAXBACKUP'] = '-1'\n",
    "\n",
    "    # for gromacs-5 or later \n",
    "    commands = gromacs_home+\"gmx_mpi editconf -f mixture.gro  -box \"+ str(L/10.0)+\"  \"+str(L/10.0)+\"  \"+str(L/10.0) + \"  \" +\" -o init.gro\"\n",
    "\n",
    "    import subprocess\n",
    "    from subprocess import PIPE\n",
    "\n",
    "    proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "    output = proc.stdout\n",
    "    #print('STDOUT: {}'.format(output))\n",
    "\n",
    "    #make top file for GAFF\n",
    "\n",
    "    top_file = \"system.top\"\n",
    "    mol_name1 = \"input\"\n",
    " \n",
    "    lines = [\n",
    "        \"; input_GMX.top created by acpype (v: 2020-07-25T09:06:13CEST) on Fri Jul 31 07:59:08 2020\",\n",
    "        \";by acpype (v: 2020-07-25T09:06:13CEST) on Fri Jul 31 07:59:08 2020\",\n",
    "        \"   \",\n",
    "        \"[ defaults ]\",\n",
    "        \"; nbfunc        comb-rule       gen-pairs       fudgeLJ fudgeQQ\",\n",
    "        \"1               2               yes             0.5     0.8333\",\n",
    "        \"    \",\n",
    "        \"; Include input.itp topology\", \n",
    "        \"#include \\\"{}.itp\\\"\".format(\"input1\"),\n",
    "        \"    \",\n",
    "        \"[ system ]\",\n",
    "        \"input\",\n",
    "        \"     \",\n",
    "        \"[ molecules ]\",\n",
    "        \"; Compound        nmols\" ,\n",
    "        mol_name1 + \"          {} \".format(num_mols1), \n",
    "    ]\n",
    "        \n",
    "    with open(top_file, mode='w') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "\n",
    "    # Energy minimization\n",
    "    import os\n",
    "    import subprocess\n",
    "    from subprocess import PIPE\n",
    "    \n",
    "    print('Minimizing energy')\n",
    "    \n",
    "    os.environ['GMX_MAXBACKUP'] = '-1'\n",
    "\n",
    "    make_mdp_em()\n",
    "\n",
    "    #grompp\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'    \n",
    "    commands = gromacs_home+\"gmx_mpi grompp -f em.mdp -p system.top -c init.gro -o em.tpr -maxwarn 10 \"\n",
    "    proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "    output = proc.stdout\n",
    "\n",
    "    #mdrun\n",
    "    os.environ['OMP_NUM_THREADS'] = '1' \n",
    "    commands = gromacs_home+\"gmx_mpi mdrun -s em.tpr -o em.trr -e em.edr -c em.gro -nb cpu\"\n",
    "    proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "    output = proc.stdout\n",
    "\n",
    "    #Relax the geometry \n",
    "    print('Running dynamics :Equilibration')\n",
    "  \n",
    "    temp = eq_temp\n",
    "    dt   = dt \n",
    "    steps = eq_steps\n",
    "    make_mdp_nvt(temp,steps,dt,eq_cutoff)\n",
    "\n",
    "    #grompp\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'    \n",
    "    commands = gromacs_home+\"gmx_mpi grompp -f run.mdp -p system.top -c em.gro -o eq.tpr -maxwarn 10 \".format(str(temp))\n",
    "    proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "    output = proc.stdout\n",
    "\n",
    "    #mdrun\n",
    "    os.environ['OMP_NUM_THREADS'] = '1' \n",
    "    commands = gromacs_home+\"gmx_mpi mdrun -s eq.tpr -o eq.trr -e eq.edr -c eq.gro -nb cpu\"\n",
    "    proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "    output = proc.stdout\n",
    "\n",
    "    print(\"elapsed time= {} sec.\".format(time.time()-init_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.5                              #[fs] MDの刻み時間：このまま使うことを推奨。\n",
    "eq_temp = 3000                       #緩和計算させるときの温度 [K]\n",
    "eq_steps = 5000000                  #緩和計算するstep数。この例だと100ps\n",
    "eq_cutoff = 9.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/MDAnalysis/coordinates/PDB.py:1028: UserWarning: Found no information for attr: 'altLocs' Using default value of ' '\n",
      "  \"\".format(attrname, default))\n",
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/MDAnalysis/coordinates/PDB.py:1028: UserWarning: Found no information for attr: 'icodes' Using default value of ' '\n",
      "  \"\".format(attrname, default))\n",
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/MDAnalysis/coordinates/PDB.py:1028: UserWarning: Found no information for attr: 'occupancies' Using default value of '1.0'\n",
      "  \"\".format(attrname, default))\n",
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/MDAnalysis/coordinates/PDB.py:1028: UserWarning: Found no information for attr: 'tempfactors' Using default value of '0.0'\n",
      "  \"\".format(attrname, default))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimizing energy\n",
      "Running dynamics :Equilibration\n",
      "elapsed time= 50.67850399017334 sec.\n"
     ]
    }
   ],
   "source": [
    "build_initial_cell_gromacs(dt,eq_cutoff,eq_temp,eq_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#構造可視化(matplotlib版)\n",
    "import matplotlib.pyplot as plt\n",
    "from ase.visualize.plot import plot_atoms\n",
    "import ase.io\n",
    "mol1 = ase.io.read('eq.gro')\n",
    "%matplotlib inline\n",
    "plot_atoms(mol1, rotation=('0x,0y,0z'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f661b7928fc4e05b221e3a400f518cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#構造可視化(nglview版)\n",
    "import nglview as nv\n",
    "import ase.io\n",
    "mol1 = ase.io.read('eq.gro')\n",
    "w = nv.show_ase(mol1)\n",
    "w.add_label(radius=1,color=\"black\",label_type=\"atom\")\n",
    "w.add_unitcell()\n",
    "w.update_unitcell()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ed674dfa8b4a4188f4b9dc879aec11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget(max_frame=1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224c4baa179a4a5589047d761114f529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Box(children=(Box(children=(Box(children=(Label(value='step'), IntSlider(value=1, min=-100)), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Trajectoryの表示\n",
    "import sys\n",
    "import mdtraj\n",
    "\n",
    "# ParmEd Imports\n",
    "from parmed import load_file\n",
    "from parmed.openmm.reporters import NetCDFReporter\n",
    "from parmed import unit as u\n",
    "\n",
    "#analysis\n",
    "import os  \n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "\n",
    "!echo \"System\" > ./inputs/anal.txt \n",
    "\n",
    "!. /home/yamazaki/usr/local/gromacs/bin/GMXRC\n",
    "gromacs_home = \"/home/yamazaki/usr/local/gromacs/bin/\"\n",
    "commands = gromacs_home+\"gmx_mpi trjconv -s eq.tpr -f eq.trr -dump 0 -o eq.pdb < ./inputs/anal.txt\"\n",
    "proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "output = proc.stdout\n",
    "commands = gromacs_home+\"gmx_mpi trjconv -s eq.tpr -f eq.trr -pbc mol -o eq_pbc.trr < ./inputs/anal.txt\"\n",
    "proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "output = proc.stdout\n",
    "\n",
    "traj=mdtraj.load(\"eq_pbc.trr\", top=\"eq.pdb\")\n",
    "view=nv.show_mdtraj(traj,gui=True)\n",
    "view.parameters =dict(\n",
    "                        camera_type=\"orthographic\",\n",
    "                        backgraound_color=\"black\",\n",
    "                        clip_dist=0\n",
    ")\n",
    "#view.clear_representations()\n",
    "#view.add_representation(\"spacefill\",selection=[i for i in range(0,n_atoms)])\n",
    "#view.add_representation(\"spacefill\",selection=[i for i in range(n_atoms,n_total_atoms)],opacity=0.1)\n",
    "view.add_unitcell()\n",
    "view.update_unitcell()\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#トラジェクトリーのフレーム数\n",
    "len(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chebyshev記述子の作成\n",
    "#Cutoff関数の定義\n",
    "import numpy as np\n",
    "def fc(r,Rc) :\n",
    "    return 0.5*(np.cos(r*np.pi/Rc)+1.0)\n",
    "\n",
    "#パラメータの設定\n",
    "Rcut = 6 #[ang. unit] 原子\n",
    "N2 = 40  #記述子の個数_原子間距離の分布\n",
    "N3 = 40  #記述子の個数_角度分布\n",
    "\n",
    "def make_atom_list(atoms):\n",
    "    mol1 = atoms\n",
    "    #原子種依存の重みの計算(C,H,Oの3原子種を仮定)\n",
    "    ats = mol1.get_atomic_numbers()\n",
    "    Hatoms=np.where(ats==1,0,ats)\n",
    "    HCatoms=np.where(Hatoms==6,-1,Hatoms)\n",
    "    w=np.where(HCatoms==8,1,HCatoms)\n",
    "    return w\n",
    "\n",
    "def calc_dist(atoms) :\n",
    "    #UnitCell内の原子間距離の計算(mic:True to use the Minimum Image Convention)\n",
    "    mol1=atoms\n",
    "    dist = mol1.get_all_distances(mic=True)\n",
    "    return dist\n",
    "    \n",
    "def Chebyshev(X):\n",
    "\n",
    "    at_id,Rcut,N2,N3,wt,dist,mol1 = X \n",
    "    #変数の代入\n",
    "    atom_ind = at_id #int \n",
    "    N2       = N2    #int \n",
    "    N3       = N3    #int\n",
    "    w        = wt    #array \n",
    "    dist     = dist  #array  \n",
    "    mol1     = mol1  #ASE atoms object \n",
    "    Rcut = Rcut      #float \n",
    "    \n",
    "    #ある原子iを中心としたCutoff半径内の原子indexのリスト作成\n",
    "    dist_i = dist[atom_ind]\n",
    "    neighbors_list = np.argwhere(dist_i<=Rcut).reshape(-1)\n",
    "    neighbors_list = neighbors_list[neighbors_list !=atom_ind]\n",
    "\n",
    "    #Cutoff半径内の任意の原子と原子iとの距離のリスト作成\n",
    "    distances = dist_i[neighbors_list]\n",
    "\n",
    "    #Cutoff半径内の任意の原子j,kと原子iの作る角度のリスト作成\n",
    "    i = atom_ind \n",
    "    indices = [(j,i,k) for j in neighbors_list if j!=i for k in neighbors_list if (k!=i) and (k>j)]\n",
    "    angles = mol1.get_angles(indices,mic=True)\n",
    "\n",
    "    #Chebyshev記述子を作る（原子間距離σ2について）\n",
    "    r = distances \n",
    "    weight_RDF = fc(distances,Rcut)\n",
    "    #Chebyshev関数\n",
    "    funcT = []\n",
    "    x = 2.0*r/Rcut - 1.0 \n",
    "    for n2 in range(N2):\n",
    "        if n2==0 :\n",
    "            funcT.append(np.ones(x.shape))\n",
    "        if n2==1 :\n",
    "            funcT.append(x) \n",
    "        if n2>1 :\n",
    "            funcT.append(2.0*x*funcT[n2-1]-funcT[n2-2])\n",
    "\n",
    "    coef2  =[]\n",
    "    coef2w =[]\n",
    "    for n2 in range(N2):\n",
    "        if n2==0 :\n",
    "            k=0.5 \n",
    "        else :\n",
    "            k=1.0 \n",
    "        #phi = k*funcT[n2]/(2.0*np.pi*np.sqrt(r/Rcut-r*r/(Rcut*Rcut)))\n",
    "        phi = funcT[n2]\n",
    "        c2 = np.sum(phi*weight_RDF)\n",
    "        c2w= np.sum(phi*weight_RDF*w[neighbors_list])\n",
    "        coef2.append(c2)\n",
    "        coef2w.append(c2w)\n",
    "    coef2 = np.array(coef2)\n",
    "    coef2w= np.array(coef2w)\n",
    "\n",
    "    #Chebyshev記述子を作る（角度分布σ3について）\n",
    "    th_rad = np.pi*angles/180.0\n",
    "    weight_ADFj = fc(dist_i[np.array(indices)[:,0]],Rcut)\n",
    "    weight_ADFk = fc(dist_i[np.array(indices)[:,2]],Rcut)        \n",
    "    #Chebyshev関数\n",
    "    funcT = []\n",
    "    x = 2.0*th_rad/np.pi - 1.0 \n",
    "    for n3 in range(N3):\n",
    "        if n3==0 :\n",
    "            funcT.append(np.ones(x.shape))\n",
    "        if n3==1 :\n",
    "            funcT.append(x) \n",
    "        if n3>1 :\n",
    "            funcT.append(2.0*x*funcT[n3-1]-funcT[n3-2])\n",
    "\n",
    "    coef3  =[]\n",
    "    coef3w =[]\n",
    "    for n3 in range(N3):\n",
    "        if n3==0 :\n",
    "            k=0.5 \n",
    "        else :\n",
    "            k=1.0 \n",
    "        #phi = k*funcT[n3]/(2.0*np.pi*np.sqrt(th_rad/np.pi-th_rad*th_rad/(np.pi*np.pi)))\n",
    "        phi = funcT[n3]\n",
    "        c3 = np.sum(phi*weight_ADFj*weight_ADFk)\n",
    "        c3w= np.sum(phi*weight_ADFj*weight_ADFk*w[np.array(indices)[:,0]]*w[np.array(indices)[:,2]])\n",
    "        coef3.append(c3)\n",
    "        coef3w.append(c3w)\n",
    "    coef3 = np.array(coef3)\n",
    "    coef3w= np.array(coef3w)\n",
    "    #良くわからないが一応正規化する\n",
    "    #coef2 = coef2 / np.linalg.norm(coef2)\n",
    "    #coef2w = coef2w / np.linalg.norm(coef2w)\n",
    "    #coef3 = coef3 / np.linalg.norm(coef3)\n",
    "    #coef3w = coef3w / np.linalg.norm(coef3w)\n",
    "    \n",
    "    return [coef2,coef2w,coef3,coef3w]\n",
    "\n",
    "#Chebyshev記述子\n",
    "#atom_ind=1\n",
    "#w = make_atom_list(atoms1)\n",
    "#dist = calc_dist(atoms1)\n",
    "#c2,c2w,c3,c3w = Chebyshev(atom_ind,Rcut,N2,N3,w,dist,atoms1)\n",
    "#desc = np.array([c2,c2w,c3,c3w]).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [01:04<00:00, 15.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#MDのTrajectoryから構造抽出し、Chebyshevで記述子化する\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "nfrs  = len(traj)\n",
    "descs_X = []\n",
    "\n",
    "#空のpandas DataFrameを作る\n",
    "import pandas as pd\n",
    "#cols = [\"atomic_number\", \"Features\"]\n",
    "#df = pd.DataFrame(columns=cols)\n",
    "\n",
    "#原子種のリスト作成\n",
    "ats = mol1.get_atomic_numbers()\n",
    "#含まれている原子種の数\n",
    "n_species = np.unique(ats).size\n",
    "ats_list,counts = np.unique(ats, return_counts=True)\n",
    "\n",
    "for nfr in tqdm(range(nfrs)):\n",
    "    traj[nfr].save_gro(\"temp.gro\")\n",
    "    atoms1 = ase.io.read('temp.gro')\n",
    "    w = make_atom_list(atoms1)\n",
    "    dist = calc_dist(atoms1)\n",
    "    natoms = len(atoms1)\n",
    "    descs = []\n",
    "    parms = [[i,Rcut,N2,N3,w,dist,atoms1] for i in range(natoms)]\n",
    "    result = joblib.Parallel(n_jobs=6)(joblib.delayed(Chebyshev)(par) for par in parms)\n",
    "    \n",
    "    for i in range(natoms):\n",
    "        c2,c2w,c3,c3w = result[i]\n",
    "        #c2,c2w,c3,c3w = Chebyshev(i,Rcut,N2,N3,w,dist,atoms1)\n",
    "        desc = np.array([c2,c2w,c3,c3w]).reshape(-1)\n",
    "        #desc = np.array([c2,c2w]).reshape(-1)\n",
    "        descs.append(desc)\n",
    "    descs=np.array(descs).reshape(natoms,-1)\n",
    "    df=pd.DataFrame(data=descs)\n",
    "    #Descsを原子種ごとに並び替える\n",
    "    df[\"atomic_number\"]=ats\n",
    "    #df[\"Features\"]=descs\n",
    "    sorted_df=df.sort_values(\"atomic_number\")\n",
    "    sorted_df=sorted_df.drop(\"atomic_number\",axis=1)\n",
    "    sorted_features = sorted_df.to_numpy()\n",
    "    descs_X.append(sorted_features)\n",
    "descs_X = np.array(descs_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 原子種ごとの原子数\n",
    "nH = counts[0] # H原子の数\n",
    "nC = counts[1] # C原子の数\n",
    "nO = counts[2] # O原子の数\n",
    "\n",
    "#入力数\n",
    "nfeatures = len(descs_X[0,0,:])\n",
    "\n",
    "indx = 0 \n",
    "descs = descs_X[indx]\n",
    "\n",
    "featuresH = descs[:nH]\n",
    "featuresC = descs[nH:nH+nC]\n",
    "featuresO = descs[nH+nC:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gromacsのenergyコマンドにより、各構造での全エネルギーを算出して教師データとする\n",
    "!echo \"Potential\" > ./inputs/anal.txt \n",
    "gromacs_home = \"/home/yamazaki/usr/local/gromacs/bin/\"\n",
    "commands = gromacs_home+\"gmx_mpi energy -f eq.edr -o energy.xvg  < ./inputs/anal.txt\"\n",
    "proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "output = proc.stdout\n",
    "\n",
    "Traj_time=[]\n",
    "Traj_pot=[]\n",
    "f = open('energy.xvg', 'r')\n",
    "while True:\n",
    "  data = f.readline()\n",
    "  if data == '':\n",
    "    break\n",
    "  if (\"#\" in data) or (\"@\" in data) :\n",
    "    pass \n",
    "  else :\n",
    "        #print (data.rstrip('\\n'))\n",
    "        Traj_time.append(float(data.split()[0]))\n",
    "        Traj_pot.append(float(data.split()[1]))\n",
    "Traj_time = np.array(Traj_time)    \n",
    "Traj_pot = np.array(Traj_pot)\n",
    "\n",
    "true_y = Traj_pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yのデータセットを標準化する\n",
    "#Xの方はベクトルのノルムを１に規格化しているが、ポテンシャルエネルギーは巨大になりがちなので標準化を施す\n",
    "import copy \n",
    "true_y_org = copy.deepcopy(true_y)\n",
    "train_y_mean = np.mean(true_y)\n",
    "train_y_std  = np.std(true_y)\n",
    "true_y = (true_y - train_y_mean)/train_y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descs_X = descs_X.detach().numpy()\n",
    "#true_y = true_y.detach().numpy()\n",
    "#descs_X = copy.deepcopy(descs_X_org)\n",
    "#true_y = copy.deepcopy(true_y_org)\n",
    "#train_y_mean = np.mean(true_y)\n",
    "#train_y_std  = np.std(true_y)\n",
    "#true_y = (true_y - train_y_mean)/train_y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy Arrayの記述子をTorchのTensor型に変換する\n",
    "import torch \n",
    "descs_X_org = copy.deepcopy(descs_X)\n",
    "descs_X = torch.from_numpy(descs_X.astype(np.float32)).clone()\n",
    "true_y = torch.from_numpy(true_y.astype(np.float32)).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X,y:901 901 / test_X,y:100 100 \n"
     ]
    }
   ],
   "source": [
    "#データ分割（訓練用とテスト用に分ける）\n",
    "test_size = 100 \n",
    "test_X = descs_X[-test_size:]\n",
    "test_y = true_y[-test_size:]\n",
    "descs_X = descs_X[:-test_size]\n",
    "true_y = true_y[:-test_size]\n",
    "print(\"train_X,y:{0} {1} / test_X,y:{2} {3} \".format(len(descs_X),len(true_y),len(test_X),len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer1H): Linear(in_features=160, out_features=10, bias=True)\n",
       "  (layer2H): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (layer_outH): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (layer1C): Linear(in_features=160, out_features=10, bias=True)\n",
       "  (layer2C): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (layer_outC): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (layer1O): Linear(in_features=160, out_features=10, bias=True)\n",
       "  (layer2O): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (layer_outO): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
    "\n",
    "# 定数（モデル定義時に必要となるもの）\n",
    "INPUT_FEATURES = nfeatures    # 入力（特徴）の数： 記述子の数\n",
    "LAYER1_NEURONS = 10      # ニューロンの数： 10\n",
    "LAYER2_NEURONS = 10      # ニューロンの数： 10\n",
    "OUTPUT_RESULTS = 1      # 出力結果の数： 1\n",
    "\n",
    "# 原子種ごとの原子数\n",
    "nH = counts[0] # H原子の数\n",
    "nC = counts[1] # C原子の数\n",
    "nO = counts[2] # O原子の数\n",
    "\n",
    "# 変数（モデル定義時に必要となるもの）\n",
    "activation1 = torch.nn.Tanh()  # 活性化関数（隠れ層用）： ReLU関数（変更可能）\n",
    "activation2 = torch.nn.Tanh()  # 活性化関数（隠れ層用）： ReLU関数（変更可能）\n",
    "#acti_out = torch.nn.Tanh()     # 活性化関数（出力層用）： tanh関数（固定）\n",
    "\n",
    "# torch.nn.Moduleによるモデルの定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 隠れ層：1つ目のレイヤー（layer）\n",
    "        self.layer1H = nn.Linear(\n",
    "            INPUT_FEATURES,                # 入力ユニット数（＝入力層）\n",
    "            LAYER1_NEURONS)                # 次のレイヤーの出力ユニット数\n",
    "        \n",
    "        # バッチ規格化層\n",
    "        self.bn1H = nn.BatchNorm1d(LAYER1_NEURONS) #バッチ正規化\n",
    "\n",
    "        # 隠れ層：2つ目のレイヤー（layer）\n",
    "        self.layer2H = nn.Linear(\n",
    "            LAYER1_NEURONS,                # 入力ユニット数\n",
    "            LAYER2_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
    "\n",
    "        # 出力層\n",
    "        self.layer_outH = nn.Linear(\n",
    "            LAYER2_NEURONS,                # 入力ユニット数\n",
    "            OUTPUT_RESULTS)                # 出力結果への出力ユニット数\n",
    "        \n",
    "        # 隠れ層：1つ目のレイヤー（layer）\n",
    "        self.layer1C = nn.Linear(\n",
    "            INPUT_FEATURES,                # 入力ユニット数（＝入力層）\n",
    "            LAYER1_NEURONS)                # 次のレイヤーの出力ユニット数\n",
    "        \n",
    "        # バッチ規格化層\n",
    "        self.bn1C = nn.BatchNorm1d(LAYER1_NEURONS) #バッチ正規化\n",
    "\n",
    "        # 隠れ層：2つ目のレイヤー（layer）\n",
    "        self.layer2C = nn.Linear(\n",
    "            LAYER1_NEURONS,                # 入力ユニット数\n",
    "            LAYER2_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
    "\n",
    "        # 出力層\n",
    "        self.layer_outC = nn.Linear(\n",
    "            LAYER2_NEURONS,                # 入力ユニット数\n",
    "            OUTPUT_RESULTS)                # 出力結果への出力ユニット数\n",
    "        \n",
    "        # 隠れ層：1つ目のレイヤー（layer）\n",
    "        self.layer1O = nn.Linear(\n",
    "            INPUT_FEATURES,                # 入力ユニット数（＝入力層）\n",
    "            LAYER1_NEURONS)                # 次のレイヤーの出力ユニット数\n",
    "        \n",
    "        # バッチ規格化層\n",
    "        self.bn1O = nn.BatchNorm1d(LAYER1_NEURONS) #バッチ正規化\n",
    "\n",
    "        # 隠れ層：2つ目のレイヤー（layer）\n",
    "        self.layer2O = nn.Linear(\n",
    "            LAYER1_NEURONS,                # 入力ユニット数\n",
    "            LAYER2_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
    "\n",
    "        # 出力層\n",
    "        self.layer_outO = nn.Linear(\n",
    "            LAYER2_NEURONS,                # 入力ユニット数\n",
    "            OUTPUT_RESULTS)                # 出力結果への出力ユニット数\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # フォワードパスを定義\n",
    "        #featuresH = x[:nH]\n",
    "        #featuresC = x[nH:nH+nC]\n",
    "        #featuresO = x[nH+nC:]\n",
    "        \n",
    "        sumpotH = 0\n",
    "        for i in range(nH) :\n",
    "            xh = x[:,i,:]            \n",
    "            # 「出力＝活性化関数（第n層（入力））」の形式で記述する\n",
    "            xh = activation1(self.layer1H(xh))  # 活性化関数は変数として定義\n",
    "            xh = self.bn1H(xh) #バッチ規格化\n",
    "            xh = activation2(self.layer2H(xh))  \n",
    "            xh = self.layer_outH(xh)  # ※最終層は線形\n",
    "            sumpotH += xh \n",
    "        \n",
    "        sumpotC = 0\n",
    "        for i in range(nC) : \n",
    "            xc = x[:,i+nH,:]\n",
    "            # 「出力＝活性化関数（第n層（入力））」の形式で記述する\n",
    "            xc = activation1(self.layer1C(xc))  # 活性化関数は変数として定義\n",
    "            xc = self.bn1C(xc) #バッチ規格化\n",
    "            xc = activation2(self.layer2C(xc))  \n",
    "            xc = self.layer_outC(xc)  # ※最終層は線形\n",
    "            sumpotC += xc\n",
    "            \n",
    "        sumpotO = 0\n",
    "        for i in range(nO) :\n",
    "            xo = x[:,i+nH+nC,:]\n",
    "            # 「出力＝活性化関数（第n層（入力））」の形式で記述する\n",
    "            xo = activation1(self.layer1O(xo))  # 活性化関数は変数として定義\n",
    "            xo = self.bn1O(xo) #バッチ規格化\n",
    "            xo = activation2(self.layer2O(xo))  \n",
    "            xo = self.layer_outO(xo)  # ※最終層は線形\n",
    "            sumpotO += xo\n",
    "            \n",
    "            z = sumpotH + sumpotC + sumpotO\n",
    "            \n",
    "        return z\n",
    "    \n",
    "# モデル（NeuralNetworkクラス）のインスタンス化\n",
    "model = Net()\n",
    "model   # モデルの内容を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim   # 「最適化」モジュールの別名定義\n",
    "\n",
    "# 定数（学習方法設計時に必要となるもの）\n",
    "LEARNING_RATE = 0.0001   # 学習率： 0.03\n",
    "REGULARIZATION = 0.0  # 正則化率： 0.0\n",
    "\n",
    "# オプティマイザを作成（パラメーターと学習率も指定）\n",
    "optimizer = optim.Adam(           # 最適化アルゴリズムに「Adam」を選択\n",
    "    model.parameters(),          # 最適化で更新対象のパラメーター（重みやバイアス）\n",
    "    lr=LEARNING_RATE,            # 更新時の学習率\n",
    "    weight_decay=REGULARIZATION) # L2正則化（※不要な場合は0か省略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数（学習方法設計時に必要となるもの）\n",
    "criterion = nn.MSELoss()  # 損失関数：平均二乗誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_X, train_y):\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "\n",
    "    # フォワードプロパゲーションで出力結果を取得\n",
    "    #train_X                # 入力データ\n",
    "    pred_y = model(train_X) # 出力結果\n",
    "    #train_y                # 正解ラベル\n",
    "\n",
    "    # 出力結果と正解ラベルから損失を計算し、勾配を求める\n",
    "    optimizer.zero_grad()   # 勾配を0で初期化（※累積してしまうため要注意）\n",
    "    loss = criterion(pred_y, train_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
    "    loss.backward()   # 逆伝播の処理として勾配を計算（自動微分）\n",
    "\n",
    "    # 勾配を使ってパラメーター（重みとバイアス）を更新\n",
    "    optimizer.step()  # 指定されたデータ分の最適化を実施\n",
    "\n",
    "    # 正解数を算出\n",
    "    #with torch.no_grad(): # 勾配は計算しないモードにする\n",
    "    #    discr_y = discretize(pred_y)         # 確率値から「-1」／「1」に変換\n",
    "    #    acc = (discr_y == train_y).sum()     # 正解数を計算する\n",
    "\n",
    "    # 損失と正解数をタプルで返す\n",
    "    #return (loss.item(), acc.item())  # ※item()=Pythonの数値\n",
    "    return (loss.item())  # ※item()=Pythonの数値\n",
    "\n",
    "def valid_step(valid_X, valid_y):\n",
    "    # 評価モードに設定（※dropoutなどの挙動が評価用になる）\n",
    "    model.eval()\n",
    "    \n",
    "    # フォワードプロパゲーションで出力結果を取得\n",
    "    #valid_X                # 入力データ\n",
    "    pred_y = model(valid_X) # 出力結果\n",
    "    #valid_y                # 正解ラベル\n",
    "\n",
    "    # 出力結果と正解ラベルから損失を計算\n",
    "    loss = criterion(pred_y, valid_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
    "    # ※評価時は勾配を計算しない\n",
    "\n",
    "    # 正解数を算出\n",
    "    #with torch.no_grad(): # 勾配は計算しないモードにする\n",
    "    #    discr_y = discretize(pred_y)     # 確率値から「-1」／「1」に変換\n",
    "    #    acc = (discr_y == valid_y).sum() # 正解数を合計する\n",
    "\n",
    "    # 損失と正解数をタプルで返す\n",
    "    #return (loss.item(), acc.item())  # ※item()=Pythonの数値\n",
    "    return (loss.item())  # ※item()=Pythonの数値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ関連のユーティリティクラスをインポート\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "\n",
    "# 定数（学習方法設計時に必要となるもの）\n",
    "BATCH_SIZE = 30  # バッチサイズ： 15（Playgroundの選択肢は「1」～「30」）\n",
    "valid_size = 10\n",
    "\n",
    "X_valid = descs_X[-valid_size:]\n",
    "y_valid = true_y[-valid_size:]\n",
    "X_train = descs_X[:-valid_size]\n",
    "y_train = true_y[:-valid_size]\n",
    "\n",
    "# NumPy多次元配列からテンソルに変換し、データ型はfloatに変換する\n",
    "#t_X_train = torch.from_numpy(X_train).float()\n",
    "#t_y_train = torch.from_numpy(y_train).float()\n",
    "#t_X_valid = torch.from_numpy(X_valid).float()\n",
    "#t_y_valid = torch.from_numpy(y_valid).float()\n",
    "\n",
    "t_X_train = X_train\n",
    "t_y_train = y_train\n",
    "t_X_valid = X_valid\n",
    "t_y_valid = y_valid\n",
    "\n",
    "# 「データ（X）」と「教師ラベル（y）」を、1つの「データセット（dataset）」にまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)  # 訓練用\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)  # 精度検証用\n",
    "\n",
    "# ミニバッチを扱うための「データローダー（loader）」（訓練用と精度検証用）を作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1/5000] loss: 1.94909\n",
      "[Epoch   2/5000] loss: 2.19466\n",
      "[Epoch   3/5000] loss: 2.30652\n",
      "[Epoch   4/5000] loss: 2.68425\n",
      "[Epoch   5/5000] loss: 2.53528\n",
      "[Epoch   6/5000] loss: 2.12967\n",
      "[Epoch   7/5000] loss: 2.72017\n",
      "[Epoch   8/5000] loss: 2.41786\n",
      "[Epoch   9/5000] loss: 1.91970\n",
      "[Epoch  10/5000] loss: 1.35317\n",
      "[Epoch  11/5000] loss: 2.79001\n",
      "[Epoch  12/5000] loss: 1.96304\n",
      "[Epoch  13/5000] loss: 1.27749\n",
      "[Epoch  14/5000] loss: 1.42883\n",
      "[Epoch  15/5000] loss: 1.26345\n",
      "[Epoch  16/5000] loss: 1.51561\n",
      "[Epoch  17/5000] loss: 1.27386\n",
      "[Epoch  18/5000] loss: 1.23823\n",
      "[Epoch  19/5000] loss: 2.26111\n",
      "[Epoch  20/5000] loss: 1.56616\n",
      "[Epoch  21/5000] loss: 1.06814\n",
      "[Epoch  22/5000] loss: 1.08853\n",
      "[Epoch  23/5000] loss: 1.63941\n",
      "[Epoch  24/5000] loss: 1.06193\n",
      "[Epoch  25/5000] loss: 1.17725\n",
      "[Epoch  26/5000] loss: 1.03967\n",
      "[Epoch  27/5000] loss: 0.83708\n",
      "[Epoch  28/5000] loss: 1.68611\n",
      "[Epoch  29/5000] loss: 1.26967\n",
      "[Epoch  30/5000] loss: 2.24562\n",
      "[Epoch  31/5000] loss: 1.03938\n",
      "[Epoch  32/5000] loss: 0.81704\n",
      "[Epoch  33/5000] loss: 0.89755\n",
      "[Epoch  34/5000] loss: 1.36540\n",
      "[Epoch  35/5000] loss: 0.91026\n",
      "[Epoch  36/5000] loss: 1.02156\n",
      "[Epoch  37/5000] loss: 1.45123\n",
      "[Epoch  38/5000] loss: 1.05817\n",
      "[Epoch  39/5000] loss: 0.73743\n",
      "[Epoch  40/5000] loss: 0.94186\n",
      "[Epoch  41/5000] loss: 0.58897\n",
      "[Epoch  42/5000] loss: 3.90506\n",
      "[Epoch  43/5000] loss: 0.63210\n",
      "[Epoch  44/5000] loss: 1.36690\n",
      "[Epoch  45/5000] loss: 1.13755\n",
      "[Epoch  46/5000] loss: 0.73845\n",
      "[Epoch  47/5000] loss: 1.27177\n",
      "[Epoch  48/5000] loss: 0.96682\n",
      "[Epoch  49/5000] loss: 0.73005\n",
      "[Epoch  50/5000] loss: 1.04958\n",
      "[Epoch  51/5000] loss: 1.14947\n",
      "[Epoch  52/5000] loss: 1.19935\n",
      "[Epoch  53/5000] loss: 0.80972\n",
      "[Epoch  54/5000] loss: 1.00142\n",
      "[Epoch  55/5000] loss: 1.55420\n",
      "[Epoch  56/5000] loss: 1.48537\n",
      "[Epoch  57/5000] loss: 1.08931\n",
      "[Epoch  58/5000] loss: 1.30412\n",
      "[Epoch  59/5000] loss: 0.90644\n",
      "[Epoch  60/5000] loss: 1.39361\n",
      "[Epoch  61/5000] loss: 0.87388\n",
      "[Epoch  62/5000] loss: 0.87483\n",
      "[Epoch  63/5000] loss: 1.16047\n",
      "[Epoch  64/5000] loss: 0.79894\n",
      "[Epoch  65/5000] loss: 1.43719\n",
      "[Epoch  66/5000] loss: 1.41034\n",
      "[Epoch  67/5000] loss: 0.81849\n",
      "[Epoch  68/5000] loss: 0.97426\n",
      "[Epoch  69/5000] loss: 0.69063\n",
      "[Epoch  70/5000] loss: 1.00282\n",
      "[Epoch  71/5000] loss: 1.23768\n",
      "[Epoch  72/5000] loss: 1.27181\n",
      "[Epoch  73/5000] loss: 1.10805\n",
      "[Epoch  74/5000] loss: 3.12869\n",
      "[Epoch  75/5000] loss: 0.80295\n",
      "[Epoch  76/5000] loss: 3.87453\n",
      "[Epoch  77/5000] loss: 1.05322\n",
      "[Epoch  78/5000] loss: 0.85486\n",
      "[Epoch  79/5000] loss: 0.94418\n",
      "[Epoch  80/5000] loss: 1.44748\n",
      "[Epoch  81/5000] loss: 1.33249\n",
      "[Epoch  82/5000] loss: 0.95199\n",
      "[Epoch  83/5000] loss: 0.93269\n",
      "[Epoch  84/5000] loss: 1.06287\n",
      "[Epoch  85/5000] loss: 0.75146\n",
      "[Epoch  86/5000] loss: 1.01581\n",
      "[Epoch  87/5000] loss: 0.84491\n",
      "[Epoch  88/5000] loss: 1.19800\n",
      "[Epoch  89/5000] loss: 0.75171\n",
      "[Epoch  90/5000] loss: 1.00593\n",
      "[Epoch  91/5000] loss: 0.83457\n",
      "[Epoch  92/5000] loss: 1.33708\n",
      "[Epoch  93/5000] loss: 0.86789\n",
      "[Epoch  94/5000] loss: 0.77774\n",
      "[Epoch  95/5000] loss: 1.55512\n",
      "[Epoch  96/5000] loss: 0.60038\n",
      "[Epoch  97/5000] loss: 0.83672\n",
      "[Epoch  98/5000] loss: 1.29162\n",
      "[Epoch  99/5000] loss: 0.78095\n",
      "[Epoch 100/5000] loss: 1.48052\n",
      "[Epoch 101/5000] loss: 0.57407\n",
      "[Epoch 102/5000] loss: 0.65778\n",
      "[Epoch 103/5000] loss: 0.84058\n",
      "[Epoch 104/5000] loss: 1.52169\n",
      "[Epoch 105/5000] loss: 0.84881\n",
      "[Epoch 106/5000] loss: 1.02871\n",
      "[Epoch 107/5000] loss: 0.76777\n",
      "[Epoch 108/5000] loss: 0.63023\n",
      "[Epoch 109/5000] loss: 1.36568\n",
      "[Epoch 110/5000] loss: 0.95148\n",
      "[Epoch 111/5000] loss: 0.95284\n",
      "[Epoch 112/5000] loss: 1.22469\n",
      "[Epoch 113/5000] loss: 0.94280\n",
      "[Epoch 114/5000] loss: 0.73828\n",
      "[Epoch 115/5000] loss: 0.91104\n",
      "[Epoch 116/5000] loss: 1.03245\n",
      "[Epoch 117/5000] loss: 1.10793\n",
      "[Epoch 118/5000] loss: 0.61339\n",
      "[Epoch 119/5000] loss: 0.76472\n",
      "[Epoch 120/5000] loss: 0.89596\n",
      "[Epoch 121/5000] loss: 1.09510\n",
      "[Epoch 122/5000] loss: 0.77755\n",
      "[Epoch 123/5000] loss: 1.46550\n",
      "[Epoch 124/5000] loss: 1.16361\n",
      "[Epoch 125/5000] loss: 1.10118\n",
      "[Epoch 126/5000] loss: 0.61442\n",
      "[Epoch 127/5000] loss: 0.66775\n",
      "[Epoch 128/5000] loss: 0.69201\n",
      "[Epoch 129/5000] loss: 0.78201\n",
      "[Epoch 130/5000] loss: 0.68840\n",
      "[Epoch 131/5000] loss: 1.23798\n",
      "[Epoch 132/5000] loss: 0.94586\n",
      "[Epoch 133/5000] loss: 1.36708\n",
      "[Epoch 134/5000] loss: 1.22830\n",
      "[Epoch 135/5000] loss: 1.28813\n",
      "[Epoch 136/5000] loss: 0.99199\n",
      "[Epoch 137/5000] loss: 1.45807\n",
      "[Epoch 138/5000] loss: 0.82610\n",
      "[Epoch 139/5000] loss: 0.78847\n",
      "[Epoch 140/5000] loss: 1.03924\n",
      "[Epoch 141/5000] loss: 1.05378\n",
      "[Epoch 142/5000] loss: 1.10934\n",
      "[Epoch 143/5000] loss: 0.66193\n",
      "[Epoch 144/5000] loss: 0.92152\n",
      "[Epoch 145/5000] loss: 0.86831\n",
      "[Epoch 146/5000] loss: 0.82900\n",
      "[Epoch 147/5000] loss: 1.81077\n",
      "[Epoch 148/5000] loss: 0.82587\n",
      "[Epoch 149/5000] loss: 0.98330\n",
      "[Epoch 150/5000] loss: 0.74494\n",
      "[Epoch 151/5000] loss: 0.67371\n",
      "[Epoch 152/5000] loss: 1.05844\n",
      "[Epoch 153/5000] loss: 1.49709\n",
      "[Epoch 154/5000] loss: 0.47974\n",
      "[Epoch 155/5000] loss: 0.68823\n",
      "[Epoch 156/5000] loss: 0.79780\n",
      "[Epoch 157/5000] loss: 1.02215\n",
      "[Epoch 158/5000] loss: 1.50245\n",
      "[Epoch 159/5000] loss: 0.98503\n",
      "[Epoch 160/5000] loss: 0.54836\n",
      "[Epoch 161/5000] loss: 0.76505\n",
      "[Epoch 162/5000] loss: 0.90273\n",
      "[Epoch 163/5000] loss: 0.92854\n",
      "[Epoch 164/5000] loss: 0.86903\n",
      "[Epoch 165/5000] loss: 1.20155\n",
      "[Epoch 166/5000] loss: 0.68915\n",
      "[Epoch 167/5000] loss: 0.67628\n",
      "[Epoch 168/5000] loss: 1.79215\n",
      "[Epoch 169/5000] loss: 1.03734\n",
      "[Epoch 170/5000] loss: 1.00345\n",
      "[Epoch 171/5000] loss: 1.13917\n",
      "[Epoch 172/5000] loss: 1.36443\n",
      "[Epoch 173/5000] loss: 0.86885\n",
      "[Epoch 174/5000] loss: 0.89357\n",
      "[Epoch 175/5000] loss: 0.76763\n",
      "[Epoch 176/5000] loss: 0.77169\n",
      "[Epoch 177/5000] loss: 0.60765\n",
      "[Epoch 178/5000] loss: 1.34085\n",
      "[Epoch 179/5000] loss: 0.87802\n",
      "[Epoch 180/5000] loss: 0.92323\n",
      "[Epoch 181/5000] loss: 0.84397\n",
      "[Epoch 182/5000] loss: 0.93332\n",
      "[Epoch 183/5000] loss: 0.94542\n",
      "[Epoch 184/5000] loss: 1.04445\n",
      "[Epoch 185/5000] loss: 0.62792\n",
      "[Epoch 186/5000] loss: 0.79313\n",
      "[Epoch 187/5000] loss: 0.64332\n",
      "[Epoch 188/5000] loss: 0.51727\n",
      "[Epoch 189/5000] loss: 0.72885\n",
      "[Epoch 190/5000] loss: 1.74218\n",
      "[Epoch 191/5000] loss: 1.23686\n",
      "[Epoch 192/5000] loss: 0.78338\n",
      "[Epoch 193/5000] loss: 1.21555\n",
      "[Epoch 194/5000] loss: 0.82816\n",
      "[Epoch 195/5000] loss: 0.64032\n",
      "[Epoch 196/5000] loss: 0.57696\n",
      "[Epoch 197/5000] loss: 0.80391\n",
      "[Epoch 198/5000] loss: 1.62523\n",
      "[Epoch 199/5000] loss: 1.63873\n",
      "[Epoch 200/5000] loss: 1.39409\n",
      "[Epoch 201/5000] loss: 1.17040\n",
      "[Epoch 202/5000] loss: 1.28928\n",
      "[Epoch 203/5000] loss: 1.03634\n",
      "[Epoch 204/5000] loss: 1.63842\n",
      "[Epoch 205/5000] loss: 0.82558\n",
      "[Epoch 206/5000] loss: 0.90440\n",
      "[Epoch 207/5000] loss: 0.91850\n",
      "[Epoch 208/5000] loss: 0.76685\n",
      "[Epoch 209/5000] loss: 1.05938\n",
      "[Epoch 210/5000] loss: 0.88484\n",
      "[Epoch 211/5000] loss: 1.57657\n",
      "[Epoch 212/5000] loss: 0.76566\n",
      "[Epoch 213/5000] loss: 0.52245\n",
      "[Epoch 214/5000] loss: 1.26153\n",
      "[Epoch 215/5000] loss: 0.70754\n",
      "[Epoch 216/5000] loss: 0.90511\n",
      "[Epoch 217/5000] loss: 1.47993\n",
      "[Epoch 218/5000] loss: 1.12333\n",
      "[Epoch 219/5000] loss: 0.80017\n",
      "[Epoch 220/5000] loss: 0.56334\n",
      "[Epoch 221/5000] loss: 1.38093\n",
      "[Epoch 222/5000] loss: 1.03965\n",
      "[Epoch 223/5000] loss: 1.13044\n",
      "[Epoch 224/5000] loss: 0.91281\n",
      "[Epoch 225/5000] loss: 1.23504\n",
      "[Epoch 226/5000] loss: 1.25443\n",
      "[Epoch 227/5000] loss: 0.65944\n",
      "[Epoch 228/5000] loss: 0.83728\n",
      "[Epoch 229/5000] loss: 1.46833\n",
      "[Epoch 230/5000] loss: 1.29985\n",
      "[Epoch 231/5000] loss: 1.68790\n",
      "[Epoch 232/5000] loss: 0.78835\n",
      "[Epoch 233/5000] loss: 0.94823\n",
      "[Epoch 234/5000] loss: 0.74152\n",
      "[Epoch 235/5000] loss: 1.06204\n",
      "[Epoch 236/5000] loss: 0.77939\n",
      "[Epoch 237/5000] loss: 3.06693\n",
      "[Epoch 238/5000] loss: 1.20835\n",
      "[Epoch 239/5000] loss: 0.84582\n",
      "[Epoch 240/5000] loss: 0.84755\n",
      "[Epoch 241/5000] loss: 1.17711\n",
      "[Epoch 242/5000] loss: 0.85924\n",
      "[Epoch 243/5000] loss: 1.06657\n",
      "[Epoch 244/5000] loss: 1.37634\n",
      "[Epoch 245/5000] loss: 1.01934\n",
      "[Epoch 246/5000] loss: 1.61756\n",
      "[Epoch 247/5000] loss: 0.69721\n",
      "[Epoch 248/5000] loss: 3.35239\n",
      "[Epoch 249/5000] loss: 1.35613\n",
      "[Epoch 250/5000] loss: 0.62640\n",
      "[Epoch 251/5000] loss: 0.91150\n",
      "[Epoch 252/5000] loss: 1.29925\n",
      "[Epoch 253/5000] loss: 1.25581\n",
      "[Epoch 254/5000] loss: 0.56282\n",
      "[Epoch 255/5000] loss: 1.14300\n",
      "[Epoch 256/5000] loss: 1.13821\n",
      "[Epoch 257/5000] loss: 4.02745\n",
      "[Epoch 258/5000] loss: 0.99474\n",
      "[Epoch 259/5000] loss: 0.91950\n",
      "[Epoch 260/5000] loss: 0.55268\n",
      "[Epoch 261/5000] loss: 0.93058\n",
      "[Epoch 262/5000] loss: 0.73495\n",
      "[Epoch 263/5000] loss: 0.75187\n",
      "[Epoch 264/5000] loss: 1.04263\n",
      "[Epoch 265/5000] loss: 1.37814\n",
      "[Epoch 266/5000] loss: 1.02551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 267/5000] loss: 0.88715\n",
      "[Epoch 268/5000] loss: 0.63115\n",
      "[Epoch 269/5000] loss: 0.88811\n",
      "[Epoch 270/5000] loss: 1.01345\n",
      "[Epoch 271/5000] loss: 0.98026\n",
      "[Epoch 272/5000] loss: 1.11369\n",
      "[Epoch 273/5000] loss: 0.59957\n",
      "[Epoch 274/5000] loss: 0.62699\n",
      "[Epoch 275/5000] loss: 1.55663\n",
      "[Epoch 276/5000] loss: 0.60274\n",
      "[Epoch 277/5000] loss: 0.56477\n",
      "[Epoch 278/5000] loss: 0.53766\n",
      "[Epoch 279/5000] loss: 1.05818\n",
      "[Epoch 280/5000] loss: 1.15150\n",
      "[Epoch 281/5000] loss: 0.95561\n",
      "[Epoch 282/5000] loss: 1.29557\n",
      "[Epoch 283/5000] loss: 1.08181\n",
      "[Epoch 284/5000] loss: 0.86400\n",
      "[Epoch 285/5000] loss: 1.06262\n",
      "[Epoch 286/5000] loss: 0.85973\n",
      "[Epoch 287/5000] loss: 0.92382\n",
      "[Epoch 288/5000] loss: 0.62662\n",
      "[Epoch 289/5000] loss: 0.87629\n",
      "[Epoch 290/5000] loss: 0.74275\n",
      "[Epoch 291/5000] loss: 1.08963\n",
      "[Epoch 292/5000] loss: 1.03147\n",
      "[Epoch 293/5000] loss: 0.93925\n",
      "[Epoch 294/5000] loss: 0.70584\n",
      "[Epoch 295/5000] loss: 0.82101\n",
      "[Epoch 296/5000] loss: 1.61776\n",
      "[Epoch 297/5000] loss: 0.93229\n",
      "[Epoch 298/5000] loss: 0.59687\n",
      "[Epoch 299/5000] loss: 3.22439\n",
      "[Epoch 300/5000] loss: 1.48445\n",
      "[Epoch 301/5000] loss: 0.94701\n",
      "[Epoch 302/5000] loss: 0.73264\n",
      "[Epoch 303/5000] loss: 0.90803\n",
      "[Epoch 304/5000] loss: 1.03800\n",
      "[Epoch 305/5000] loss: 1.43627\n",
      "[Epoch 306/5000] loss: 1.33411\n",
      "[Epoch 307/5000] loss: 1.12972\n",
      "[Epoch 308/5000] loss: 1.07689\n",
      "[Epoch 309/5000] loss: 1.35760\n",
      "[Epoch 310/5000] loss: 1.16357\n",
      "[Epoch 311/5000] loss: 1.26225\n",
      "[Epoch 312/5000] loss: 1.73421\n",
      "[Epoch 313/5000] loss: 1.43273\n",
      "[Epoch 314/5000] loss: 1.62217\n",
      "[Epoch 315/5000] loss: 1.20491\n",
      "[Epoch 316/5000] loss: 0.86577\n",
      "[Epoch 317/5000] loss: 0.59778\n",
      "[Epoch 318/5000] loss: 0.60967\n",
      "[Epoch 319/5000] loss: 0.64827\n",
      "[Epoch 320/5000] loss: 0.85827\n",
      "[Epoch 321/5000] loss: 1.13628\n",
      "[Epoch 322/5000] loss: 0.46882\n",
      "[Epoch 323/5000] loss: 0.87669\n",
      "[Epoch 324/5000] loss: 1.20560\n",
      "[Epoch 325/5000] loss: 0.52622\n",
      "[Epoch 326/5000] loss: 1.20704\n",
      "[Epoch 327/5000] loss: 1.19311\n",
      "[Epoch 328/5000] loss: 0.82617\n",
      "[Epoch 329/5000] loss: 1.44585\n",
      "[Epoch 330/5000] loss: 0.88001\n",
      "[Epoch 331/5000] loss: 1.04316\n",
      "[Epoch 332/5000] loss: 0.92137\n",
      "[Epoch 333/5000] loss: 1.07491\n",
      "[Epoch 334/5000] loss: 0.85042\n",
      "[Epoch 335/5000] loss: 0.81716\n",
      "[Epoch 336/5000] loss: 1.19941\n",
      "[Epoch 337/5000] loss: 0.66027\n",
      "[Epoch 338/5000] loss: 1.21190\n",
      "[Epoch 339/5000] loss: 1.35730\n",
      "[Epoch 340/5000] loss: 0.57685\n",
      "[Epoch 341/5000] loss: 1.20631\n",
      "[Epoch 342/5000] loss: 0.91857\n",
      "[Epoch 343/5000] loss: 0.62263\n",
      "[Epoch 344/5000] loss: 0.68800\n",
      "[Epoch 345/5000] loss: 0.95432\n",
      "[Epoch 346/5000] loss: 1.06625\n",
      "[Epoch 347/5000] loss: 3.28480\n",
      "[Epoch 348/5000] loss: 0.96437\n",
      "[Epoch 349/5000] loss: 0.77169\n",
      "[Epoch 350/5000] loss: 1.49708\n",
      "[Epoch 351/5000] loss: 0.94189\n",
      "[Epoch 352/5000] loss: 0.84781\n",
      "[Epoch 353/5000] loss: 3.30365\n",
      "[Epoch 354/5000] loss: 0.44366\n",
      "[Epoch 355/5000] loss: 1.12396\n",
      "[Epoch 356/5000] loss: 0.85670\n",
      "[Epoch 357/5000] loss: 0.86125\n",
      "[Epoch 358/5000] loss: 0.79605\n",
      "[Epoch 359/5000] loss: 1.68576\n",
      "[Epoch 360/5000] loss: 0.91212\n",
      "[Epoch 361/5000] loss: 0.58707\n",
      "[Epoch 362/5000] loss: 1.37206\n",
      "[Epoch 363/5000] loss: 1.63872\n",
      "[Epoch 364/5000] loss: 0.64458\n",
      "[Epoch 365/5000] loss: 0.76762\n",
      "[Epoch 366/5000] loss: 0.75327\n",
      "[Epoch 367/5000] loss: 1.14491\n",
      "[Epoch 368/5000] loss: 0.83754\n",
      "[Epoch 369/5000] loss: 1.01809\n",
      "[Epoch 370/5000] loss: 1.11013\n",
      "[Epoch 371/5000] loss: 0.68983\n",
      "[Epoch 372/5000] loss: 0.86433\n",
      "[Epoch 373/5000] loss: 1.27625\n",
      "[Epoch 374/5000] loss: 3.19055\n",
      "[Epoch 375/5000] loss: 1.17740\n",
      "[Epoch 376/5000] loss: 1.17061\n",
      "[Epoch 377/5000] loss: 0.85334\n",
      "[Epoch 378/5000] loss: 0.80658\n",
      "[Epoch 379/5000] loss: 0.83114\n",
      "[Epoch 380/5000] loss: 0.76873\n",
      "[Epoch 381/5000] loss: 0.80126\n",
      "[Epoch 382/5000] loss: 1.32921\n",
      "[Epoch 383/5000] loss: 0.91081\n",
      "[Epoch 384/5000] loss: 0.63600\n",
      "[Epoch 385/5000] loss: 0.73178\n",
      "[Epoch 386/5000] loss: 0.69280\n",
      "[Epoch 387/5000] loss: 0.76100\n",
      "[Epoch 388/5000] loss: 0.89416\n",
      "[Epoch 389/5000] loss: 1.52109\n",
      "[Epoch 390/5000] loss: 1.22651\n",
      "[Epoch 391/5000] loss: 1.00360\n",
      "[Epoch 392/5000] loss: 0.65166\n",
      "[Epoch 393/5000] loss: 0.86445\n",
      "[Epoch 394/5000] loss: 0.96264\n",
      "[Epoch 395/5000] loss: 0.53406\n",
      "[Epoch 396/5000] loss: 1.13171\n",
      "[Epoch 397/5000] loss: 1.43538\n",
      "[Epoch 398/5000] loss: 0.95213\n",
      "[Epoch 399/5000] loss: 1.01476\n",
      "[Epoch 400/5000] loss: 1.14619\n",
      "[Epoch 401/5000] loss: 1.18882\n",
      "[Epoch 402/5000] loss: 0.62497\n",
      "[Epoch 403/5000] loss: 0.97447\n",
      "[Epoch 404/5000] loss: 1.01234\n",
      "[Epoch 405/5000] loss: 0.93214\n",
      "[Epoch 406/5000] loss: 1.10773\n",
      "[Epoch 407/5000] loss: 0.79367\n",
      "[Epoch 408/5000] loss: 0.89407\n",
      "[Epoch 409/5000] loss: 1.00799\n",
      "[Epoch 410/5000] loss: 1.00061\n",
      "[Epoch 411/5000] loss: 0.39394\n",
      "[Epoch 412/5000] loss: 1.10828\n",
      "[Epoch 413/5000] loss: 1.13351\n",
      "[Epoch 414/5000] loss: 0.54377\n",
      "[Epoch 415/5000] loss: 1.07988\n",
      "[Epoch 416/5000] loss: 3.10337\n",
      "[Epoch 417/5000] loss: 0.60623\n",
      "[Epoch 418/5000] loss: 0.69957\n",
      "[Epoch 419/5000] loss: 0.75148\n",
      "[Epoch 420/5000] loss: 2.99012\n",
      "[Epoch 421/5000] loss: 1.10395\n",
      "[Epoch 422/5000] loss: 1.36957\n",
      "[Epoch 423/5000] loss: 0.90231\n",
      "[Epoch 424/5000] loss: 1.32602\n",
      "[Epoch 425/5000] loss: 0.98349\n",
      "[Epoch 426/5000] loss: 1.21310\n",
      "[Epoch 427/5000] loss: 0.86640\n",
      "[Epoch 428/5000] loss: 1.43907\n",
      "[Epoch 429/5000] loss: 1.14309\n",
      "[Epoch 430/5000] loss: 1.38586\n",
      "[Epoch 431/5000] loss: 1.10318\n",
      "[Epoch 432/5000] loss: 0.81055\n",
      "[Epoch 433/5000] loss: 1.25215\n",
      "[Epoch 434/5000] loss: 1.11949\n",
      "[Epoch 435/5000] loss: 0.91653\n",
      "[Epoch 436/5000] loss: 1.14477\n",
      "[Epoch 437/5000] loss: 0.56357\n",
      "[Epoch 438/5000] loss: 3.55732\n",
      "[Epoch 439/5000] loss: 0.65065\n",
      "[Epoch 440/5000] loss: 0.85887\n",
      "[Epoch 441/5000] loss: 0.94302\n",
      "[Epoch 442/5000] loss: 1.55009\n",
      "[Epoch 443/5000] loss: 1.40844\n",
      "[Epoch 444/5000] loss: 1.03903\n",
      "[Epoch 445/5000] loss: 1.08088\n",
      "[Epoch 446/5000] loss: 0.66802\n",
      "[Epoch 447/5000] loss: 0.89993\n",
      "[Epoch 448/5000] loss: 0.70141\n",
      "[Epoch 449/5000] loss: 1.48791\n",
      "[Epoch 450/5000] loss: 1.64579\n",
      "[Epoch 451/5000] loss: 0.67293\n",
      "[Epoch 452/5000] loss: 1.03762\n",
      "[Epoch 453/5000] loss: 1.00819\n",
      "[Epoch 454/5000] loss: 0.63821\n",
      "[Epoch 455/5000] loss: 0.84215\n",
      "[Epoch 456/5000] loss: 1.44021\n",
      "[Epoch 457/5000] loss: 1.48099\n",
      "[Epoch 458/5000] loss: 1.08464\n",
      "[Epoch 459/5000] loss: 0.73476\n",
      "[Epoch 460/5000] loss: 1.01300\n",
      "[Epoch 461/5000] loss: 0.51912\n",
      "[Epoch 462/5000] loss: 1.05496\n",
      "[Epoch 463/5000] loss: 0.53268\n",
      "[Epoch 464/5000] loss: 0.62778\n",
      "[Epoch 465/5000] loss: 1.02538\n",
      "[Epoch 466/5000] loss: 1.31673\n",
      "[Epoch 467/5000] loss: 1.15743\n",
      "[Epoch 468/5000] loss: 0.60112\n",
      "[Epoch 469/5000] loss: 1.25412\n",
      "[Epoch 470/5000] loss: 0.94527\n",
      "[Epoch 471/5000] loss: 0.91029\n",
      "[Epoch 472/5000] loss: 0.96160\n",
      "[Epoch 473/5000] loss: 1.03785\n",
      "[Epoch 474/5000] loss: 1.12844\n",
      "[Epoch 475/5000] loss: 1.23710\n",
      "[Epoch 476/5000] loss: 1.04047\n",
      "[Epoch 477/5000] loss: 0.88531\n",
      "[Epoch 478/5000] loss: 0.84135\n",
      "[Epoch 479/5000] loss: 0.46078\n",
      "[Epoch 480/5000] loss: 1.18529\n",
      "[Epoch 481/5000] loss: 1.24165\n",
      "[Epoch 482/5000] loss: 1.54229\n",
      "[Epoch 483/5000] loss: 1.36410\n",
      "[Epoch 484/5000] loss: 1.33046\n",
      "[Epoch 485/5000] loss: 0.82159\n",
      "[Epoch 486/5000] loss: 0.80417\n",
      "[Epoch 487/5000] loss: 0.90854\n",
      "[Epoch 488/5000] loss: 2.04640\n",
      "[Epoch 489/5000] loss: 0.84536\n",
      "[Epoch 490/5000] loss: 1.06474\n",
      "[Epoch 491/5000] loss: 0.96381\n",
      "[Epoch 492/5000] loss: 1.38078\n",
      "[Epoch 493/5000] loss: 0.78784\n",
      "[Epoch 494/5000] loss: 1.69496\n",
      "[Epoch 495/5000] loss: 0.57682\n",
      "[Epoch 496/5000] loss: 0.98301\n",
      "[Epoch 497/5000] loss: 1.14419\n",
      "[Epoch 498/5000] loss: 0.72771\n",
      "[Epoch 499/5000] loss: 1.32664\n",
      "[Epoch 500/5000] loss: 1.35937\n",
      "[Epoch 501/5000] loss: 1.35721\n",
      "[Epoch 502/5000] loss: 0.91081\n",
      "[Epoch 503/5000] loss: 0.56324\n",
      "[Epoch 504/5000] loss: 1.00846\n",
      "[Epoch 505/5000] loss: 1.10615\n",
      "[Epoch 506/5000] loss: 1.42525\n",
      "[Epoch 507/5000] loss: 1.12232\n",
      "[Epoch 508/5000] loss: 1.17415\n",
      "[Epoch 509/5000] loss: 1.12435\n",
      "[Epoch 510/5000] loss: 0.60303\n",
      "[Epoch 511/5000] loss: 0.81424\n",
      "[Epoch 512/5000] loss: 1.06421\n",
      "[Epoch 513/5000] loss: 1.65177\n",
      "[Epoch 514/5000] loss: 0.87690\n",
      "[Epoch 515/5000] loss: 1.16071\n",
      "[Epoch 516/5000] loss: 0.83420\n",
      "[Epoch 517/5000] loss: 0.55659\n",
      "[Epoch 518/5000] loss: 0.76252\n",
      "[Epoch 519/5000] loss: 0.78264\n",
      "[Epoch 520/5000] loss: 1.02471\n",
      "[Epoch 521/5000] loss: 0.66871\n",
      "[Epoch 522/5000] loss: 1.44583\n",
      "[Epoch 523/5000] loss: 1.35585\n",
      "[Epoch 524/5000] loss: 0.90980\n",
      "[Epoch 525/5000] loss: 1.30927\n",
      "[Epoch 526/5000] loss: 0.88223\n",
      "[Epoch 527/5000] loss: 0.99145\n",
      "[Epoch 528/5000] loss: 0.58230\n",
      "[Epoch 529/5000] loss: 0.94818\n",
      "[Epoch 530/5000] loss: 1.90153\n",
      "[Epoch 531/5000] loss: 1.39603\n",
      "[Epoch 532/5000] loss: 1.02426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 533/5000] loss: 0.61553\n",
      "[Epoch 534/5000] loss: 0.48870\n",
      "[Epoch 535/5000] loss: 1.05558\n",
      "[Epoch 536/5000] loss: 0.49218\n",
      "[Epoch 537/5000] loss: 0.86624\n",
      "[Epoch 538/5000] loss: 0.63690\n",
      "[Epoch 539/5000] loss: 0.56103\n",
      "[Epoch 540/5000] loss: 0.78670\n",
      "[Epoch 541/5000] loss: 3.26714\n",
      "[Epoch 542/5000] loss: 0.87384\n",
      "[Epoch 543/5000] loss: 1.09881\n",
      "[Epoch 544/5000] loss: 3.02692\n",
      "[Epoch 545/5000] loss: 1.40912\n",
      "[Epoch 546/5000] loss: 0.81128\n",
      "[Epoch 547/5000] loss: 0.82748\n",
      "[Epoch 548/5000] loss: 1.08609\n",
      "[Epoch 549/5000] loss: 0.63184\n",
      "[Epoch 550/5000] loss: 0.49453\n",
      "[Epoch 551/5000] loss: 0.79758\n",
      "[Epoch 552/5000] loss: 1.14194\n",
      "[Epoch 553/5000] loss: 0.70575\n",
      "[Epoch 554/5000] loss: 0.37611\n",
      "[Epoch 555/5000] loss: 1.13519\n",
      "[Epoch 556/5000] loss: 1.24835\n",
      "[Epoch 557/5000] loss: 0.68323\n",
      "[Epoch 558/5000] loss: 1.55977\n",
      "[Epoch 559/5000] loss: 0.71100\n",
      "[Epoch 560/5000] loss: 0.94146\n",
      "[Epoch 561/5000] loss: 0.97081\n",
      "[Epoch 562/5000] loss: 0.82525\n",
      "[Epoch 563/5000] loss: 3.46457\n",
      "[Epoch 564/5000] loss: 0.60786\n",
      "[Epoch 565/5000] loss: 1.43690\n",
      "[Epoch 566/5000] loss: 0.83018\n",
      "[Epoch 567/5000] loss: 1.02719\n",
      "[Epoch 568/5000] loss: 1.10497\n",
      "[Epoch 569/5000] loss: 0.98471\n",
      "[Epoch 570/5000] loss: 1.49979\n",
      "[Epoch 571/5000] loss: 0.89417\n",
      "[Epoch 572/5000] loss: 0.81571\n",
      "[Epoch 573/5000] loss: 0.81749\n",
      "[Epoch 574/5000] loss: 0.89800\n",
      "[Epoch 575/5000] loss: 1.22176\n",
      "[Epoch 576/5000] loss: 0.93639\n",
      "[Epoch 577/5000] loss: 0.88366\n",
      "[Epoch 578/5000] loss: 1.31530\n",
      "[Epoch 579/5000] loss: 0.82566\n",
      "[Epoch 580/5000] loss: 0.97487\n",
      "[Epoch 581/5000] loss: 1.31151\n",
      "[Epoch 582/5000] loss: 0.65799\n",
      "[Epoch 583/5000] loss: 0.89600\n",
      "[Epoch 584/5000] loss: 1.15821\n",
      "[Epoch 585/5000] loss: 1.20896\n",
      "[Epoch 586/5000] loss: 0.87149\n",
      "[Epoch 587/5000] loss: 0.68740\n",
      "[Epoch 588/5000] loss: 0.86637\n",
      "[Epoch 589/5000] loss: 1.03737\n",
      "[Epoch 590/5000] loss: 0.76764\n",
      "[Epoch 591/5000] loss: 0.95392\n",
      "[Epoch 592/5000] loss: 1.00425\n",
      "[Epoch 593/5000] loss: 0.94694\n",
      "[Epoch 594/5000] loss: 0.64636\n",
      "[Epoch 595/5000] loss: 0.85535\n",
      "[Epoch 596/5000] loss: 0.79239\n",
      "[Epoch 597/5000] loss: 0.87352\n",
      "[Epoch 598/5000] loss: 0.56689\n",
      "[Epoch 599/5000] loss: 0.88217\n",
      "[Epoch 600/5000] loss: 1.20969\n",
      "[Epoch 601/5000] loss: 0.78419\n",
      "[Epoch 602/5000] loss: 0.66357\n",
      "[Epoch 603/5000] loss: 0.83272\n",
      "[Epoch 604/5000] loss: 0.96033\n",
      "[Epoch 605/5000] loss: 1.28290\n",
      "[Epoch 606/5000] loss: 0.46218\n",
      "[Epoch 607/5000] loss: 0.59706\n",
      "[Epoch 608/5000] loss: 0.85967\n",
      "[Epoch 609/5000] loss: 1.09212\n",
      "[Epoch 610/5000] loss: 0.72927\n",
      "[Epoch 611/5000] loss: 1.16212\n",
      "[Epoch 612/5000] loss: 1.21070\n",
      "[Epoch 613/5000] loss: 0.77863\n",
      "[Epoch 614/5000] loss: 0.67671\n",
      "[Epoch 615/5000] loss: 0.77075\n",
      "[Epoch 616/5000] loss: 1.05944\n",
      "[Epoch 617/5000] loss: 0.93547\n",
      "[Epoch 618/5000] loss: 0.45699\n",
      "[Epoch 619/5000] loss: 1.19244\n",
      "[Epoch 620/5000] loss: 1.16749\n",
      "[Epoch 621/5000] loss: 0.79018\n",
      "[Epoch 622/5000] loss: 1.00536\n",
      "[Epoch 623/5000] loss: 1.24045\n",
      "[Epoch 624/5000] loss: 1.50203\n",
      "[Epoch 625/5000] loss: 0.73201\n",
      "[Epoch 626/5000] loss: 0.55315\n",
      "[Epoch 627/5000] loss: 0.56937\n",
      "[Epoch 628/5000] loss: 1.08414\n",
      "[Epoch 629/5000] loss: 0.73966\n",
      "[Epoch 630/5000] loss: 0.96981\n",
      "[Epoch 631/5000] loss: 0.89807\n",
      "[Epoch 632/5000] loss: 1.36865\n",
      "[Epoch 633/5000] loss: 0.96317\n",
      "[Epoch 634/5000] loss: 1.34609\n",
      "[Epoch 635/5000] loss: 0.75281\n",
      "[Epoch 636/5000] loss: 1.21054\n",
      "[Epoch 637/5000] loss: 1.05858\n",
      "[Epoch 638/5000] loss: 0.84909\n",
      "[Epoch 639/5000] loss: 0.91563\n",
      "[Epoch 640/5000] loss: 0.73679\n",
      "[Epoch 641/5000] loss: 0.97849\n",
      "[Epoch 642/5000] loss: 1.53939\n",
      "[Epoch 643/5000] loss: 0.47541\n",
      "[Epoch 644/5000] loss: 0.88000\n",
      "[Epoch 645/5000] loss: 0.57654\n",
      "[Epoch 646/5000] loss: 0.60685\n",
      "[Epoch 647/5000] loss: 0.60647\n",
      "[Epoch 648/5000] loss: 1.57751\n",
      "[Epoch 649/5000] loss: 1.37893\n",
      "[Epoch 650/5000] loss: 0.55375\n",
      "[Epoch 651/5000] loss: 0.62627\n",
      "[Epoch 652/5000] loss: 0.84954\n",
      "[Epoch 653/5000] loss: 0.71105\n",
      "[Epoch 654/5000] loss: 0.78777\n",
      "[Epoch 655/5000] loss: 1.20762\n",
      "[Epoch 656/5000] loss: 0.75216\n",
      "[Epoch 657/5000] loss: 0.74109\n",
      "[Epoch 658/5000] loss: 1.26216\n",
      "[Epoch 659/5000] loss: 0.86010\n",
      "[Epoch 660/5000] loss: 0.98207\n",
      "[Epoch 661/5000] loss: 0.53767\n",
      "[Epoch 662/5000] loss: 1.02230\n",
      "[Epoch 663/5000] loss: 0.88385\n",
      "[Epoch 664/5000] loss: 0.81254\n",
      "[Epoch 665/5000] loss: 0.65920\n",
      "[Epoch 666/5000] loss: 0.98009\n",
      "[Epoch 667/5000] loss: 0.93557\n",
      "[Epoch 668/5000] loss: 0.80515\n",
      "[Epoch 669/5000] loss: 0.91642\n",
      "[Epoch 670/5000] loss: 3.10537\n",
      "[Epoch 671/5000] loss: 0.58615\n",
      "[Epoch 672/5000] loss: 1.05465\n",
      "[Epoch 673/5000] loss: 1.07191\n",
      "[Epoch 674/5000] loss: 0.62625\n",
      "[Epoch 675/5000] loss: 1.43514\n",
      "[Epoch 676/5000] loss: 0.40519\n",
      "[Epoch 677/5000] loss: 1.26242\n",
      "[Epoch 678/5000] loss: 1.01442\n",
      "[Epoch 679/5000] loss: 0.81492\n",
      "[Epoch 680/5000] loss: 0.82052\n",
      "[Epoch 681/5000] loss: 0.93945\n",
      "[Epoch 682/5000] loss: 0.85548\n",
      "[Epoch 683/5000] loss: 1.48484\n",
      "[Epoch 684/5000] loss: 0.84717\n",
      "[Epoch 685/5000] loss: 1.32722\n",
      "[Epoch 686/5000] loss: 0.93851\n",
      "[Epoch 687/5000] loss: 0.88228\n",
      "[Epoch 688/5000] loss: 0.90334\n",
      "[Epoch 689/5000] loss: 1.24081\n",
      "[Epoch 690/5000] loss: 0.75543\n",
      "[Epoch 691/5000] loss: 1.05453\n",
      "[Epoch 692/5000] loss: 0.88731\n",
      "[Epoch 693/5000] loss: 0.95898\n",
      "[Epoch 694/5000] loss: 1.23068\n",
      "[Epoch 695/5000] loss: 1.11076\n",
      "[Epoch 696/5000] loss: 1.53107\n",
      "[Epoch 697/5000] loss: 0.44653\n",
      "[Epoch 698/5000] loss: 0.96822\n",
      "[Epoch 699/5000] loss: 1.79777\n",
      "[Epoch 700/5000] loss: 0.82023\n",
      "[Epoch 701/5000] loss: 0.61821\n",
      "[Epoch 702/5000] loss: 0.86628\n",
      "[Epoch 703/5000] loss: 0.77084\n",
      "[Epoch 704/5000] loss: 1.31327\n",
      "[Epoch 705/5000] loss: 1.29051\n",
      "[Epoch 706/5000] loss: 0.84832\n",
      "[Epoch 707/5000] loss: 0.69285\n",
      "[Epoch 708/5000] loss: 0.73633\n",
      "[Epoch 709/5000] loss: 1.30242\n",
      "[Epoch 710/5000] loss: 0.92293\n",
      "[Epoch 711/5000] loss: 1.15807\n",
      "[Epoch 712/5000] loss: 0.70612\n",
      "[Epoch 713/5000] loss: 1.32598\n",
      "[Epoch 714/5000] loss: 1.05348\n",
      "[Epoch 715/5000] loss: 1.02405\n",
      "[Epoch 716/5000] loss: 1.02210\n",
      "[Epoch 717/5000] loss: 0.71886\n",
      "[Epoch 718/5000] loss: 1.33414\n",
      "[Epoch 719/5000] loss: 1.02372\n",
      "[Epoch 720/5000] loss: 1.15985\n",
      "[Epoch 721/5000] loss: 0.84159\n",
      "[Epoch 722/5000] loss: 0.69856\n",
      "[Epoch 723/5000] loss: 0.91759\n",
      "[Epoch 724/5000] loss: 0.98301\n",
      "[Epoch 725/5000] loss: 0.55968\n",
      "[Epoch 726/5000] loss: 0.82742\n",
      "[Epoch 727/5000] loss: 1.03247\n",
      "[Epoch 728/5000] loss: 1.09544\n",
      "[Epoch 729/5000] loss: 1.10667\n",
      "[Epoch 730/5000] loss: 0.93341\n",
      "[Epoch 731/5000] loss: 1.10491\n",
      "[Epoch 732/5000] loss: 0.52565\n",
      "[Epoch 733/5000] loss: 0.67721\n",
      "[Epoch 734/5000] loss: 0.80747\n",
      "[Epoch 735/5000] loss: 0.55884\n",
      "[Epoch 736/5000] loss: 0.94984\n",
      "[Epoch 737/5000] loss: 1.13737\n",
      "[Epoch 738/5000] loss: 1.13850\n",
      "[Epoch 739/5000] loss: 1.08244\n",
      "[Epoch 740/5000] loss: 0.95592\n",
      "[Epoch 741/5000] loss: 1.34966\n",
      "[Epoch 742/5000] loss: 0.99017\n",
      "[Epoch 743/5000] loss: 0.82687\n",
      "[Epoch 744/5000] loss: 1.26632\n",
      "[Epoch 745/5000] loss: 1.44070\n",
      "[Epoch 746/5000] loss: 2.02454\n",
      "[Epoch 747/5000] loss: 0.88493\n",
      "[Epoch 748/5000] loss: 0.70744\n",
      "[Epoch 749/5000] loss: 1.22263\n",
      "[Epoch 750/5000] loss: 1.00333\n",
      "[Epoch 751/5000] loss: 0.59974\n",
      "[Epoch 752/5000] loss: 1.42394\n",
      "[Epoch 753/5000] loss: 1.52947\n",
      "[Epoch 754/5000] loss: 1.29348\n",
      "[Epoch 755/5000] loss: 1.36440\n",
      "[Epoch 756/5000] loss: 1.41501\n",
      "[Epoch 757/5000] loss: 1.31828\n",
      "[Epoch 758/5000] loss: 1.03963\n",
      "[Epoch 759/5000] loss: 0.70201\n",
      "[Epoch 760/5000] loss: 1.09267\n",
      "[Epoch 761/5000] loss: 1.11320\n",
      "[Epoch 762/5000] loss: 0.96034\n",
      "[Epoch 763/5000] loss: 0.56458\n",
      "[Epoch 764/5000] loss: 1.15811\n",
      "[Epoch 765/5000] loss: 1.61857\n",
      "[Epoch 766/5000] loss: 0.94459\n",
      "[Epoch 767/5000] loss: 1.44860\n",
      "[Epoch 768/5000] loss: 1.09208\n",
      "[Epoch 769/5000] loss: 1.13236\n",
      "[Epoch 770/5000] loss: 1.05548\n",
      "[Epoch 771/5000] loss: 1.27189\n",
      "[Epoch 772/5000] loss: 1.39237\n",
      "[Epoch 773/5000] loss: 1.01149\n",
      "[Epoch 774/5000] loss: 0.95738\n",
      "[Epoch 775/5000] loss: 1.49947\n",
      "[Epoch 776/5000] loss: 0.92594\n",
      "[Epoch 777/5000] loss: 0.74206\n",
      "[Epoch 778/5000] loss: 1.41128\n",
      "[Epoch 779/5000] loss: 0.84580\n",
      "[Epoch 780/5000] loss: 1.43453\n",
      "[Epoch 781/5000] loss: 1.04740\n",
      "[Epoch 782/5000] loss: 1.14493\n",
      "[Epoch 783/5000] loss: 1.17542\n",
      "[Epoch 784/5000] loss: 1.03110\n",
      "[Epoch 785/5000] loss: 0.79023\n",
      "[Epoch 786/5000] loss: 0.86780\n",
      "[Epoch 787/5000] loss: 0.77330\n",
      "[Epoch 788/5000] loss: 0.79005\n",
      "[Epoch 789/5000] loss: 1.10222\n",
      "[Epoch 790/5000] loss: 1.07110\n",
      "[Epoch 791/5000] loss: 0.88490\n",
      "[Epoch 792/5000] loss: 1.24267\n",
      "[Epoch 793/5000] loss: 0.94011\n",
      "[Epoch 794/5000] loss: 0.88876\n",
      "[Epoch 795/5000] loss: 0.71570\n",
      "[Epoch 796/5000] loss: 0.71015\n",
      "[Epoch 797/5000] loss: 0.99347\n",
      "[Epoch 798/5000] loss: 0.77147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 799/5000] loss: 1.70175\n",
      "[Epoch 800/5000] loss: 1.17741\n",
      "[Epoch 801/5000] loss: 1.38524\n",
      "[Epoch 802/5000] loss: 0.57210\n",
      "[Epoch 803/5000] loss: 0.46320\n",
      "[Epoch 804/5000] loss: 0.96437\n",
      "[Epoch 805/5000] loss: 1.20778\n",
      "[Epoch 806/5000] loss: 1.45629\n",
      "[Epoch 807/5000] loss: 1.10533\n",
      "[Epoch 808/5000] loss: 1.39858\n",
      "[Epoch 809/5000] loss: 1.04859\n",
      "[Epoch 810/5000] loss: 0.77665\n",
      "[Epoch 811/5000] loss: 0.82502\n",
      "[Epoch 812/5000] loss: 0.75916\n",
      "[Epoch 813/5000] loss: 0.86322\n",
      "[Epoch 814/5000] loss: 1.60848\n",
      "[Epoch 815/5000] loss: 0.58345\n",
      "[Epoch 816/5000] loss: 1.47228\n",
      "[Epoch 817/5000] loss: 1.31605\n",
      "[Epoch 818/5000] loss: 0.88746\n",
      "[Epoch 819/5000] loss: 0.96448\n",
      "[Epoch 820/5000] loss: 1.42438\n",
      "[Epoch 821/5000] loss: 0.92008\n",
      "[Epoch 822/5000] loss: 1.19473\n",
      "[Epoch 823/5000] loss: 0.51267\n",
      "[Epoch 824/5000] loss: 0.75416\n",
      "[Epoch 825/5000] loss: 0.75723\n",
      "[Epoch 826/5000] loss: 0.71800\n",
      "[Epoch 827/5000] loss: 1.16535\n",
      "[Epoch 828/5000] loss: 0.92975\n",
      "[Epoch 829/5000] loss: 0.99092\n",
      "[Epoch 830/5000] loss: 0.64150\n",
      "[Epoch 831/5000] loss: 0.87245\n",
      "[Epoch 832/5000] loss: 0.98327\n",
      "[Epoch 833/5000] loss: 1.58886\n",
      "[Epoch 834/5000] loss: 0.45777\n",
      "[Epoch 835/5000] loss: 0.93690\n",
      "[Epoch 836/5000] loss: 0.70665\n",
      "[Epoch 837/5000] loss: 0.53665\n",
      "[Epoch 838/5000] loss: 0.84578\n",
      "[Epoch 839/5000] loss: 0.87553\n",
      "[Epoch 840/5000] loss: 1.07268\n",
      "[Epoch 841/5000] loss: 0.61456\n",
      "[Epoch 842/5000] loss: 0.98714\n",
      "[Epoch 843/5000] loss: 0.99443\n",
      "[Epoch 844/5000] loss: 0.82611\n",
      "[Epoch 845/5000] loss: 1.15901\n",
      "[Epoch 846/5000] loss: 0.77634\n",
      "[Epoch 847/5000] loss: 0.55635\n",
      "[Epoch 848/5000] loss: 0.72838\n",
      "[Epoch 849/5000] loss: 1.49152\n",
      "[Epoch 850/5000] loss: 0.85339\n",
      "[Epoch 851/5000] loss: 0.58633\n",
      "[Epoch 852/5000] loss: 0.92869\n",
      "[Epoch 853/5000] loss: 1.52054\n",
      "[Epoch 854/5000] loss: 0.78548\n",
      "[Epoch 855/5000] loss: 1.85085\n",
      "[Epoch 856/5000] loss: 0.48277\n",
      "[Epoch 857/5000] loss: 0.88341\n",
      "[Epoch 858/5000] loss: 0.53722\n",
      "[Epoch 859/5000] loss: 0.86859\n",
      "[Epoch 860/5000] loss: 0.82463\n",
      "[Epoch 861/5000] loss: 0.30832\n",
      "[Epoch 862/5000] loss: 0.76397\n",
      "[Epoch 863/5000] loss: 1.35181\n",
      "[Epoch 864/5000] loss: 0.58946\n",
      "[Epoch 865/5000] loss: 0.80994\n",
      "[Epoch 866/5000] loss: 1.47372\n",
      "[Epoch 867/5000] loss: 0.75034\n",
      "[Epoch 868/5000] loss: 0.62983\n",
      "[Epoch 869/5000] loss: 0.96386\n",
      "[Epoch 870/5000] loss: 1.15945\n",
      "[Epoch 871/5000] loss: 0.83833\n",
      "[Epoch 872/5000] loss: 0.84793\n",
      "[Epoch 873/5000] loss: 0.78724\n",
      "[Epoch 874/5000] loss: 1.41482\n",
      "[Epoch 875/5000] loss: 0.86837\n",
      "[Epoch 876/5000] loss: 1.26344\n",
      "[Epoch 877/5000] loss: 0.66701\n",
      "[Epoch 878/5000] loss: 0.43001\n",
      "[Epoch 879/5000] loss: 0.22579\n",
      "[Epoch 880/5000] loss: 0.87870\n",
      "[Epoch 881/5000] loss: 0.85275\n",
      "[Epoch 882/5000] loss: 0.67717\n",
      "[Epoch 883/5000] loss: 1.01164\n",
      "[Epoch 884/5000] loss: 0.87381\n",
      "[Epoch 885/5000] loss: 0.81985\n",
      "[Epoch 886/5000] loss: 0.86624\n",
      "[Epoch 887/5000] loss: 0.64779\n",
      "[Epoch 888/5000] loss: 0.63698\n",
      "[Epoch 889/5000] loss: 0.77995\n",
      "[Epoch 890/5000] loss: 0.83674\n",
      "[Epoch 891/5000] loss: 0.79563\n",
      "[Epoch 892/5000] loss: 1.08113\n",
      "[Epoch 893/5000] loss: 0.57534\n",
      "[Epoch 894/5000] loss: 1.02599\n",
      "[Epoch 895/5000] loss: 1.06892\n",
      "[Epoch 896/5000] loss: 0.48206\n",
      "[Epoch 897/5000] loss: 1.14771\n",
      "[Epoch 898/5000] loss: 1.16271\n",
      "[Epoch 899/5000] loss: 0.73468\n",
      "[Epoch 900/5000] loss: 1.28721\n",
      "[Epoch 901/5000] loss: 0.83560\n",
      "[Epoch 902/5000] loss: 1.12109\n",
      "[Epoch 903/5000] loss: 0.50266\n",
      "[Epoch 904/5000] loss: 0.89792\n",
      "[Epoch 905/5000] loss: 0.75057\n",
      "[Epoch 906/5000] loss: 0.83481\n",
      "[Epoch 907/5000] loss: 0.77658\n",
      "[Epoch 908/5000] loss: 1.05266\n",
      "[Epoch 909/5000] loss: 0.44009\n",
      "[Epoch 910/5000] loss: 0.76035\n",
      "[Epoch 911/5000] loss: 3.56285\n",
      "[Epoch 912/5000] loss: 1.09367\n",
      "[Epoch 913/5000] loss: 0.67835\n",
      "[Epoch 914/5000] loss: 0.89371\n",
      "[Epoch 915/5000] loss: 0.66460\n",
      "[Epoch 916/5000] loss: 0.55216\n",
      "[Epoch 917/5000] loss: 3.33587\n",
      "[Epoch 918/5000] loss: 1.08825\n",
      "[Epoch 919/5000] loss: 0.76355\n",
      "[Epoch 920/5000] loss: 1.26036\n",
      "[Epoch 921/5000] loss: 0.79982\n",
      "[Epoch 922/5000] loss: 0.84749\n",
      "[Epoch 923/5000] loss: 0.84031\n",
      "[Epoch 924/5000] loss: 0.82901\n",
      "[Epoch 925/5000] loss: 0.55034\n",
      "[Epoch 926/5000] loss: 1.11377\n",
      "[Epoch 927/5000] loss: 0.96731\n",
      "[Epoch 928/5000] loss: 0.80495\n",
      "[Epoch 929/5000] loss: 0.38062\n",
      "[Epoch 930/5000] loss: 0.69900\n",
      "[Epoch 931/5000] loss: 0.95166\n",
      "[Epoch 932/5000] loss: 0.95026\n",
      "[Epoch 933/5000] loss: 1.39722\n",
      "[Epoch 934/5000] loss: 0.68449\n",
      "[Epoch 935/5000] loss: 0.74179\n",
      "[Epoch 936/5000] loss: 0.90733\n",
      "[Epoch 937/5000] loss: 1.64062\n",
      "[Epoch 938/5000] loss: 1.09955\n",
      "[Epoch 939/5000] loss: 1.09666\n",
      "[Epoch 940/5000] loss: 0.99466\n",
      "[Epoch 941/5000] loss: 0.95321\n",
      "[Epoch 942/5000] loss: 0.99762\n",
      "[Epoch 943/5000] loss: 1.07006\n",
      "[Epoch 944/5000] loss: 0.96253\n",
      "[Epoch 945/5000] loss: 0.54844\n",
      "[Epoch 946/5000] loss: 0.80418\n",
      "[Epoch 947/5000] loss: 0.76249\n",
      "[Epoch 948/5000] loss: 0.83827\n",
      "[Epoch 949/5000] loss: 1.00433\n",
      "[Epoch 950/5000] loss: 0.68319\n",
      "[Epoch 951/5000] loss: 0.90052\n",
      "[Epoch 952/5000] loss: 0.73479\n",
      "[Epoch 953/5000] loss: 0.83227\n",
      "[Epoch 954/5000] loss: 1.20698\n",
      "[Epoch 955/5000] loss: 0.93265\n",
      "[Epoch 956/5000] loss: 1.21266\n",
      "[Epoch 957/5000] loss: 0.72717\n",
      "[Epoch 958/5000] loss: 0.78285\n",
      "[Epoch 959/5000] loss: 1.10341\n",
      "[Epoch 960/5000] loss: 1.15534\n",
      "[Epoch 961/5000] loss: 0.77971\n",
      "[Epoch 962/5000] loss: 1.16518\n",
      "[Epoch 963/5000] loss: 1.17730\n",
      "[Epoch 964/5000] loss: 0.89304\n",
      "[Epoch 965/5000] loss: 0.94302\n",
      "[Epoch 966/5000] loss: 0.97616\n",
      "[Epoch 967/5000] loss: 1.50953\n",
      "[Epoch 968/5000] loss: 0.98423\n",
      "[Epoch 969/5000] loss: 0.97222\n",
      "[Epoch 970/5000] loss: 0.95181\n",
      "[Epoch 971/5000] loss: 0.86984\n",
      "[Epoch 972/5000] loss: 0.79288\n",
      "[Epoch 973/5000] loss: 0.92332\n",
      "[Epoch 974/5000] loss: 0.99555\n",
      "[Epoch 975/5000] loss: 1.12680\n",
      "[Epoch 976/5000] loss: 1.09534\n",
      "[Epoch 977/5000] loss: 0.91716\n",
      "[Epoch 978/5000] loss: 0.74829\n",
      "[Epoch 979/5000] loss: 0.84392\n",
      "[Epoch 980/5000] loss: 0.73868\n",
      "[Epoch 981/5000] loss: 0.78946\n",
      "[Epoch 982/5000] loss: 1.13810\n",
      "[Epoch 983/5000] loss: 0.65905\n",
      "[Epoch 984/5000] loss: 0.79116\n",
      "[Epoch 985/5000] loss: 0.86653\n",
      "[Epoch 986/5000] loss: 0.63438\n",
      "[Epoch 987/5000] loss: 0.64841\n",
      "[Epoch 988/5000] loss: 1.13141\n",
      "[Epoch 989/5000] loss: 0.98232\n",
      "[Epoch 990/5000] loss: 0.95371\n",
      "[Epoch 991/5000] loss: 0.79210\n",
      "[Epoch 992/5000] loss: 0.96788\n",
      "[Epoch 993/5000] loss: 0.96940\n",
      "[Epoch 994/5000] loss: 0.74446\n",
      "[Epoch 995/5000] loss: 0.65633\n",
      "[Epoch 996/5000] loss: 1.47425\n",
      "[Epoch 997/5000] loss: 0.49776\n",
      "[Epoch 998/5000] loss: 0.81362\n",
      "[Epoch 999/5000] loss: 1.74676\n",
      "[Epoch 1000/5000] loss: 1.06532\n",
      "[Epoch 1001/5000] loss: 0.68084\n",
      "[Epoch 1002/5000] loss: 1.74043\n",
      "[Epoch 1003/5000] loss: 1.04912\n",
      "[Epoch 1004/5000] loss: 1.28457\n",
      "[Epoch 1005/5000] loss: 0.68774\n",
      "[Epoch 1006/5000] loss: 1.43439\n",
      "[Epoch 1007/5000] loss: 0.96130\n",
      "[Epoch 1008/5000] loss: 0.94974\n",
      "[Epoch 1009/5000] loss: 0.76415\n",
      "[Epoch 1010/5000] loss: 0.63773\n",
      "[Epoch 1011/5000] loss: 1.11613\n",
      "[Epoch 1012/5000] loss: 1.23899\n",
      "[Epoch 1013/5000] loss: 0.83753\n",
      "[Epoch 1014/5000] loss: 1.34443\n",
      "[Epoch 1015/5000] loss: 1.01420\n",
      "[Epoch 1016/5000] loss: 1.24417\n",
      "[Epoch 1017/5000] loss: 3.32156\n",
      "[Epoch 1018/5000] loss: 0.63422\n",
      "[Epoch 1019/5000] loss: 1.05760\n",
      "[Epoch 1020/5000] loss: 1.03391\n",
      "[Epoch 1021/5000] loss: 1.29915\n",
      "[Epoch 1022/5000] loss: 0.95072\n",
      "[Epoch 1023/5000] loss: 1.07351\n",
      "[Epoch 1024/5000] loss: 0.97965\n",
      "[Epoch 1025/5000] loss: 0.46967\n",
      "[Epoch 1026/5000] loss: 1.28832\n",
      "[Epoch 1027/5000] loss: 0.57967\n",
      "[Epoch 1028/5000] loss: 1.00162\n",
      "[Epoch 1029/5000] loss: 0.83431\n",
      "[Epoch 1030/5000] loss: 0.77109\n",
      "[Epoch 1031/5000] loss: 3.64719\n",
      "[Epoch 1032/5000] loss: 0.71736\n",
      "[Epoch 1033/5000] loss: 1.25680\n",
      "[Epoch 1034/5000] loss: 1.15804\n",
      "[Epoch 1035/5000] loss: 0.94187\n",
      "[Epoch 1036/5000] loss: 0.97726\n",
      "[Epoch 1037/5000] loss: 1.13499\n",
      "[Epoch 1038/5000] loss: 1.16250\n",
      "[Epoch 1039/5000] loss: 1.07426\n",
      "[Epoch 1040/5000] loss: 0.62941\n",
      "[Epoch 1041/5000] loss: 0.76986\n",
      "[Epoch 1042/5000] loss: 0.69663\n",
      "[Epoch 1043/5000] loss: 1.35217\n",
      "[Epoch 1044/5000] loss: 0.39706\n",
      "[Epoch 1045/5000] loss: 1.13286\n",
      "[Epoch 1046/5000] loss: 1.31661\n",
      "[Epoch 1047/5000] loss: 0.97204\n",
      "[Epoch 1048/5000] loss: 0.97279\n",
      "[Epoch 1049/5000] loss: 1.39863\n",
      "[Epoch 1050/5000] loss: 0.82251\n",
      "[Epoch 1051/5000] loss: 1.00668\n",
      "[Epoch 1052/5000] loss: 0.92829\n",
      "[Epoch 1053/5000] loss: 0.71178\n",
      "[Epoch 1054/5000] loss: 0.64231\n",
      "[Epoch 1055/5000] loss: 0.66859\n",
      "[Epoch 1056/5000] loss: 0.52390\n",
      "[Epoch 1057/5000] loss: 1.22687\n",
      "[Epoch 1058/5000] loss: 0.79699\n",
      "[Epoch 1059/5000] loss: 0.40048\n",
      "[Epoch 1060/5000] loss: 1.34593\n",
      "[Epoch 1061/5000] loss: 1.12214\n",
      "[Epoch 1062/5000] loss: 3.25444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1063/5000] loss: 0.87099\n",
      "[Epoch 1064/5000] loss: 0.79327\n",
      "[Epoch 1065/5000] loss: 0.69318\n",
      "[Epoch 1066/5000] loss: 0.69804\n",
      "[Epoch 1067/5000] loss: 1.09311\n",
      "[Epoch 1068/5000] loss: 0.99721\n",
      "[Epoch 1069/5000] loss: 1.00235\n",
      "[Epoch 1070/5000] loss: 1.45161\n",
      "[Epoch 1071/5000] loss: 0.92711\n",
      "[Epoch 1072/5000] loss: 0.92694\n",
      "[Epoch 1073/5000] loss: 0.70173\n",
      "[Epoch 1074/5000] loss: 1.02891\n",
      "[Epoch 1075/5000] loss: 1.63221\n",
      "[Epoch 1076/5000] loss: 0.71181\n",
      "[Epoch 1077/5000] loss: 0.88774\n",
      "[Epoch 1078/5000] loss: 1.03401\n",
      "[Epoch 1079/5000] loss: 0.76579\n",
      "[Epoch 1080/5000] loss: 1.16805\n",
      "[Epoch 1081/5000] loss: 0.99349\n",
      "[Epoch 1082/5000] loss: 1.02417\n",
      "[Epoch 1083/5000] loss: 1.08922\n",
      "[Epoch 1084/5000] loss: 1.23986\n",
      "[Epoch 1085/5000] loss: 1.06449\n",
      "[Epoch 1086/5000] loss: 0.54442\n",
      "[Epoch 1087/5000] loss: 0.88662\n",
      "[Epoch 1088/5000] loss: 1.36307\n",
      "[Epoch 1089/5000] loss: 0.79586\n",
      "[Epoch 1090/5000] loss: 0.53014\n",
      "[Epoch 1091/5000] loss: 0.88395\n",
      "[Epoch 1092/5000] loss: 0.98887\n",
      "[Epoch 1093/5000] loss: 1.04018\n",
      "[Epoch 1094/5000] loss: 0.85601\n",
      "[Epoch 1095/5000] loss: 1.18081\n",
      "[Epoch 1096/5000] loss: 0.61387\n",
      "[Epoch 1097/5000] loss: 1.12831\n",
      "[Epoch 1098/5000] loss: 0.58936\n",
      "[Epoch 1099/5000] loss: 0.81822\n",
      "[Epoch 1100/5000] loss: 1.20798\n",
      "[Epoch 1101/5000] loss: 1.18016\n",
      "[Epoch 1102/5000] loss: 1.16077\n",
      "[Epoch 1103/5000] loss: 1.14701\n",
      "[Epoch 1104/5000] loss: 1.09978\n",
      "[Epoch 1105/5000] loss: 0.92554\n",
      "[Epoch 1106/5000] loss: 0.89253\n",
      "[Epoch 1107/5000] loss: 0.73202\n",
      "[Epoch 1108/5000] loss: 0.98992\n",
      "[Epoch 1109/5000] loss: 0.99295\n",
      "[Epoch 1110/5000] loss: 0.53561\n",
      "[Epoch 1111/5000] loss: 1.48695\n",
      "[Epoch 1112/5000] loss: 0.73722\n",
      "[Epoch 1113/5000] loss: 1.33693\n",
      "[Epoch 1114/5000] loss: 0.91367\n",
      "[Epoch 1115/5000] loss: 1.52488\n",
      "[Epoch 1116/5000] loss: 0.63941\n",
      "[Epoch 1117/5000] loss: 0.41819\n",
      "[Epoch 1118/5000] loss: 0.79060\n",
      "[Epoch 1119/5000] loss: 0.69891\n",
      "[Epoch 1120/5000] loss: 0.55771\n",
      "[Epoch 1121/5000] loss: 1.48127\n",
      "[Epoch 1122/5000] loss: 1.06091\n",
      "[Epoch 1123/5000] loss: 0.66294\n",
      "[Epoch 1124/5000] loss: 1.10557\n",
      "[Epoch 1125/5000] loss: 0.62789\n",
      "[Epoch 1126/5000] loss: 0.81770\n",
      "[Epoch 1127/5000] loss: 0.81545\n",
      "[Epoch 1128/5000] loss: 0.77902\n",
      "[Epoch 1129/5000] loss: 0.56451\n",
      "[Epoch 1130/5000] loss: 1.05003\n",
      "[Epoch 1131/5000] loss: 1.08625\n",
      "[Epoch 1132/5000] loss: 0.79656\n",
      "[Epoch 1133/5000] loss: 1.02821\n",
      "[Epoch 1134/5000] loss: 0.51009\n",
      "[Epoch 1135/5000] loss: 0.87119\n",
      "[Epoch 1136/5000] loss: 0.71415\n",
      "[Epoch 1137/5000] loss: 0.82451\n",
      "[Epoch 1138/5000] loss: 0.94762\n",
      "[Epoch 1139/5000] loss: 0.79095\n",
      "[Epoch 1140/5000] loss: 1.05581\n",
      "[Epoch 1141/5000] loss: 0.99356\n",
      "[Epoch 1142/5000] loss: 1.36895\n",
      "[Epoch 1143/5000] loss: 0.87734\n",
      "[Epoch 1144/5000] loss: 0.67858\n",
      "[Epoch 1145/5000] loss: 0.80447\n",
      "[Epoch 1146/5000] loss: 0.69815\n",
      "[Epoch 1147/5000] loss: 0.70560\n",
      "[Epoch 1148/5000] loss: 0.49570\n",
      "[Epoch 1149/5000] loss: 2.90154\n",
      "[Epoch 1150/5000] loss: 1.13432\n",
      "[Epoch 1151/5000] loss: 0.78010\n",
      "[Epoch 1152/5000] loss: 1.25811\n",
      "[Epoch 1153/5000] loss: 0.83392\n",
      "[Epoch 1154/5000] loss: 1.13082\n",
      "[Epoch 1155/5000] loss: 1.16944\n",
      "[Epoch 1156/5000] loss: 1.05480\n",
      "[Epoch 1157/5000] loss: 0.71166\n",
      "[Epoch 1158/5000] loss: 1.18417\n",
      "[Epoch 1159/5000] loss: 1.29240\n",
      "[Epoch 1160/5000] loss: 0.98335\n",
      "[Epoch 1161/5000] loss: 0.88946\n",
      "[Epoch 1162/5000] loss: 0.86521\n",
      "[Epoch 1163/5000] loss: 0.85862\n",
      "[Epoch 1164/5000] loss: 1.61040\n",
      "[Epoch 1165/5000] loss: 0.90444\n",
      "[Epoch 1166/5000] loss: 0.76335\n",
      "[Epoch 1167/5000] loss: 1.22534\n",
      "[Epoch 1168/5000] loss: 0.60573\n",
      "[Epoch 1169/5000] loss: 1.30530\n",
      "[Epoch 1170/5000] loss: 0.98534\n",
      "[Epoch 1171/5000] loss: 0.60799\n",
      "[Epoch 1172/5000] loss: 3.42345\n",
      "[Epoch 1173/5000] loss: 1.30596\n",
      "[Epoch 1174/5000] loss: 0.86409\n",
      "[Epoch 1175/5000] loss: 1.29985\n",
      "[Epoch 1176/5000] loss: 1.26525\n",
      "[Epoch 1177/5000] loss: 1.36037\n",
      "[Epoch 1178/5000] loss: 0.66930\n",
      "[Epoch 1179/5000] loss: 1.33873\n",
      "[Epoch 1180/5000] loss: 1.33313\n",
      "[Epoch 1181/5000] loss: 0.80312\n",
      "[Epoch 1182/5000] loss: 0.73879\n",
      "[Epoch 1183/5000] loss: 0.98571\n",
      "[Epoch 1184/5000] loss: 1.44923\n",
      "[Epoch 1185/5000] loss: 2.01939\n",
      "[Epoch 1186/5000] loss: 0.84227\n",
      "[Epoch 1187/5000] loss: 1.02790\n",
      "[Epoch 1188/5000] loss: 0.73780\n",
      "[Epoch 1189/5000] loss: 0.84677\n",
      "[Epoch 1190/5000] loss: 1.02063\n",
      "[Epoch 1191/5000] loss: 0.80397\n",
      "[Epoch 1192/5000] loss: 3.52940\n",
      "[Epoch 1193/5000] loss: 1.00776\n",
      "[Epoch 1194/5000] loss: 0.94272\n",
      "[Epoch 1195/5000] loss: 0.51742\n",
      "[Epoch 1196/5000] loss: 0.49510\n",
      "[Epoch 1197/5000] loss: 1.52165\n",
      "[Epoch 1198/5000] loss: 1.01729\n",
      "[Epoch 1199/5000] loss: 0.78475\n",
      "[Epoch 1200/5000] loss: 0.90703\n",
      "[Epoch 1201/5000] loss: 0.91002\n",
      "[Epoch 1202/5000] loss: 1.13103\n",
      "[Epoch 1203/5000] loss: 0.62336\n",
      "[Epoch 1204/5000] loss: 0.58410\n",
      "[Epoch 1205/5000] loss: 1.07968\n",
      "[Epoch 1206/5000] loss: 0.88984\n",
      "[Epoch 1207/5000] loss: 1.05467\n",
      "[Epoch 1208/5000] loss: 1.21223\n",
      "[Epoch 1209/5000] loss: 0.60274\n",
      "[Epoch 1210/5000] loss: 1.08265\n",
      "[Epoch 1211/5000] loss: 0.86993\n",
      "[Epoch 1212/5000] loss: 1.20578\n",
      "[Epoch 1213/5000] loss: 0.79392\n",
      "[Epoch 1214/5000] loss: 0.79467\n",
      "[Epoch 1215/5000] loss: 0.73915\n",
      "[Epoch 1216/5000] loss: 0.86359\n",
      "[Epoch 1217/5000] loss: 1.11221\n",
      "[Epoch 1218/5000] loss: 0.52960\n",
      "[Epoch 1219/5000] loss: 0.70008\n",
      "[Epoch 1220/5000] loss: 0.94941\n",
      "[Epoch 1221/5000] loss: 0.70927\n",
      "[Epoch 1222/5000] loss: 0.78909\n",
      "[Epoch 1223/5000] loss: 0.81073\n",
      "[Epoch 1224/5000] loss: 0.88090\n",
      "[Epoch 1225/5000] loss: 0.73409\n",
      "[Epoch 1226/5000] loss: 0.71858\n",
      "[Epoch 1227/5000] loss: 1.16934\n",
      "[Epoch 1228/5000] loss: 0.94228\n",
      "[Epoch 1229/5000] loss: 0.99463\n",
      "[Epoch 1230/5000] loss: 1.24486\n",
      "[Epoch 1231/5000] loss: 1.39309\n",
      "[Epoch 1232/5000] loss: 0.74709\n",
      "[Epoch 1233/5000] loss: 1.09342\n",
      "[Epoch 1234/5000] loss: 1.04965\n",
      "[Epoch 1235/5000] loss: 1.03435\n",
      "[Epoch 1236/5000] loss: 0.98499\n",
      "[Epoch 1237/5000] loss: 1.03531\n",
      "[Epoch 1238/5000] loss: 1.26446\n",
      "[Epoch 1239/5000] loss: 1.21207\n",
      "[Epoch 1240/5000] loss: 0.86134\n",
      "[Epoch 1241/5000] loss: 1.22042\n",
      "[Epoch 1242/5000] loss: 0.91623\n",
      "[Epoch 1243/5000] loss: 1.38619\n",
      "[Epoch 1244/5000] loss: 0.91448\n",
      "[Epoch 1245/5000] loss: 1.02719\n",
      "[Epoch 1246/5000] loss: 1.14405\n",
      "[Epoch 1247/5000] loss: 1.28149\n",
      "[Epoch 1248/5000] loss: 1.14236\n",
      "[Epoch 1249/5000] loss: 0.91603\n",
      "[Epoch 1250/5000] loss: 1.27090\n",
      "[Epoch 1251/5000] loss: 0.83538\n",
      "[Epoch 1252/5000] loss: 1.35731\n",
      "[Epoch 1253/5000] loss: 0.43450\n",
      "[Epoch 1254/5000] loss: 1.31495\n",
      "[Epoch 1255/5000] loss: 0.87656\n",
      "[Epoch 1256/5000] loss: 0.83536\n",
      "[Epoch 1257/5000] loss: 0.90862\n",
      "[Epoch 1258/5000] loss: 0.85975\n",
      "[Epoch 1259/5000] loss: 1.60221\n",
      "[Epoch 1260/5000] loss: 0.50197\n",
      "[Epoch 1261/5000] loss: 1.24031\n",
      "[Epoch 1262/5000] loss: 0.72691\n",
      "[Epoch 1263/5000] loss: 1.08415\n",
      "[Epoch 1264/5000] loss: 0.85891\n",
      "[Epoch 1265/5000] loss: 0.77312\n",
      "[Epoch 1266/5000] loss: 0.83451\n",
      "[Epoch 1267/5000] loss: 1.03577\n",
      "[Epoch 1268/5000] loss: 0.73585\n",
      "[Epoch 1269/5000] loss: 1.12765\n",
      "[Epoch 1270/5000] loss: 0.64497\n",
      "[Epoch 1271/5000] loss: 0.77040\n",
      "[Epoch 1272/5000] loss: 1.04067\n",
      "[Epoch 1273/5000] loss: 0.75661\n",
      "[Epoch 1274/5000] loss: 0.60193\n",
      "[Epoch 1275/5000] loss: 0.70784\n",
      "[Epoch 1276/5000] loss: 0.85711\n",
      "[Epoch 1277/5000] loss: 0.83661\n",
      "[Epoch 1278/5000] loss: 0.54470\n",
      "[Epoch 1279/5000] loss: 0.64478\n",
      "[Epoch 1280/5000] loss: 0.97454\n",
      "[Epoch 1281/5000] loss: 0.69075\n",
      "[Epoch 1282/5000] loss: 0.56387\n",
      "[Epoch 1283/5000] loss: 0.81901\n",
      "[Epoch 1284/5000] loss: 0.99205\n",
      "[Epoch 1285/5000] loss: 0.56202\n",
      "[Epoch 1286/5000] loss: 0.93087\n",
      "[Epoch 1287/5000] loss: 0.76351\n",
      "[Epoch 1288/5000] loss: 0.78732\n",
      "[Epoch 1289/5000] loss: 1.72004\n",
      "[Epoch 1290/5000] loss: 1.25667\n",
      "[Epoch 1291/5000] loss: 1.21715\n",
      "[Epoch 1292/5000] loss: 0.68938\n",
      "[Epoch 1293/5000] loss: 1.27453\n",
      "[Epoch 1294/5000] loss: 0.96443\n",
      "[Epoch 1295/5000] loss: 1.13576\n",
      "[Epoch 1296/5000] loss: 0.90548\n",
      "[Epoch 1297/5000] loss: 0.75533\n",
      "[Epoch 1298/5000] loss: 0.88295\n",
      "[Epoch 1299/5000] loss: 0.84418\n",
      "[Epoch 1300/5000] loss: 1.52868\n",
      "[Epoch 1301/5000] loss: 1.21881\n",
      "[Epoch 1302/5000] loss: 0.92995\n",
      "[Epoch 1303/5000] loss: 0.62147\n",
      "[Epoch 1304/5000] loss: 0.96162\n",
      "[Epoch 1305/5000] loss: 0.89818\n",
      "[Epoch 1306/5000] loss: 1.00511\n",
      "[Epoch 1307/5000] loss: 0.75363\n",
      "[Epoch 1308/5000] loss: 0.65708\n",
      "[Epoch 1309/5000] loss: 0.78681\n",
      "[Epoch 1310/5000] loss: 0.85652\n",
      "[Epoch 1311/5000] loss: 0.66028\n",
      "[Epoch 1312/5000] loss: 0.58035\n",
      "[Epoch 1313/5000] loss: 1.08891\n",
      "[Epoch 1314/5000] loss: 1.01747\n",
      "[Epoch 1315/5000] loss: 1.03103\n",
      "[Epoch 1316/5000] loss: 3.05983\n",
      "[Epoch 1317/5000] loss: 0.80234\n",
      "[Epoch 1318/5000] loss: 0.93439\n",
      "[Epoch 1319/5000] loss: 0.77116\n",
      "[Epoch 1320/5000] loss: 0.76599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1321/5000] loss: 3.00198\n",
      "[Epoch 1322/5000] loss: 1.31925\n",
      "[Epoch 1323/5000] loss: 1.09756\n",
      "[Epoch 1324/5000] loss: 1.29106\n",
      "[Epoch 1325/5000] loss: 0.56699\n",
      "[Epoch 1326/5000] loss: 0.70615\n",
      "[Epoch 1327/5000] loss: 1.31061\n",
      "[Epoch 1328/5000] loss: 1.90283\n",
      "[Epoch 1329/5000] loss: 0.96738\n",
      "[Epoch 1330/5000] loss: 0.88626\n",
      "[Epoch 1331/5000] loss: 0.66516\n",
      "[Epoch 1332/5000] loss: 1.20347\n",
      "[Epoch 1333/5000] loss: 1.27136\n",
      "[Epoch 1334/5000] loss: 0.72787\n",
      "[Epoch 1335/5000] loss: 0.65303\n",
      "[Epoch 1336/5000] loss: 0.82224\n",
      "[Epoch 1337/5000] loss: 0.71659\n",
      "[Epoch 1338/5000] loss: 0.98691\n",
      "[Epoch 1339/5000] loss: 1.18518\n",
      "[Epoch 1340/5000] loss: 1.82367\n",
      "[Epoch 1341/5000] loss: 0.81316\n",
      "[Epoch 1342/5000] loss: 0.57519\n",
      "[Epoch 1343/5000] loss: 0.75228\n",
      "[Epoch 1344/5000] loss: 0.65357\n",
      "[Epoch 1345/5000] loss: 1.05845\n",
      "[Epoch 1346/5000] loss: 0.90190\n",
      "[Epoch 1347/5000] loss: 0.63029\n",
      "[Epoch 1348/5000] loss: 0.57848\n",
      "[Epoch 1349/5000] loss: 0.86823\n",
      "[Epoch 1350/5000] loss: 0.66867\n",
      "[Epoch 1351/5000] loss: 0.98151\n",
      "[Epoch 1352/5000] loss: 1.23414\n",
      "[Epoch 1353/5000] loss: 1.24922\n",
      "[Epoch 1354/5000] loss: 1.09343\n",
      "[Epoch 1355/5000] loss: 1.21151\n",
      "[Epoch 1356/5000] loss: 3.35090\n",
      "[Epoch 1357/5000] loss: 0.96035\n",
      "[Epoch 1358/5000] loss: 1.00987\n",
      "[Epoch 1359/5000] loss: 0.63422\n",
      "[Epoch 1360/5000] loss: 0.90119\n",
      "[Epoch 1361/5000] loss: 1.07938\n",
      "[Epoch 1362/5000] loss: 0.91036\n",
      "[Epoch 1363/5000] loss: 1.32187\n",
      "[Epoch 1364/5000] loss: 0.63136\n",
      "[Epoch 1365/5000] loss: 0.63677\n",
      "[Epoch 1366/5000] loss: 1.45218\n",
      "[Epoch 1367/5000] loss: 0.70678\n",
      "[Epoch 1368/5000] loss: 0.77323\n",
      "[Epoch 1369/5000] loss: 0.61416\n",
      "[Epoch 1370/5000] loss: 0.78943\n",
      "[Epoch 1371/5000] loss: 0.64763\n",
      "[Epoch 1372/5000] loss: 0.56627\n",
      "[Epoch 1373/5000] loss: 0.66401\n",
      "[Epoch 1374/5000] loss: 0.60121\n",
      "[Epoch 1375/5000] loss: 3.04347\n",
      "[Epoch 1376/5000] loss: 0.83885\n",
      "[Epoch 1377/5000] loss: 0.90792\n",
      "[Epoch 1378/5000] loss: 0.69665\n",
      "[Epoch 1379/5000] loss: 0.43368\n",
      "[Epoch 1380/5000] loss: 1.39993\n",
      "[Epoch 1381/5000] loss: 1.15360\n",
      "[Epoch 1382/5000] loss: 0.96646\n",
      "[Epoch 1383/5000] loss: 1.21759\n",
      "[Epoch 1384/5000] loss: 1.02823\n",
      "[Epoch 1385/5000] loss: 0.63106\n",
      "[Epoch 1386/5000] loss: 1.06313\n",
      "[Epoch 1387/5000] loss: 0.95156\n",
      "[Epoch 1388/5000] loss: 1.00466\n",
      "[Epoch 1389/5000] loss: 1.63690\n",
      "[Epoch 1390/5000] loss: 0.63388\n",
      "[Epoch 1391/5000] loss: 0.80089\n",
      "[Epoch 1392/5000] loss: 0.84429\n",
      "[Epoch 1393/5000] loss: 1.07623\n",
      "[Epoch 1394/5000] loss: 1.01610\n",
      "[Epoch 1395/5000] loss: 0.56762\n",
      "[Epoch 1396/5000] loss: 0.85295\n",
      "[Epoch 1397/5000] loss: 0.82292\n",
      "[Epoch 1398/5000] loss: 1.28756\n",
      "[Epoch 1399/5000] loss: 0.74874\n",
      "[Epoch 1400/5000] loss: 0.99336\n",
      "[Epoch 1401/5000] loss: 0.90614\n",
      "[Epoch 1402/5000] loss: 0.99465\n",
      "[Epoch 1403/5000] loss: 1.01452\n",
      "[Epoch 1404/5000] loss: 1.38885\n",
      "[Epoch 1405/5000] loss: 0.42737\n",
      "[Epoch 1406/5000] loss: 1.09626\n",
      "[Epoch 1407/5000] loss: 0.70362\n",
      "[Epoch 1408/5000] loss: 1.14888\n",
      "[Epoch 1409/5000] loss: 0.63244\n",
      "[Epoch 1410/5000] loss: 0.38534\n",
      "[Epoch 1411/5000] loss: 0.69581\n",
      "[Epoch 1412/5000] loss: 0.68106\n",
      "[Epoch 1413/5000] loss: 0.62613\n",
      "[Epoch 1414/5000] loss: 1.07716\n",
      "[Epoch 1415/5000] loss: 1.35405\n",
      "[Epoch 1416/5000] loss: 0.53601\n",
      "[Epoch 1417/5000] loss: 0.97944\n",
      "[Epoch 1418/5000] loss: 0.76890\n",
      "[Epoch 1419/5000] loss: 1.12744\n",
      "[Epoch 1420/5000] loss: 0.57988\n",
      "[Epoch 1421/5000] loss: 1.23479\n",
      "[Epoch 1422/5000] loss: 1.12372\n",
      "[Epoch 1423/5000] loss: 0.36890\n",
      "[Epoch 1424/5000] loss: 1.14103\n",
      "[Epoch 1425/5000] loss: 0.91486\n",
      "[Epoch 1426/5000] loss: 1.38219\n",
      "[Epoch 1427/5000] loss: 0.98884\n",
      "[Epoch 1428/5000] loss: 0.97556\n",
      "[Epoch 1429/5000] loss: 0.97751\n",
      "[Epoch 1430/5000] loss: 0.92991\n",
      "[Epoch 1431/5000] loss: 0.57249\n",
      "[Epoch 1432/5000] loss: 0.58287\n",
      "[Epoch 1433/5000] loss: 0.97490\n",
      "[Epoch 1434/5000] loss: 0.67621\n",
      "[Epoch 1435/5000] loss: 1.06185\n",
      "[Epoch 1436/5000] loss: 0.68497\n",
      "[Epoch 1437/5000] loss: 1.15971\n",
      "[Epoch 1438/5000] loss: 0.89281\n",
      "[Epoch 1439/5000] loss: 0.63769\n",
      "[Epoch 1440/5000] loss: 1.20600\n",
      "[Epoch 1441/5000] loss: 0.38421\n",
      "[Epoch 1442/5000] loss: 0.89786\n",
      "[Epoch 1443/5000] loss: 0.90859\n",
      "[Epoch 1444/5000] loss: 0.74025\n",
      "[Epoch 1445/5000] loss: 0.98699\n",
      "[Epoch 1446/5000] loss: 0.91421\n",
      "[Epoch 1447/5000] loss: 1.41324\n",
      "[Epoch 1448/5000] loss: 0.94428\n",
      "[Epoch 1449/5000] loss: 1.03790\n",
      "[Epoch 1450/5000] loss: 0.61733\n",
      "[Epoch 1451/5000] loss: 1.01618\n",
      "[Epoch 1452/5000] loss: 1.41316\n",
      "[Epoch 1453/5000] loss: 0.67957\n",
      "[Epoch 1454/5000] loss: 0.60246\n",
      "[Epoch 1455/5000] loss: 1.03630\n",
      "[Epoch 1456/5000] loss: 0.35697\n",
      "[Epoch 1457/5000] loss: 0.85486\n",
      "[Epoch 1458/5000] loss: 1.34784\n",
      "[Epoch 1459/5000] loss: 0.76147\n",
      "[Epoch 1460/5000] loss: 0.96282\n",
      "[Epoch 1461/5000] loss: 0.84856\n",
      "[Epoch 1462/5000] loss: 0.90950\n",
      "[Epoch 1463/5000] loss: 0.97432\n",
      "[Epoch 1464/5000] loss: 0.75057\n",
      "[Epoch 1465/5000] loss: 0.90873\n",
      "[Epoch 1466/5000] loss: 1.38643\n",
      "[Epoch 1467/5000] loss: 1.39326\n",
      "[Epoch 1468/5000] loss: 1.56397\n",
      "[Epoch 1469/5000] loss: 1.33181\n",
      "[Epoch 1470/5000] loss: 0.81548\n",
      "[Epoch 1471/5000] loss: 0.89746\n",
      "[Epoch 1472/5000] loss: 1.11823\n",
      "[Epoch 1473/5000] loss: 0.80551\n",
      "[Epoch 1474/5000] loss: 1.09019\n",
      "[Epoch 1475/5000] loss: 1.01755\n",
      "[Epoch 1476/5000] loss: 1.02908\n",
      "[Epoch 1477/5000] loss: 0.92747\n",
      "[Epoch 1478/5000] loss: 0.89208\n",
      "[Epoch 1479/5000] loss: 0.90590\n",
      "[Epoch 1480/5000] loss: 1.03931\n",
      "[Epoch 1481/5000] loss: 1.32675\n",
      "[Epoch 1482/5000] loss: 1.02523\n",
      "[Epoch 1483/5000] loss: 0.88892\n",
      "[Epoch 1484/5000] loss: 0.73071\n",
      "[Epoch 1485/5000] loss: 0.92588\n",
      "[Epoch 1486/5000] loss: 0.68682\n",
      "[Epoch 1487/5000] loss: 0.60314\n",
      "[Epoch 1488/5000] loss: 1.02595\n",
      "[Epoch 1489/5000] loss: 0.51364\n",
      "[Epoch 1490/5000] loss: 0.92622\n",
      "[Epoch 1491/5000] loss: 1.30613\n",
      "[Epoch 1492/5000] loss: 1.11179\n",
      "[Epoch 1493/5000] loss: 0.71820\n",
      "[Epoch 1494/5000] loss: 0.61349\n",
      "[Epoch 1495/5000] loss: 0.88291\n",
      "[Epoch 1496/5000] loss: 0.93064\n",
      "[Epoch 1497/5000] loss: 0.83714\n",
      "[Epoch 1498/5000] loss: 0.73250\n",
      "[Epoch 1499/5000] loss: 0.81011\n",
      "[Epoch 1500/5000] loss: 0.91098\n",
      "[Epoch 1501/5000] loss: 0.81865\n",
      "[Epoch 1502/5000] loss: 0.89336\n",
      "[Epoch 1503/5000] loss: 0.77131\n",
      "[Epoch 1504/5000] loss: 1.02499\n",
      "[Epoch 1505/5000] loss: 0.87193\n",
      "[Epoch 1506/5000] loss: 0.75855\n",
      "[Epoch 1507/5000] loss: 0.78752\n",
      "[Epoch 1508/5000] loss: 1.17229\n",
      "[Epoch 1509/5000] loss: 1.35487\n",
      "[Epoch 1510/5000] loss: 1.15933\n",
      "[Epoch 1511/5000] loss: 0.74648\n",
      "[Epoch 1512/5000] loss: 0.84915\n",
      "[Epoch 1513/5000] loss: 0.82833\n",
      "[Epoch 1514/5000] loss: 0.69820\n",
      "[Epoch 1515/5000] loss: 3.50279\n",
      "[Epoch 1516/5000] loss: 1.03554\n",
      "[Epoch 1517/5000] loss: 0.69648\n",
      "[Epoch 1518/5000] loss: 0.98913\n",
      "[Epoch 1519/5000] loss: 0.62697\n",
      "[Epoch 1520/5000] loss: 1.10755\n",
      "[Epoch 1521/5000] loss: 1.22145\n",
      "[Epoch 1522/5000] loss: 1.02237\n",
      "[Epoch 1523/5000] loss: 0.85483\n",
      "[Epoch 1524/5000] loss: 1.40606\n",
      "[Epoch 1525/5000] loss: 0.79623\n",
      "[Epoch 1526/5000] loss: 0.61268\n",
      "[Epoch 1527/5000] loss: 0.77901\n",
      "[Epoch 1528/5000] loss: 0.75298\n",
      "[Epoch 1529/5000] loss: 0.87228\n",
      "[Epoch 1530/5000] loss: 0.80704\n",
      "[Epoch 1531/5000] loss: 0.83420\n",
      "[Epoch 1532/5000] loss: 1.05844\n",
      "[Epoch 1533/5000] loss: 1.39043\n",
      "[Epoch 1534/5000] loss: 1.06833\n",
      "[Epoch 1535/5000] loss: 0.89327\n",
      "[Epoch 1536/5000] loss: 3.27223\n",
      "[Epoch 1537/5000] loss: 0.81964\n",
      "[Epoch 1538/5000] loss: 0.75976\n",
      "[Epoch 1539/5000] loss: 1.32773\n",
      "[Epoch 1540/5000] loss: 0.57936\n",
      "[Epoch 1541/5000] loss: 3.82999\n",
      "[Epoch 1542/5000] loss: 0.51008\n",
      "[Epoch 1543/5000] loss: 0.80769\n",
      "[Epoch 1544/5000] loss: 0.72389\n",
      "[Epoch 1545/5000] loss: 3.20341\n",
      "[Epoch 1546/5000] loss: 0.95218\n",
      "[Epoch 1547/5000] loss: 0.84532\n",
      "[Epoch 1548/5000] loss: 1.01522\n",
      "[Epoch 1549/5000] loss: 1.35195\n",
      "[Epoch 1550/5000] loss: 0.73165\n",
      "[Epoch 1551/5000] loss: 1.02007\n",
      "[Epoch 1552/5000] loss: 1.01344\n",
      "[Epoch 1553/5000] loss: 0.77931\n",
      "[Epoch 1554/5000] loss: 0.70662\n",
      "[Epoch 1555/5000] loss: 1.15023\n",
      "[Epoch 1556/5000] loss: 1.03248\n",
      "[Epoch 1557/5000] loss: 1.25054\n",
      "[Epoch 1558/5000] loss: 1.29517\n",
      "[Epoch 1559/5000] loss: 0.68041\n",
      "[Epoch 1560/5000] loss: 0.51641\n",
      "[Epoch 1561/5000] loss: 1.20278\n",
      "[Epoch 1562/5000] loss: 3.74766\n",
      "[Epoch 1563/5000] loss: 0.80444\n",
      "[Epoch 1564/5000] loss: 0.72419\n",
      "[Epoch 1565/5000] loss: 1.54901\n",
      "[Epoch 1566/5000] loss: 3.20224\n",
      "[Epoch 1567/5000] loss: 0.64190\n",
      "[Epoch 1568/5000] loss: 0.83261\n",
      "[Epoch 1569/5000] loss: 1.11499\n",
      "[Epoch 1570/5000] loss: 1.04084\n",
      "[Epoch 1571/5000] loss: 1.00064\n",
      "[Epoch 1572/5000] loss: 0.52713\n",
      "[Epoch 1573/5000] loss: 0.99905\n",
      "[Epoch 1574/5000] loss: 1.19338\n",
      "[Epoch 1575/5000] loss: 0.94313\n",
      "[Epoch 1576/5000] loss: 0.68922\n",
      "[Epoch 1577/5000] loss: 0.62675\n",
      "[Epoch 1578/5000] loss: 1.20410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1579/5000] loss: 0.65288\n",
      "[Epoch 1580/5000] loss: 1.06749\n",
      "[Epoch 1581/5000] loss: 0.89631\n",
      "[Epoch 1582/5000] loss: 1.23835\n",
      "[Epoch 1583/5000] loss: 0.84962\n",
      "[Epoch 1584/5000] loss: 0.62536\n",
      "[Epoch 1585/5000] loss: 1.24577\n",
      "[Epoch 1586/5000] loss: 1.07768\n",
      "[Epoch 1587/5000] loss: 0.77673\n",
      "[Epoch 1588/5000] loss: 0.91082\n",
      "[Epoch 1589/5000] loss: 0.88380\n",
      "[Epoch 1590/5000] loss: 0.93341\n",
      "[Epoch 1591/5000] loss: 0.85935\n",
      "[Epoch 1592/5000] loss: 0.93807\n",
      "[Epoch 1593/5000] loss: 1.05265\n",
      "[Epoch 1594/5000] loss: 0.76211\n",
      "[Epoch 1595/5000] loss: 1.23311\n",
      "[Epoch 1596/5000] loss: 1.06498\n",
      "[Epoch 1597/5000] loss: 1.04121\n",
      "[Epoch 1598/5000] loss: 0.93395\n",
      "[Epoch 1599/5000] loss: 0.90884\n",
      "[Epoch 1600/5000] loss: 0.44369\n",
      "[Epoch 1601/5000] loss: 1.19251\n",
      "[Epoch 1602/5000] loss: 0.87333\n",
      "[Epoch 1603/5000] loss: 1.15798\n",
      "[Epoch 1604/5000] loss: 1.58265\n",
      "[Epoch 1605/5000] loss: 0.96545\n",
      "[Epoch 1606/5000] loss: 0.91618\n",
      "[Epoch 1607/5000] loss: 0.84646\n",
      "[Epoch 1608/5000] loss: 0.72675\n",
      "[Epoch 1609/5000] loss: 0.93953\n",
      "[Epoch 1610/5000] loss: 0.79734\n",
      "[Epoch 1611/5000] loss: 0.76094\n",
      "[Epoch 1612/5000] loss: 3.63741\n",
      "[Epoch 1613/5000] loss: 0.54622\n",
      "[Epoch 1614/5000] loss: 1.15775\n",
      "[Epoch 1615/5000] loss: 1.08724\n",
      "[Epoch 1616/5000] loss: 0.96965\n",
      "[Epoch 1617/5000] loss: 1.17169\n",
      "[Epoch 1618/5000] loss: 0.78186\n",
      "[Epoch 1619/5000] loss: 0.90847\n",
      "[Epoch 1620/5000] loss: 0.96442\n",
      "[Epoch 1621/5000] loss: 0.45482\n",
      "[Epoch 1622/5000] loss: 1.24278\n",
      "[Epoch 1623/5000] loss: 0.94031\n",
      "[Epoch 1624/5000] loss: 0.96710\n",
      "[Epoch 1625/5000] loss: 0.76023\n",
      "[Epoch 1626/5000] loss: 0.91685\n",
      "[Epoch 1627/5000] loss: 1.03714\n",
      "[Epoch 1628/5000] loss: 1.41690\n",
      "[Epoch 1629/5000] loss: 1.18498\n",
      "[Epoch 1630/5000] loss: 1.25816\n",
      "[Epoch 1631/5000] loss: 1.35062\n",
      "[Epoch 1632/5000] loss: 0.89865\n",
      "[Epoch 1633/5000] loss: 0.94584\n",
      "[Epoch 1634/5000] loss: 1.33657\n",
      "[Epoch 1635/5000] loss: 0.83867\n",
      "[Epoch 1636/5000] loss: 0.58785\n",
      "[Epoch 1637/5000] loss: 0.80402\n",
      "[Epoch 1638/5000] loss: 0.61249\n",
      "[Epoch 1639/5000] loss: 0.47815\n",
      "[Epoch 1640/5000] loss: 0.52831\n",
      "[Epoch 1641/5000] loss: 1.03726\n",
      "[Epoch 1642/5000] loss: 1.60358\n",
      "[Epoch 1643/5000] loss: 1.16495\n",
      "[Epoch 1644/5000] loss: 0.56847\n",
      "[Epoch 1645/5000] loss: 0.81787\n",
      "[Epoch 1646/5000] loss: 1.15990\n",
      "[Epoch 1647/5000] loss: 0.55179\n",
      "[Epoch 1648/5000] loss: 1.38938\n",
      "[Epoch 1649/5000] loss: 0.88823\n",
      "[Epoch 1650/5000] loss: 0.99917\n",
      "[Epoch 1651/5000] loss: 0.60688\n",
      "[Epoch 1652/5000] loss: 0.51742\n",
      "[Epoch 1653/5000] loss: 0.63587\n",
      "[Epoch 1654/5000] loss: 0.92981\n",
      "[Epoch 1655/5000] loss: 0.62773\n",
      "[Epoch 1656/5000] loss: 1.58427\n",
      "[Epoch 1657/5000] loss: 0.93955\n",
      "[Epoch 1658/5000] loss: 1.43829\n",
      "[Epoch 1659/5000] loss: 0.92264\n",
      "[Epoch 1660/5000] loss: 0.86362\n",
      "[Epoch 1661/5000] loss: 1.41364\n",
      "[Epoch 1662/5000] loss: 1.53649\n",
      "[Epoch 1663/5000] loss: 1.66906\n",
      "[Epoch 1664/5000] loss: 0.92708\n",
      "[Epoch 1665/5000] loss: 0.97866\n",
      "[Epoch 1666/5000] loss: 1.11281\n",
      "[Epoch 1667/5000] loss: 0.49654\n",
      "[Epoch 1668/5000] loss: 0.91929\n",
      "[Epoch 1669/5000] loss: 0.87998\n",
      "[Epoch 1670/5000] loss: 0.49916\n",
      "[Epoch 1671/5000] loss: 1.34931\n",
      "[Epoch 1672/5000] loss: 0.67987\n",
      "[Epoch 1673/5000] loss: 0.62936\n",
      "[Epoch 1674/5000] loss: 0.61816\n",
      "[Epoch 1675/5000] loss: 1.38194\n",
      "[Epoch 1676/5000] loss: 1.39250\n",
      "[Epoch 1677/5000] loss: 0.63104\n",
      "[Epoch 1678/5000] loss: 0.95807\n",
      "[Epoch 1679/5000] loss: 1.06028\n",
      "[Epoch 1680/5000] loss: 0.89011\n",
      "[Epoch 1681/5000] loss: 0.76473\n",
      "[Epoch 1682/5000] loss: 1.65468\n",
      "[Epoch 1683/5000] loss: 0.82791\n",
      "[Epoch 1684/5000] loss: 0.91311\n",
      "[Epoch 1685/5000] loss: 0.65529\n",
      "[Epoch 1686/5000] loss: 1.20854\n",
      "[Epoch 1687/5000] loss: 0.80992\n",
      "[Epoch 1688/5000] loss: 1.12644\n",
      "[Epoch 1689/5000] loss: 1.38824\n",
      "[Epoch 1690/5000] loss: 0.70651\n",
      "[Epoch 1691/5000] loss: 0.60488\n",
      "[Epoch 1692/5000] loss: 0.81061\n",
      "[Epoch 1693/5000] loss: 0.83803\n",
      "[Epoch 1694/5000] loss: 3.23836\n",
      "[Epoch 1695/5000] loss: 1.04365\n",
      "[Epoch 1696/5000] loss: 1.34597\n",
      "[Epoch 1697/5000] loss: 0.89912\n",
      "[Epoch 1698/5000] loss: 0.86657\n",
      "[Epoch 1699/5000] loss: 0.83506\n",
      "[Epoch 1700/5000] loss: 1.05786\n",
      "[Epoch 1701/5000] loss: 0.71114\n",
      "[Epoch 1702/5000] loss: 0.63857\n",
      "[Epoch 1703/5000] loss: 0.54750\n",
      "[Epoch 1704/5000] loss: 0.70812\n",
      "[Epoch 1705/5000] loss: 1.21609\n",
      "[Epoch 1706/5000] loss: 1.11112\n",
      "[Epoch 1707/5000] loss: 0.41441\n",
      "[Epoch 1708/5000] loss: 0.44474\n",
      "[Epoch 1709/5000] loss: 1.41609\n",
      "[Epoch 1710/5000] loss: 0.94730\n",
      "[Epoch 1711/5000] loss: 1.48914\n",
      "[Epoch 1712/5000] loss: 1.04482\n",
      "[Epoch 1713/5000] loss: 0.84678\n",
      "[Epoch 1714/5000] loss: 1.00789\n",
      "[Epoch 1715/5000] loss: 1.09492\n",
      "[Epoch 1716/5000] loss: 1.03154\n",
      "[Epoch 1717/5000] loss: 0.62225\n",
      "[Epoch 1718/5000] loss: 0.98733\n",
      "[Epoch 1719/5000] loss: 1.39037\n",
      "[Epoch 1720/5000] loss: 0.97854\n",
      "[Epoch 1721/5000] loss: 0.49342\n",
      "[Epoch 1722/5000] loss: 3.21139\n",
      "[Epoch 1723/5000] loss: 0.60078\n",
      "[Epoch 1724/5000] loss: 0.70310\n",
      "[Epoch 1725/5000] loss: 0.55555\n",
      "[Epoch 1726/5000] loss: 1.42582\n",
      "[Epoch 1727/5000] loss: 1.24201\n",
      "[Epoch 1728/5000] loss: 1.01189\n",
      "[Epoch 1729/5000] loss: 0.67359\n",
      "[Epoch 1730/5000] loss: 0.69526\n",
      "[Epoch 1731/5000] loss: 1.06404\n",
      "[Epoch 1732/5000] loss: 0.95883\n",
      "[Epoch 1733/5000] loss: 0.59168\n",
      "[Epoch 1734/5000] loss: 1.16586\n",
      "[Epoch 1735/5000] loss: 0.95721\n",
      "[Epoch 1736/5000] loss: 1.02874\n",
      "[Epoch 1737/5000] loss: 1.23771\n",
      "[Epoch 1738/5000] loss: 0.74626\n",
      "[Epoch 1739/5000] loss: 1.30770\n",
      "[Epoch 1740/5000] loss: 0.54172\n",
      "[Epoch 1741/5000] loss: 0.37639\n",
      "[Epoch 1742/5000] loss: 0.92903\n",
      "[Epoch 1743/5000] loss: 1.03232\n",
      "[Epoch 1744/5000] loss: 0.89155\n",
      "[Epoch 1745/5000] loss: 0.59679\n",
      "[Epoch 1746/5000] loss: 2.90277\n",
      "[Epoch 1747/5000] loss: 1.58393\n",
      "[Epoch 1748/5000] loss: 1.17444\n",
      "[Epoch 1749/5000] loss: 0.70293\n",
      "[Epoch 1750/5000] loss: 0.48427\n",
      "[Epoch 1751/5000] loss: 0.98157\n",
      "[Epoch 1752/5000] loss: 1.14100\n",
      "[Epoch 1753/5000] loss: 0.68156\n",
      "[Epoch 1754/5000] loss: 0.90006\n",
      "[Epoch 1755/5000] loss: 1.38375\n",
      "[Epoch 1756/5000] loss: 1.21281\n",
      "[Epoch 1757/5000] loss: 0.78135\n",
      "[Epoch 1758/5000] loss: 1.66496\n",
      "[Epoch 1759/5000] loss: 1.10898\n",
      "[Epoch 1760/5000] loss: 0.99425\n",
      "[Epoch 1761/5000] loss: 0.93648\n",
      "[Epoch 1762/5000] loss: 0.83543\n",
      "[Epoch 1763/5000] loss: 3.57076\n",
      "[Epoch 1764/5000] loss: 0.78926\n",
      "[Epoch 1765/5000] loss: 1.15050\n",
      "[Epoch 1766/5000] loss: 0.99900\n",
      "[Epoch 1767/5000] loss: 0.79868\n",
      "[Epoch 1768/5000] loss: 1.87491\n",
      "[Epoch 1769/5000] loss: 0.79818\n",
      "[Epoch 1770/5000] loss: 1.63927\n",
      "[Epoch 1771/5000] loss: 0.75420\n",
      "[Epoch 1772/5000] loss: 3.32523\n",
      "[Epoch 1773/5000] loss: 0.69152\n",
      "[Epoch 1774/5000] loss: 0.88855\n",
      "[Epoch 1775/5000] loss: 0.69886\n",
      "[Epoch 1776/5000] loss: 1.06513\n",
      "[Epoch 1777/5000] loss: 0.77604\n",
      "[Epoch 1778/5000] loss: 0.72326\n",
      "[Epoch 1779/5000] loss: 0.77532\n",
      "[Epoch 1780/5000] loss: 0.56696\n",
      "[Epoch 1781/5000] loss: 1.30186\n",
      "[Epoch 1782/5000] loss: 1.49585\n",
      "[Epoch 1783/5000] loss: 0.93710\n",
      "[Epoch 1784/5000] loss: 1.34465\n",
      "[Epoch 1785/5000] loss: 0.66857\n",
      "[Epoch 1786/5000] loss: 1.20786\n",
      "[Epoch 1787/5000] loss: 0.59906\n",
      "[Epoch 1788/5000] loss: 0.64783\n",
      "[Epoch 1789/5000] loss: 1.57370\n",
      "[Epoch 1790/5000] loss: 1.16798\n",
      "[Epoch 1791/5000] loss: 0.85510\n",
      "[Epoch 1792/5000] loss: 1.23058\n",
      "[Epoch 1793/5000] loss: 1.06532\n",
      "[Epoch 1794/5000] loss: 0.72699\n",
      "[Epoch 1795/5000] loss: 0.43930\n",
      "[Epoch 1796/5000] loss: 1.13756\n",
      "[Epoch 1797/5000] loss: 0.58191\n",
      "[Epoch 1798/5000] loss: 0.90544\n",
      "[Epoch 1799/5000] loss: 0.79955\n",
      "[Epoch 1800/5000] loss: 0.96103\n",
      "[Epoch 1801/5000] loss: 0.78834\n",
      "[Epoch 1802/5000] loss: 1.37400\n",
      "[Epoch 1803/5000] loss: 3.57914\n",
      "[Epoch 1804/5000] loss: 0.79511\n",
      "[Epoch 1805/5000] loss: 0.77943\n",
      "[Epoch 1806/5000] loss: 1.02078\n",
      "[Epoch 1807/5000] loss: 0.55768\n",
      "[Epoch 1808/5000] loss: 1.37249\n",
      "[Epoch 1809/5000] loss: 0.64612\n",
      "[Epoch 1810/5000] loss: 0.86935\n",
      "[Epoch 1811/5000] loss: 3.16553\n",
      "[Epoch 1812/5000] loss: 1.26640\n",
      "[Epoch 1813/5000] loss: 0.74628\n",
      "[Epoch 1814/5000] loss: 3.32017\n",
      "[Epoch 1815/5000] loss: 0.77938\n",
      "[Epoch 1816/5000] loss: 0.58697\n",
      "[Epoch 1817/5000] loss: 0.38264\n",
      "[Epoch 1818/5000] loss: 0.94974\n",
      "[Epoch 1819/5000] loss: 0.62161\n",
      "[Epoch 1820/5000] loss: 0.63722\n",
      "[Epoch 1821/5000] loss: 1.13652\n",
      "[Epoch 1822/5000] loss: 0.87783\n",
      "[Epoch 1823/5000] loss: 1.04070\n",
      "[Epoch 1824/5000] loss: 1.10301\n",
      "[Epoch 1825/5000] loss: 1.19922\n",
      "[Epoch 1826/5000] loss: 0.98242\n",
      "[Epoch 1827/5000] loss: 1.19510\n",
      "[Epoch 1828/5000] loss: 0.62718\n",
      "[Epoch 1829/5000] loss: 0.77116\n",
      "[Epoch 1830/5000] loss: 1.32764\n",
      "[Epoch 1831/5000] loss: 0.78804\n",
      "[Epoch 1832/5000] loss: 0.95006\n",
      "[Epoch 1833/5000] loss: 1.03232\n",
      "[Epoch 1834/5000] loss: 0.81671\n",
      "[Epoch 1835/5000] loss: 0.87738\n",
      "[Epoch 1836/5000] loss: 0.85453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1837/5000] loss: 0.71910\n",
      "[Epoch 1838/5000] loss: 0.90001\n",
      "[Epoch 1839/5000] loss: 0.92414\n",
      "[Epoch 1840/5000] loss: 0.78889\n",
      "[Epoch 1841/5000] loss: 0.79144\n",
      "[Epoch 1842/5000] loss: 1.08839\n",
      "[Epoch 1843/5000] loss: 1.00116\n",
      "[Epoch 1844/5000] loss: 0.71708\n",
      "[Epoch 1845/5000] loss: 1.03266\n",
      "[Epoch 1846/5000] loss: 0.89160\n",
      "[Epoch 1847/5000] loss: 0.88368\n",
      "[Epoch 1848/5000] loss: 0.66177\n",
      "[Epoch 1849/5000] loss: 1.01904\n",
      "[Epoch 1850/5000] loss: 1.30643\n",
      "[Epoch 1851/5000] loss: 0.80256\n",
      "[Epoch 1852/5000] loss: 0.87421\n",
      "[Epoch 1853/5000] loss: 0.70949\n",
      "[Epoch 1854/5000] loss: 1.08397\n",
      "[Epoch 1855/5000] loss: 0.75897\n",
      "[Epoch 1856/5000] loss: 0.89879\n",
      "[Epoch 1857/5000] loss: 0.83451\n",
      "[Epoch 1858/5000] loss: 0.67919\n",
      "[Epoch 1859/5000] loss: 0.86922\n",
      "[Epoch 1860/5000] loss: 0.87004\n",
      "[Epoch 1861/5000] loss: 0.68095\n",
      "[Epoch 1862/5000] loss: 0.83623\n",
      "[Epoch 1863/5000] loss: 0.90650\n",
      "[Epoch 1864/5000] loss: 1.35417\n",
      "[Epoch 1865/5000] loss: 0.74176\n",
      "[Epoch 1866/5000] loss: 0.63759\n",
      "[Epoch 1867/5000] loss: 0.95968\n",
      "[Epoch 1868/5000] loss: 0.94102\n",
      "[Epoch 1869/5000] loss: 0.95568\n",
      "[Epoch 1870/5000] loss: 0.75724\n",
      "[Epoch 1871/5000] loss: 0.76247\n",
      "[Epoch 1872/5000] loss: 0.94291\n",
      "[Epoch 1873/5000] loss: 1.11414\n",
      "[Epoch 1874/5000] loss: 0.48726\n",
      "[Epoch 1875/5000] loss: 1.00091\n",
      "[Epoch 1876/5000] loss: 0.86321\n",
      "[Epoch 1877/5000] loss: 0.93760\n",
      "[Epoch 1878/5000] loss: 0.74940\n",
      "[Epoch 1879/5000] loss: 1.01500\n",
      "[Epoch 1880/5000] loss: 1.09347\n",
      "[Epoch 1881/5000] loss: 3.13615\n",
      "[Epoch 1882/5000] loss: 0.93219\n",
      "[Epoch 1883/5000] loss: 0.67879\n",
      "[Epoch 1884/5000] loss: 1.21754\n",
      "[Epoch 1885/5000] loss: 1.23379\n",
      "[Epoch 1886/5000] loss: 0.84524\n",
      "[Epoch 1887/5000] loss: 1.32238\n",
      "[Epoch 1888/5000] loss: 0.80301\n",
      "[Epoch 1889/5000] loss: 0.90446\n",
      "[Epoch 1890/5000] loss: 1.11944\n",
      "[Epoch 1891/5000] loss: 0.79877\n",
      "[Epoch 1892/5000] loss: 0.78373\n",
      "[Epoch 1893/5000] loss: 1.16383\n",
      "[Epoch 1894/5000] loss: 0.50480\n",
      "[Epoch 1895/5000] loss: 3.32689\n",
      "[Epoch 1896/5000] loss: 0.82357\n",
      "[Epoch 1897/5000] loss: 0.79790\n",
      "[Epoch 1898/5000] loss: 1.30560\n",
      "[Epoch 1899/5000] loss: 0.86963\n",
      "[Epoch 1900/5000] loss: 1.33614\n",
      "[Epoch 1901/5000] loss: 1.18231\n",
      "[Epoch 1902/5000] loss: 1.13801\n",
      "[Epoch 1903/5000] loss: 1.32900\n",
      "[Epoch 1904/5000] loss: 1.04091\n",
      "[Epoch 1905/5000] loss: 1.48703\n",
      "[Epoch 1906/5000] loss: 1.13064\n",
      "[Epoch 1907/5000] loss: 1.07351\n",
      "[Epoch 1908/5000] loss: 2.15948\n",
      "[Epoch 1909/5000] loss: 1.09756\n",
      "[Epoch 1910/5000] loss: 1.36074\n",
      "[Epoch 1911/5000] loss: 0.75083\n",
      "[Epoch 1912/5000] loss: 0.80258\n",
      "[Epoch 1913/5000] loss: 0.77277\n",
      "[Epoch 1914/5000] loss: 0.80234\n",
      "[Epoch 1915/5000] loss: 0.78801\n",
      "[Epoch 1916/5000] loss: 0.76909\n",
      "[Epoch 1917/5000] loss: 1.20816\n",
      "[Epoch 1918/5000] loss: 1.16078\n",
      "[Epoch 1919/5000] loss: 0.97362\n",
      "[Epoch 1920/5000] loss: 1.54695\n",
      "[Epoch 1921/5000] loss: 0.62691\n",
      "[Epoch 1922/5000] loss: 0.52819\n",
      "[Epoch 1923/5000] loss: 0.80755\n",
      "[Epoch 1924/5000] loss: 1.13055\n",
      "[Epoch 1925/5000] loss: 1.08006\n",
      "[Epoch 1926/5000] loss: 0.85686\n",
      "[Epoch 1927/5000] loss: 1.24233\n",
      "[Epoch 1928/5000] loss: 1.35417\n",
      "[Epoch 1929/5000] loss: 1.03093\n",
      "[Epoch 1930/5000] loss: 0.68189\n",
      "[Epoch 1931/5000] loss: 0.62117\n",
      "[Epoch 1932/5000] loss: 0.91368\n",
      "[Epoch 1933/5000] loss: 1.02236\n",
      "[Epoch 1934/5000] loss: 0.77678\n",
      "[Epoch 1935/5000] loss: 0.84612\n",
      "[Epoch 1936/5000] loss: 1.43962\n",
      "[Epoch 1937/5000] loss: 3.09167\n",
      "[Epoch 1938/5000] loss: 1.03631\n",
      "[Epoch 1939/5000] loss: 0.61288\n",
      "[Epoch 1940/5000] loss: 1.23842\n",
      "[Epoch 1941/5000] loss: 1.38061\n",
      "[Epoch 1942/5000] loss: 1.11191\n",
      "[Epoch 1943/5000] loss: 1.08530\n",
      "[Epoch 1944/5000] loss: 0.88311\n",
      "[Epoch 1945/5000] loss: 1.19468\n",
      "[Epoch 1946/5000] loss: 0.97503\n",
      "[Epoch 1947/5000] loss: 1.07899\n",
      "[Epoch 1948/5000] loss: 0.71687\n",
      "[Epoch 1949/5000] loss: 2.06860\n",
      "[Epoch 1950/5000] loss: 0.98240\n",
      "[Epoch 1951/5000] loss: 0.32472\n",
      "[Epoch 1952/5000] loss: 0.80864\n",
      "[Epoch 1953/5000] loss: 0.77593\n",
      "[Epoch 1954/5000] loss: 1.09651\n",
      "[Epoch 1955/5000] loss: 0.87736\n",
      "[Epoch 1956/5000] loss: 0.56018\n",
      "[Epoch 1957/5000] loss: 1.26275\n",
      "[Epoch 1958/5000] loss: 0.75334\n",
      "[Epoch 1959/5000] loss: 0.90639\n",
      "[Epoch 1960/5000] loss: 0.71743\n",
      "[Epoch 1961/5000] loss: 1.23806\n",
      "[Epoch 1962/5000] loss: 0.30742\n",
      "[Epoch 1963/5000] loss: 1.08613\n",
      "[Epoch 1964/5000] loss: 1.07688\n",
      "[Epoch 1965/5000] loss: 1.46332\n",
      "[Epoch 1966/5000] loss: 0.64948\n",
      "[Epoch 1967/5000] loss: 0.70611\n",
      "[Epoch 1968/5000] loss: 0.91898\n",
      "[Epoch 1969/5000] loss: 1.62679\n",
      "[Epoch 1970/5000] loss: 1.18570\n",
      "[Epoch 1971/5000] loss: 0.45292\n",
      "[Epoch 1972/5000] loss: 0.82396\n",
      "[Epoch 1973/5000] loss: 1.48998\n",
      "[Epoch 1974/5000] loss: 0.86775\n",
      "[Epoch 1975/5000] loss: 0.89630\n",
      "[Epoch 1976/5000] loss: 0.79395\n",
      "[Epoch 1977/5000] loss: 1.14452\n",
      "[Epoch 1978/5000] loss: 1.25781\n",
      "[Epoch 1979/5000] loss: 1.14374\n",
      "[Epoch 1980/5000] loss: 0.52997\n",
      "[Epoch 1981/5000] loss: 1.19392\n",
      "[Epoch 1982/5000] loss: 1.20487\n",
      "[Epoch 1983/5000] loss: 0.78114\n",
      "[Epoch 1984/5000] loss: 0.71665\n",
      "[Epoch 1985/5000] loss: 1.67572\n",
      "[Epoch 1986/5000] loss: 0.87093\n",
      "[Epoch 1987/5000] loss: 0.94688\n",
      "[Epoch 1988/5000] loss: 1.14815\n",
      "[Epoch 1989/5000] loss: 1.08944\n",
      "[Epoch 1990/5000] loss: 0.92596\n",
      "[Epoch 1991/5000] loss: 1.39328\n",
      "[Epoch 1992/5000] loss: 1.38950\n",
      "[Epoch 1993/5000] loss: 1.32442\n",
      "[Epoch 1994/5000] loss: 0.72849\n",
      "[Epoch 1995/5000] loss: 0.66592\n",
      "[Epoch 1996/5000] loss: 3.47188\n",
      "[Epoch 1997/5000] loss: 1.03298\n",
      "[Epoch 1998/5000] loss: 3.14319\n",
      "[Epoch 1999/5000] loss: 0.38372\n",
      "[Epoch 2000/5000] loss: 0.98991\n",
      "[Epoch 2001/5000] loss: 1.17350\n",
      "[Epoch 2002/5000] loss: 0.70484\n",
      "[Epoch 2003/5000] loss: 1.35313\n",
      "[Epoch 2004/5000] loss: 1.30739\n",
      "[Epoch 2005/5000] loss: 0.74863\n",
      "[Epoch 2006/5000] loss: 0.89649\n",
      "[Epoch 2007/5000] loss: 1.17724\n",
      "[Epoch 2008/5000] loss: 0.99156\n",
      "[Epoch 2009/5000] loss: 1.08868\n",
      "[Epoch 2010/5000] loss: 0.63195\n",
      "[Epoch 2011/5000] loss: 0.99413\n",
      "[Epoch 2012/5000] loss: 0.85670\n",
      "[Epoch 2013/5000] loss: 1.02421\n",
      "[Epoch 2014/5000] loss: 0.39492\n",
      "[Epoch 2015/5000] loss: 1.16108\n",
      "[Epoch 2016/5000] loss: 0.67450\n",
      "[Epoch 2017/5000] loss: 0.89236\n",
      "[Epoch 2018/5000] loss: 0.85739\n",
      "[Epoch 2019/5000] loss: 1.31029\n",
      "[Epoch 2020/5000] loss: 0.76033\n",
      "[Epoch 2021/5000] loss: 1.04948\n",
      "[Epoch 2022/5000] loss: 1.09263\n",
      "[Epoch 2023/5000] loss: 0.87267\n",
      "[Epoch 2024/5000] loss: 1.35307\n",
      "[Epoch 2025/5000] loss: 1.73155\n",
      "[Epoch 2026/5000] loss: 1.10755\n",
      "[Epoch 2027/5000] loss: 1.45540\n",
      "[Epoch 2028/5000] loss: 0.93407\n",
      "[Epoch 2029/5000] loss: 0.93044\n",
      "[Epoch 2030/5000] loss: 0.28569\n",
      "[Epoch 2031/5000] loss: 1.31727\n",
      "[Epoch 2032/5000] loss: 0.78477\n",
      "[Epoch 2033/5000] loss: 1.13298\n",
      "[Epoch 2034/5000] loss: 0.88728\n",
      "[Epoch 2035/5000] loss: 1.26661\n",
      "[Epoch 2036/5000] loss: 1.31631\n",
      "[Epoch 2037/5000] loss: 0.86362\n",
      "[Epoch 2038/5000] loss: 0.73397\n",
      "[Epoch 2039/5000] loss: 1.10438\n",
      "[Epoch 2040/5000] loss: 1.26440\n",
      "[Epoch 2041/5000] loss: 0.50473\n",
      "[Epoch 2042/5000] loss: 0.52035\n",
      "[Epoch 2043/5000] loss: 0.78866\n",
      "[Epoch 2044/5000] loss: 0.98905\n",
      "[Epoch 2045/5000] loss: 1.76273\n",
      "[Epoch 2046/5000] loss: 3.31981\n",
      "[Epoch 2047/5000] loss: 0.65170\n",
      "[Epoch 2048/5000] loss: 1.54957\n",
      "[Epoch 2049/5000] loss: 0.52436\n",
      "[Epoch 2050/5000] loss: 1.47704\n",
      "[Epoch 2051/5000] loss: 1.36582\n",
      "[Epoch 2052/5000] loss: 0.51021\n",
      "[Epoch 2053/5000] loss: 1.25956\n",
      "[Epoch 2054/5000] loss: 1.05526\n",
      "[Epoch 2055/5000] loss: 0.54491\n",
      "[Epoch 2056/5000] loss: 0.66193\n",
      "[Epoch 2057/5000] loss: 3.41550\n",
      "[Epoch 2058/5000] loss: 0.91446\n",
      "[Epoch 2059/5000] loss: 1.15787\n",
      "[Epoch 2060/5000] loss: 0.97980\n",
      "[Epoch 2061/5000] loss: 1.07048\n",
      "[Epoch 2062/5000] loss: 1.04775\n",
      "[Epoch 2063/5000] loss: 1.05838\n",
      "[Epoch 2064/5000] loss: 0.97857\n",
      "[Epoch 2065/5000] loss: 0.92543\n",
      "[Epoch 2066/5000] loss: 0.95301\n",
      "[Epoch 2067/5000] loss: 1.34840\n",
      "[Epoch 2068/5000] loss: 0.88196\n",
      "[Epoch 2069/5000] loss: 0.41183\n",
      "[Epoch 2070/5000] loss: 1.13345\n",
      "[Epoch 2071/5000] loss: 0.85580\n",
      "[Epoch 2072/5000] loss: 1.05981\n",
      "[Epoch 2073/5000] loss: 0.78018\n",
      "[Epoch 2074/5000] loss: 0.99872\n",
      "[Epoch 2075/5000] loss: 0.62402\n",
      "[Epoch 2076/5000] loss: 0.92624\n",
      "[Epoch 2077/5000] loss: 3.38622\n",
      "[Epoch 2078/5000] loss: 0.66485\n",
      "[Epoch 2079/5000] loss: 0.93005\n",
      "[Epoch 2080/5000] loss: 1.17855\n",
      "[Epoch 2081/5000] loss: 1.02616\n",
      "[Epoch 2082/5000] loss: 0.85244\n",
      "[Epoch 2083/5000] loss: 1.00616\n",
      "[Epoch 2084/5000] loss: 0.97391\n",
      "[Epoch 2085/5000] loss: 0.86929\n",
      "[Epoch 2086/5000] loss: 0.87781\n",
      "[Epoch 2087/5000] loss: 0.77210\n",
      "[Epoch 2088/5000] loss: 0.74100\n",
      "[Epoch 2089/5000] loss: 1.05016\n",
      "[Epoch 2090/5000] loss: 1.02305\n",
      "[Epoch 2091/5000] loss: 0.78594\n",
      "[Epoch 2092/5000] loss: 1.31068\n",
      "[Epoch 2093/5000] loss: 1.30086\n",
      "[Epoch 2094/5000] loss: 1.14712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2095/5000] loss: 1.37093\n",
      "[Epoch 2096/5000] loss: 0.70063\n",
      "[Epoch 2097/5000] loss: 0.64086\n",
      "[Epoch 2098/5000] loss: 1.17970\n",
      "[Epoch 2099/5000] loss: 0.58823\n",
      "[Epoch 2100/5000] loss: 0.80178\n",
      "[Epoch 2101/5000] loss: 1.39215\n",
      "[Epoch 2102/5000] loss: 0.33736\n",
      "[Epoch 2103/5000] loss: 0.72223\n",
      "[Epoch 2104/5000] loss: 0.73745\n",
      "[Epoch 2105/5000] loss: 0.70839\n",
      "[Epoch 2106/5000] loss: 0.96423\n",
      "[Epoch 2107/5000] loss: 0.99963\n",
      "[Epoch 2108/5000] loss: 0.58945\n",
      "[Epoch 2109/5000] loss: 0.84281\n",
      "[Epoch 2110/5000] loss: 0.95405\n",
      "[Epoch 2111/5000] loss: 1.08911\n",
      "[Epoch 2112/5000] loss: 0.75823\n",
      "[Epoch 2113/5000] loss: 1.49644\n",
      "[Epoch 2114/5000] loss: 0.66736\n",
      "[Epoch 2115/5000] loss: 0.83098\n",
      "[Epoch 2116/5000] loss: 0.79133\n",
      "[Epoch 2117/5000] loss: 0.75224\n",
      "[Epoch 2118/5000] loss: 0.54163\n",
      "[Epoch 2119/5000] loss: 1.09720\n",
      "[Epoch 2120/5000] loss: 0.89618\n",
      "[Epoch 2121/5000] loss: 0.56352\n",
      "[Epoch 2122/5000] loss: 1.25107\n",
      "[Epoch 2123/5000] loss: 1.46267\n",
      "[Epoch 2124/5000] loss: 1.07193\n",
      "[Epoch 2125/5000] loss: 0.96080\n",
      "[Epoch 2126/5000] loss: 0.94312\n",
      "[Epoch 2127/5000] loss: 1.43380\n",
      "[Epoch 2128/5000] loss: 1.20621\n",
      "[Epoch 2129/5000] loss: 0.69061\n",
      "[Epoch 2130/5000] loss: 1.38317\n",
      "[Epoch 2131/5000] loss: 0.64393\n",
      "[Epoch 2132/5000] loss: 1.04370\n",
      "[Epoch 2133/5000] loss: 0.76809\n",
      "[Epoch 2134/5000] loss: 1.00391\n",
      "[Epoch 2135/5000] loss: 3.07597\n",
      "[Epoch 2136/5000] loss: 0.73378\n",
      "[Epoch 2137/5000] loss: 0.60593\n",
      "[Epoch 2138/5000] loss: 1.36452\n",
      "[Epoch 2139/5000] loss: 3.62135\n",
      "[Epoch 2140/5000] loss: 0.80162\n",
      "[Epoch 2141/5000] loss: 1.07266\n",
      "[Epoch 2142/5000] loss: 0.70287\n",
      "[Epoch 2143/5000] loss: 0.51749\n",
      "[Epoch 2144/5000] loss: 0.96905\n",
      "[Epoch 2145/5000] loss: 1.30556\n",
      "[Epoch 2146/5000] loss: 1.05714\n",
      "[Epoch 2147/5000] loss: 0.99380\n",
      "[Epoch 2148/5000] loss: 1.00722\n",
      "[Epoch 2149/5000] loss: 0.90498\n",
      "[Epoch 2150/5000] loss: 0.59578\n",
      "[Epoch 2151/5000] loss: 1.04814\n",
      "[Epoch 2152/5000] loss: 1.08043\n",
      "[Epoch 2153/5000] loss: 1.45121\n",
      "[Epoch 2154/5000] loss: 1.29007\n",
      "[Epoch 2155/5000] loss: 1.19064\n",
      "[Epoch 2156/5000] loss: 0.87899\n",
      "[Epoch 2157/5000] loss: 1.86430\n",
      "[Epoch 2158/5000] loss: 1.46158\n",
      "[Epoch 2159/5000] loss: 1.06201\n",
      "[Epoch 2160/5000] loss: 0.75608\n",
      "[Epoch 2161/5000] loss: 0.62670\n",
      "[Epoch 2162/5000] loss: 0.77086\n",
      "[Epoch 2163/5000] loss: 0.70295\n",
      "[Epoch 2164/5000] loss: 1.25551\n",
      "[Epoch 2165/5000] loss: 0.90982\n",
      "[Epoch 2166/5000] loss: 0.68544\n",
      "[Epoch 2167/5000] loss: 0.74912\n",
      "[Epoch 2168/5000] loss: 0.69640\n",
      "[Epoch 2169/5000] loss: 1.25692\n",
      "[Epoch 2170/5000] loss: 1.28583\n",
      "[Epoch 2171/5000] loss: 0.75727\n",
      "[Epoch 2172/5000] loss: 0.91452\n",
      "[Epoch 2173/5000] loss: 1.02622\n",
      "[Epoch 2174/5000] loss: 0.86295\n",
      "[Epoch 2175/5000] loss: 0.59804\n",
      "[Epoch 2176/5000] loss: 1.18142\n",
      "[Epoch 2177/5000] loss: 1.04594\n",
      "[Epoch 2178/5000] loss: 1.21383\n",
      "[Epoch 2179/5000] loss: 0.98012\n",
      "[Epoch 2180/5000] loss: 0.65632\n",
      "[Epoch 2181/5000] loss: 1.34526\n",
      "[Epoch 2182/5000] loss: 0.72468\n",
      "[Epoch 2183/5000] loss: 0.89537\n",
      "[Epoch 2184/5000] loss: 0.59092\n",
      "[Epoch 2185/5000] loss: 1.41740\n",
      "[Epoch 2186/5000] loss: 0.75914\n",
      "[Epoch 2187/5000] loss: 0.63871\n",
      "[Epoch 2188/5000] loss: 0.69834\n",
      "[Epoch 2189/5000] loss: 0.97696\n",
      "[Epoch 2190/5000] loss: 1.25672\n",
      "[Epoch 2191/5000] loss: 0.72963\n",
      "[Epoch 2192/5000] loss: 0.99012\n",
      "[Epoch 2193/5000] loss: 1.04805\n",
      "[Epoch 2194/5000] loss: 0.70625\n",
      "[Epoch 2195/5000] loss: 0.90294\n",
      "[Epoch 2196/5000] loss: 0.63335\n",
      "[Epoch 2197/5000] loss: 0.99965\n",
      "[Epoch 2198/5000] loss: 1.11843\n",
      "[Epoch 2199/5000] loss: 1.00153\n",
      "[Epoch 2200/5000] loss: 0.89226\n",
      "[Epoch 2201/5000] loss: 1.09793\n",
      "[Epoch 2202/5000] loss: 1.17510\n",
      "[Epoch 2203/5000] loss: 1.31406\n",
      "[Epoch 2204/5000] loss: 0.95213\n",
      "[Epoch 2205/5000] loss: 0.88163\n",
      "[Epoch 2206/5000] loss: 0.86163\n",
      "[Epoch 2207/5000] loss: 1.45040\n",
      "[Epoch 2208/5000] loss: 0.54902\n",
      "[Epoch 2209/5000] loss: 0.70369\n",
      "[Epoch 2210/5000] loss: 0.58324\n",
      "[Epoch 2211/5000] loss: 0.85773\n",
      "[Epoch 2212/5000] loss: 1.62455\n",
      "[Epoch 2213/5000] loss: 0.98348\n",
      "[Epoch 2214/5000] loss: 2.79587\n",
      "[Epoch 2215/5000] loss: 0.55062\n",
      "[Epoch 2216/5000] loss: 0.80812\n",
      "[Epoch 2217/5000] loss: 1.27690\n",
      "[Epoch 2218/5000] loss: 1.07596\n",
      "[Epoch 2219/5000] loss: 1.00738\n",
      "[Epoch 2220/5000] loss: 1.09099\n",
      "[Epoch 2221/5000] loss: 0.37888\n",
      "[Epoch 2222/5000] loss: 0.62657\n",
      "[Epoch 2223/5000] loss: 0.96514\n",
      "[Epoch 2224/5000] loss: 0.50589\n",
      "[Epoch 2225/5000] loss: 0.32852\n",
      "[Epoch 2226/5000] loss: 1.33615\n",
      "[Epoch 2227/5000] loss: 0.71302\n",
      "[Epoch 2228/5000] loss: 1.30322\n",
      "[Epoch 2229/5000] loss: 1.20120\n",
      "[Epoch 2230/5000] loss: 0.48070\n",
      "[Epoch 2231/5000] loss: 1.15072\n",
      "[Epoch 2232/5000] loss: 0.90576\n",
      "[Epoch 2233/5000] loss: 0.58117\n",
      "[Epoch 2234/5000] loss: 1.34229\n",
      "[Epoch 2235/5000] loss: 1.31551\n",
      "[Epoch 2236/5000] loss: 1.32883\n",
      "[Epoch 2237/5000] loss: 0.61124\n",
      "[Epoch 2238/5000] loss: 1.08881\n",
      "[Epoch 2239/5000] loss: 0.83996\n",
      "[Epoch 2240/5000] loss: 0.57702\n",
      "[Epoch 2241/5000] loss: 0.65014\n",
      "[Epoch 2242/5000] loss: 1.07846\n",
      "[Epoch 2243/5000] loss: 0.81887\n",
      "[Epoch 2244/5000] loss: 1.07933\n",
      "[Epoch 2245/5000] loss: 0.86030\n",
      "[Epoch 2246/5000] loss: 0.87042\n",
      "[Epoch 2247/5000] loss: 1.35173\n",
      "[Epoch 2248/5000] loss: 0.83884\n",
      "[Epoch 2249/5000] loss: 1.04915\n",
      "[Epoch 2250/5000] loss: 1.13765\n",
      "[Epoch 2251/5000] loss: 1.17733\n",
      "[Epoch 2252/5000] loss: 1.00928\n",
      "[Epoch 2253/5000] loss: 1.13867\n",
      "[Epoch 2254/5000] loss: 0.80097\n",
      "[Epoch 2255/5000] loss: 0.52524\n",
      "[Epoch 2256/5000] loss: 0.50442\n",
      "[Epoch 2257/5000] loss: 1.08937\n",
      "[Epoch 2258/5000] loss: 1.30149\n",
      "[Epoch 2259/5000] loss: 0.89000\n",
      "[Epoch 2260/5000] loss: 0.70672\n",
      "[Epoch 2261/5000] loss: 0.70721\n",
      "[Epoch 2262/5000] loss: 0.85216\n",
      "[Epoch 2263/5000] loss: 1.33354\n",
      "[Epoch 2264/5000] loss: 0.82729\n",
      "[Epoch 2265/5000] loss: 0.84741\n",
      "[Epoch 2266/5000] loss: 1.31562\n",
      "[Epoch 2267/5000] loss: 0.70469\n",
      "[Epoch 2268/5000] loss: 1.99119\n",
      "[Epoch 2269/5000] loss: 0.90872\n",
      "[Epoch 2270/5000] loss: 0.77469\n",
      "[Epoch 2271/5000] loss: 0.71802\n",
      "[Epoch 2272/5000] loss: 0.48745\n",
      "[Epoch 2273/5000] loss: 1.11255\n",
      "[Epoch 2274/5000] loss: 1.31403\n",
      "[Epoch 2275/5000] loss: 0.62794\n",
      "[Epoch 2276/5000] loss: 1.23568\n",
      "[Epoch 2277/5000] loss: 0.66404\n",
      "[Epoch 2278/5000] loss: 1.26589\n",
      "[Epoch 2279/5000] loss: 0.81476\n",
      "[Epoch 2280/5000] loss: 0.90757\n",
      "[Epoch 2281/5000] loss: 1.25351\n",
      "[Epoch 2282/5000] loss: 0.66822\n",
      "[Epoch 2283/5000] loss: 0.78849\n",
      "[Epoch 2284/5000] loss: 0.79224\n",
      "[Epoch 2285/5000] loss: 0.97657\n",
      "[Epoch 2286/5000] loss: 0.29528\n",
      "[Epoch 2287/5000] loss: 0.77448\n",
      "[Epoch 2288/5000] loss: 1.18914\n",
      "[Epoch 2289/5000] loss: 1.30370\n",
      "[Epoch 2290/5000] loss: 1.21630\n",
      "[Epoch 2291/5000] loss: 0.90087\n",
      "[Epoch 2292/5000] loss: 0.38047\n",
      "[Epoch 2293/5000] loss: 0.76215\n",
      "[Epoch 2294/5000] loss: 0.57946\n",
      "[Epoch 2295/5000] loss: 3.20550\n",
      "[Epoch 2296/5000] loss: 0.83281\n",
      "[Epoch 2297/5000] loss: 0.98801\n",
      "[Epoch 2298/5000] loss: 0.70910\n",
      "[Epoch 2299/5000] loss: 1.31047\n",
      "[Epoch 2300/5000] loss: 0.81894\n",
      "[Epoch 2301/5000] loss: 0.61230\n",
      "[Epoch 2302/5000] loss: 0.92663\n",
      "[Epoch 2303/5000] loss: 1.00540\n",
      "[Epoch 2304/5000] loss: 0.90148\n",
      "[Epoch 2305/5000] loss: 0.90460\n",
      "[Epoch 2306/5000] loss: 0.62942\n",
      "[Epoch 2307/5000] loss: 1.33866\n",
      "[Epoch 2308/5000] loss: 0.80888\n",
      "[Epoch 2309/5000] loss: 0.99849\n",
      "[Epoch 2310/5000] loss: 0.91724\n",
      "[Epoch 2311/5000] loss: 1.29708\n",
      "[Epoch 2312/5000] loss: 1.03227\n",
      "[Epoch 2313/5000] loss: 0.85881\n",
      "[Epoch 2314/5000] loss: 1.29165\n",
      "[Epoch 2315/5000] loss: 0.90834\n",
      "[Epoch 2316/5000] loss: 0.84680\n",
      "[Epoch 2317/5000] loss: 0.95146\n",
      "[Epoch 2318/5000] loss: 1.20413\n",
      "[Epoch 2319/5000] loss: 0.74351\n",
      "[Epoch 2320/5000] loss: 0.73836\n",
      "[Epoch 2321/5000] loss: 0.99559\n",
      "[Epoch 2322/5000] loss: 0.48743\n",
      "[Epoch 2323/5000] loss: 0.65321\n",
      "[Epoch 2324/5000] loss: 1.46723\n",
      "[Epoch 2325/5000] loss: 1.07637\n",
      "[Epoch 2326/5000] loss: 1.26591\n",
      "[Epoch 2327/5000] loss: 1.23763\n",
      "[Epoch 2328/5000] loss: 1.00581\n",
      "[Epoch 2329/5000] loss: 1.49800\n",
      "[Epoch 2330/5000] loss: 0.84151\n",
      "[Epoch 2331/5000] loss: 1.40469\n",
      "[Epoch 2332/5000] loss: 1.33738\n",
      "[Epoch 2333/5000] loss: 1.14939\n",
      "[Epoch 2334/5000] loss: 0.56232\n",
      "[Epoch 2335/5000] loss: 1.29511\n",
      "[Epoch 2336/5000] loss: 0.67968\n",
      "[Epoch 2337/5000] loss: 0.54937\n",
      "[Epoch 2338/5000] loss: 0.81700\n",
      "[Epoch 2339/5000] loss: 0.69257\n",
      "[Epoch 2340/5000] loss: 0.68910\n",
      "[Epoch 2341/5000] loss: 0.93658\n",
      "[Epoch 2342/5000] loss: 0.55421\n",
      "[Epoch 2343/5000] loss: 0.49722\n",
      "[Epoch 2344/5000] loss: 1.25226\n",
      "[Epoch 2345/5000] loss: 0.98304\n",
      "[Epoch 2346/5000] loss: 1.20623\n",
      "[Epoch 2347/5000] loss: 0.94344\n",
      "[Epoch 2348/5000] loss: 0.86407\n",
      "[Epoch 2349/5000] loss: 1.12534\n",
      "[Epoch 2350/5000] loss: 0.61252\n",
      "[Epoch 2351/5000] loss: 3.10568\n",
      "[Epoch 2352/5000] loss: 1.05666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2353/5000] loss: 1.12724\n",
      "[Epoch 2354/5000] loss: 0.73934\n",
      "[Epoch 2355/5000] loss: 0.35491\n",
      "[Epoch 2356/5000] loss: 1.12347\n",
      "[Epoch 2357/5000] loss: 1.01072\n",
      "[Epoch 2358/5000] loss: 1.00966\n",
      "[Epoch 2359/5000] loss: 0.85425\n",
      "[Epoch 2360/5000] loss: 0.68261\n",
      "[Epoch 2361/5000] loss: 0.87155\n",
      "[Epoch 2362/5000] loss: 1.28867\n",
      "[Epoch 2363/5000] loss: 1.72954\n",
      "[Epoch 2364/5000] loss: 1.41711\n",
      "[Epoch 2365/5000] loss: 0.68333\n",
      "[Epoch 2366/5000] loss: 1.21118\n",
      "[Epoch 2367/5000] loss: 1.03156\n",
      "[Epoch 2368/5000] loss: 1.18897\n",
      "[Epoch 2369/5000] loss: 1.06969\n",
      "[Epoch 2370/5000] loss: 0.65202\n",
      "[Epoch 2371/5000] loss: 1.21102\n",
      "[Epoch 2372/5000] loss: 0.66491\n",
      "[Epoch 2373/5000] loss: 0.86581\n",
      "[Epoch 2374/5000] loss: 1.52936\n",
      "[Epoch 2375/5000] loss: 0.78308\n",
      "[Epoch 2376/5000] loss: 0.91033\n",
      "[Epoch 2377/5000] loss: 0.70747\n",
      "[Epoch 2378/5000] loss: 1.01683\n",
      "[Epoch 2379/5000] loss: 0.81160\n",
      "[Epoch 2380/5000] loss: 0.50264\n",
      "[Epoch 2381/5000] loss: 0.63491\n",
      "[Epoch 2382/5000] loss: 0.98373\n",
      "[Epoch 2383/5000] loss: 0.66588\n",
      "[Epoch 2384/5000] loss: 0.78435\n",
      "[Epoch 2385/5000] loss: 2.04990\n",
      "[Epoch 2386/5000] loss: 0.66547\n",
      "[Epoch 2387/5000] loss: 1.67753\n",
      "[Epoch 2388/5000] loss: 0.61080\n",
      "[Epoch 2389/5000] loss: 0.46279\n",
      "[Epoch 2390/5000] loss: 0.89751\n",
      "[Epoch 2391/5000] loss: 1.07132\n",
      "[Epoch 2392/5000] loss: 1.74960\n",
      "[Epoch 2393/5000] loss: 1.36871\n",
      "[Epoch 2394/5000] loss: 0.81321\n",
      "[Epoch 2395/5000] loss: 0.49777\n",
      "[Epoch 2396/5000] loss: 0.88177\n",
      "[Epoch 2397/5000] loss: 0.69467\n",
      "[Epoch 2398/5000] loss: 1.13354\n",
      "[Epoch 2399/5000] loss: 0.76681\n",
      "[Epoch 2400/5000] loss: 0.55693\n",
      "[Epoch 2401/5000] loss: 0.61875\n",
      "[Epoch 2402/5000] loss: 0.72413\n",
      "[Epoch 2403/5000] loss: 0.80870\n",
      "[Epoch 2404/5000] loss: 1.34413\n",
      "[Epoch 2405/5000] loss: 1.25281\n",
      "[Epoch 2406/5000] loss: 0.80556\n",
      "[Epoch 2407/5000] loss: 1.37486\n",
      "[Epoch 2408/5000] loss: 0.97582\n",
      "[Epoch 2409/5000] loss: 1.22018\n",
      "[Epoch 2410/5000] loss: 0.48439\n",
      "[Epoch 2411/5000] loss: 0.66789\n",
      "[Epoch 2412/5000] loss: 1.04068\n",
      "[Epoch 2413/5000] loss: 1.09095\n",
      "[Epoch 2414/5000] loss: 1.06006\n",
      "[Epoch 2415/5000] loss: 0.54503\n",
      "[Epoch 2416/5000] loss: 1.32037\n",
      "[Epoch 2417/5000] loss: 1.01624\n",
      "[Epoch 2418/5000] loss: 1.12386\n",
      "[Epoch 2419/5000] loss: 1.13691\n",
      "[Epoch 2420/5000] loss: 0.80230\n",
      "[Epoch 2421/5000] loss: 0.73758\n",
      "[Epoch 2422/5000] loss: 0.50809\n",
      "[Epoch 2423/5000] loss: 1.36431\n",
      "[Epoch 2424/5000] loss: 0.78409\n",
      "[Epoch 2425/5000] loss: 0.75079\n",
      "[Epoch 2426/5000] loss: 0.51846\n",
      "[Epoch 2427/5000] loss: 0.74582\n",
      "[Epoch 2428/5000] loss: 1.29348\n",
      "[Epoch 2429/5000] loss: 0.53205\n",
      "[Epoch 2430/5000] loss: 1.35209\n",
      "[Epoch 2431/5000] loss: 0.65517\n",
      "[Epoch 2432/5000] loss: 1.47424\n",
      "[Epoch 2433/5000] loss: 0.55894\n",
      "[Epoch 2434/5000] loss: 1.07113\n",
      "[Epoch 2435/5000] loss: 0.81504\n",
      "[Epoch 2436/5000] loss: 0.91979\n",
      "[Epoch 2437/5000] loss: 1.25490\n",
      "[Epoch 2438/5000] loss: 0.77087\n",
      "[Epoch 2439/5000] loss: 1.08966\n",
      "[Epoch 2440/5000] loss: 1.03814\n",
      "[Epoch 2441/5000] loss: 0.80312\n",
      "[Epoch 2442/5000] loss: 0.43917\n",
      "[Epoch 2443/5000] loss: 0.90834\n",
      "[Epoch 2444/5000] loss: 0.92261\n",
      "[Epoch 2445/5000] loss: 0.96778\n",
      "[Epoch 2446/5000] loss: 1.49102\n",
      "[Epoch 2447/5000] loss: 1.06654\n",
      "[Epoch 2448/5000] loss: 0.64424\n",
      "[Epoch 2449/5000] loss: 1.22150\n",
      "[Epoch 2450/5000] loss: 1.04515\n",
      "[Epoch 2451/5000] loss: 1.02180\n",
      "[Epoch 2452/5000] loss: 0.78943\n",
      "[Epoch 2453/5000] loss: 0.89870\n",
      "[Epoch 2454/5000] loss: 0.82626\n",
      "[Epoch 2455/5000] loss: 0.93816\n",
      "[Epoch 2456/5000] loss: 1.61509\n",
      "[Epoch 2457/5000] loss: 0.85003\n",
      "[Epoch 2458/5000] loss: 0.42825\n",
      "[Epoch 2459/5000] loss: 1.23283\n",
      "[Epoch 2460/5000] loss: 0.98397\n",
      "[Epoch 2461/5000] loss: 1.29658\n",
      "[Epoch 2462/5000] loss: 1.07646\n",
      "[Epoch 2463/5000] loss: 0.97040\n",
      "[Epoch 2464/5000] loss: 0.77052\n",
      "[Epoch 2465/5000] loss: 1.22458\n",
      "[Epoch 2466/5000] loss: 1.12871\n",
      "[Epoch 2467/5000] loss: 2.86634\n",
      "[Epoch 2468/5000] loss: 0.67330\n",
      "[Epoch 2469/5000] loss: 1.08411\n",
      "[Epoch 2470/5000] loss: 1.12217\n",
      "[Epoch 2471/5000] loss: 0.95885\n",
      "[Epoch 2472/5000] loss: 0.94075\n",
      "[Epoch 2473/5000] loss: 0.90752\n",
      "[Epoch 2474/5000] loss: 0.85767\n",
      "[Epoch 2475/5000] loss: 0.89595\n",
      "[Epoch 2476/5000] loss: 0.91424\n",
      "[Epoch 2477/5000] loss: 0.97812\n",
      "[Epoch 2478/5000] loss: 0.44965\n",
      "[Epoch 2479/5000] loss: 1.23509\n",
      "[Epoch 2480/5000] loss: 1.38868\n",
      "[Epoch 2481/5000] loss: 0.92832\n",
      "[Epoch 2482/5000] loss: 1.05230\n",
      "[Epoch 2483/5000] loss: 1.35747\n",
      "[Epoch 2484/5000] loss: 0.56149\n",
      "[Epoch 2485/5000] loss: 0.61010\n",
      "[Epoch 2486/5000] loss: 0.66182\n",
      "[Epoch 2487/5000] loss: 0.82979\n",
      "[Epoch 2488/5000] loss: 0.89167\n",
      "[Epoch 2489/5000] loss: 0.92121\n",
      "[Epoch 2490/5000] loss: 0.63290\n",
      "[Epoch 2491/5000] loss: 0.83043\n",
      "[Epoch 2492/5000] loss: 1.26733\n",
      "[Epoch 2493/5000] loss: 0.60815\n",
      "[Epoch 2494/5000] loss: 1.37974\n",
      "[Epoch 2495/5000] loss: 1.22502\n",
      "[Epoch 2496/5000] loss: 0.79635\n",
      "[Epoch 2497/5000] loss: 1.07413\n",
      "[Epoch 2498/5000] loss: 0.72910\n",
      "[Epoch 2499/5000] loss: 1.01120\n",
      "[Epoch 2500/5000] loss: 0.93654\n",
      "[Epoch 2501/5000] loss: 0.62535\n",
      "[Epoch 2502/5000] loss: 1.31068\n",
      "[Epoch 2503/5000] loss: 0.76389\n",
      "[Epoch 2504/5000] loss: 1.18563\n",
      "[Epoch 2505/5000] loss: 3.13131\n",
      "[Epoch 2506/5000] loss: 1.04657\n",
      "[Epoch 2507/5000] loss: 0.85688\n",
      "[Epoch 2508/5000] loss: 0.54540\n",
      "[Epoch 2509/5000] loss: 1.14150\n",
      "[Epoch 2510/5000] loss: 1.01947\n",
      "[Epoch 2511/5000] loss: 1.26585\n",
      "[Epoch 2512/5000] loss: 0.85271\n",
      "[Epoch 2513/5000] loss: 0.89955\n",
      "[Epoch 2514/5000] loss: 0.90945\n",
      "[Epoch 2515/5000] loss: 0.93620\n",
      "[Epoch 2516/5000] loss: 1.50423\n",
      "[Epoch 2517/5000] loss: 0.84916\n",
      "[Epoch 2518/5000] loss: 0.70413\n",
      "[Epoch 2519/5000] loss: 1.66980\n",
      "[Epoch 2520/5000] loss: 0.78643\n",
      "[Epoch 2521/5000] loss: 1.58172\n",
      "[Epoch 2522/5000] loss: 0.96171\n",
      "[Epoch 2523/5000] loss: 1.22506\n",
      "[Epoch 2524/5000] loss: 0.81643\n",
      "[Epoch 2525/5000] loss: 1.16437\n",
      "[Epoch 2526/5000] loss: 0.95953\n",
      "[Epoch 2527/5000] loss: 0.70311\n",
      "[Epoch 2528/5000] loss: 0.87130\n",
      "[Epoch 2529/5000] loss: 0.76437\n",
      "[Epoch 2530/5000] loss: 0.92230\n",
      "[Epoch 2531/5000] loss: 0.75095\n",
      "[Epoch 2532/5000] loss: 0.79920\n",
      "[Epoch 2533/5000] loss: 0.96945\n",
      "[Epoch 2534/5000] loss: 1.46730\n",
      "[Epoch 2535/5000] loss: 0.77796\n",
      "[Epoch 2536/5000] loss: 1.19713\n",
      "[Epoch 2537/5000] loss: 0.77908\n",
      "[Epoch 2538/5000] loss: 1.55587\n",
      "[Epoch 2539/5000] loss: 1.10637\n",
      "[Epoch 2540/5000] loss: 0.96977\n",
      "[Epoch 2541/5000] loss: 1.06027\n",
      "[Epoch 2542/5000] loss: 1.38181\n",
      "[Epoch 2543/5000] loss: 0.81043\n",
      "[Epoch 2544/5000] loss: 0.68769\n",
      "[Epoch 2545/5000] loss: 1.32152\n",
      "[Epoch 2546/5000] loss: 0.60236\n",
      "[Epoch 2547/5000] loss: 1.01946\n",
      "[Epoch 2548/5000] loss: 1.31778\n",
      "[Epoch 2549/5000] loss: 1.00297\n",
      "[Epoch 2550/5000] loss: 1.34189\n",
      "[Epoch 2551/5000] loss: 1.16421\n",
      "[Epoch 2552/5000] loss: 0.84364\n",
      "[Epoch 2553/5000] loss: 1.54045\n",
      "[Epoch 2554/5000] loss: 0.68174\n",
      "[Epoch 2555/5000] loss: 0.57263\n",
      "[Epoch 2556/5000] loss: 1.04421\n",
      "[Epoch 2557/5000] loss: 0.98500\n",
      "[Epoch 2558/5000] loss: 0.97714\n",
      "[Epoch 2559/5000] loss: 0.68790\n",
      "[Epoch 2560/5000] loss: 1.05250\n",
      "[Epoch 2561/5000] loss: 0.57486\n",
      "[Epoch 2562/5000] loss: 0.62530\n",
      "[Epoch 2563/5000] loss: 0.74891\n",
      "[Epoch 2564/5000] loss: 0.90598\n",
      "[Epoch 2565/5000] loss: 0.58626\n",
      "[Epoch 2566/5000] loss: 3.06517\n",
      "[Epoch 2567/5000] loss: 0.56555\n",
      "[Epoch 2568/5000] loss: 0.94596\n",
      "[Epoch 2569/5000] loss: 1.18773\n",
      "[Epoch 2570/5000] loss: 1.42464\n",
      "[Epoch 2571/5000] loss: 1.22393\n",
      "[Epoch 2572/5000] loss: 0.99332\n",
      "[Epoch 2573/5000] loss: 1.40643\n",
      "[Epoch 2574/5000] loss: 0.65796\n",
      "[Epoch 2575/5000] loss: 0.66826\n",
      "[Epoch 2576/5000] loss: 0.92781\n",
      "[Epoch 2577/5000] loss: 1.40572\n",
      "[Epoch 2578/5000] loss: 0.65024\n",
      "[Epoch 2579/5000] loss: 1.01413\n",
      "[Epoch 2580/5000] loss: 0.87872\n",
      "[Epoch 2581/5000] loss: 1.13276\n",
      "[Epoch 2582/5000] loss: 1.08266\n",
      "[Epoch 2583/5000] loss: 0.76415\n",
      "[Epoch 2584/5000] loss: 0.87255\n",
      "[Epoch 2585/5000] loss: 1.36352\n",
      "[Epoch 2586/5000] loss: 0.94574\n",
      "[Epoch 2587/5000] loss: 0.87404\n",
      "[Epoch 2588/5000] loss: 0.61345\n",
      "[Epoch 2589/5000] loss: 1.14327\n",
      "[Epoch 2590/5000] loss: 0.84780\n",
      "[Epoch 2591/5000] loss: 0.79233\n",
      "[Epoch 2592/5000] loss: 0.96130\n",
      "[Epoch 2593/5000] loss: 0.80644\n",
      "[Epoch 2594/5000] loss: 0.68160\n",
      "[Epoch 2595/5000] loss: 1.10436\n",
      "[Epoch 2596/5000] loss: 0.92662\n",
      "[Epoch 2597/5000] loss: 0.62720\n",
      "[Epoch 2598/5000] loss: 1.26466\n",
      "[Epoch 2599/5000] loss: 1.61222\n",
      "[Epoch 2600/5000] loss: 1.20682\n",
      "[Epoch 2601/5000] loss: 1.09156\n",
      "[Epoch 2602/5000] loss: 1.18821\n",
      "[Epoch 2603/5000] loss: 1.64387\n",
      "[Epoch 2604/5000] loss: 1.70025\n",
      "[Epoch 2605/5000] loss: 1.57753\n",
      "[Epoch 2606/5000] loss: 0.91046\n",
      "[Epoch 2607/5000] loss: 0.90360\n",
      "[Epoch 2608/5000] loss: 1.61934\n",
      "[Epoch 2609/5000] loss: 0.74965\n",
      "[Epoch 2610/5000] loss: 0.93275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2611/5000] loss: 1.16584\n",
      "[Epoch 2612/5000] loss: 3.22766\n",
      "[Epoch 2613/5000] loss: 0.66732\n",
      "[Epoch 2614/5000] loss: 0.99440\n",
      "[Epoch 2615/5000] loss: 0.72217\n",
      "[Epoch 2616/5000] loss: 0.91328\n",
      "[Epoch 2617/5000] loss: 1.37976\n",
      "[Epoch 2618/5000] loss: 1.11340\n",
      "[Epoch 2619/5000] loss: 0.51135\n",
      "[Epoch 2620/5000] loss: 0.94483\n",
      "[Epoch 2621/5000] loss: 1.06219\n",
      "[Epoch 2622/5000] loss: 0.87894\n",
      "[Epoch 2623/5000] loss: 1.17780\n",
      "[Epoch 2624/5000] loss: 0.93570\n",
      "[Epoch 2625/5000] loss: 0.54514\n",
      "[Epoch 2626/5000] loss: 1.00476\n",
      "[Epoch 2627/5000] loss: 1.25889\n",
      "[Epoch 2628/5000] loss: 0.87740\n",
      "[Epoch 2629/5000] loss: 1.08335\n",
      "[Epoch 2630/5000] loss: 1.33280\n",
      "[Epoch 2631/5000] loss: 0.48608\n",
      "[Epoch 2632/5000] loss: 2.97145\n",
      "[Epoch 2633/5000] loss: 1.86078\n",
      "[Epoch 2634/5000] loss: 3.80610\n",
      "[Epoch 2635/5000] loss: 1.09770\n",
      "[Epoch 2636/5000] loss: 1.31793\n",
      "[Epoch 2637/5000] loss: 0.58355\n",
      "[Epoch 2638/5000] loss: 0.62109\n",
      "[Epoch 2639/5000] loss: 1.14570\n",
      "[Epoch 2640/5000] loss: 0.70324\n",
      "[Epoch 2641/5000] loss: 1.07536\n",
      "[Epoch 2642/5000] loss: 0.83717\n",
      "[Epoch 2643/5000] loss: 0.89159\n",
      "[Epoch 2644/5000] loss: 0.86709\n",
      "[Epoch 2645/5000] loss: 0.98133\n",
      "[Epoch 2646/5000] loss: 0.74319\n",
      "[Epoch 2647/5000] loss: 0.99142\n",
      "[Epoch 2648/5000] loss: 1.07233\n",
      "[Epoch 2649/5000] loss: 1.32345\n",
      "[Epoch 2650/5000] loss: 1.09333\n",
      "[Epoch 2651/5000] loss: 0.87349\n",
      "[Epoch 2652/5000] loss: 1.22243\n",
      "[Epoch 2653/5000] loss: 0.90998\n",
      "[Epoch 2654/5000] loss: 1.08858\n",
      "[Epoch 2655/5000] loss: 0.81018\n",
      "[Epoch 2656/5000] loss: 1.10097\n",
      "[Epoch 2657/5000] loss: 0.81263\n",
      "[Epoch 2658/5000] loss: 0.79904\n",
      "[Epoch 2659/5000] loss: 0.79582\n",
      "[Epoch 2660/5000] loss: 0.66739\n",
      "[Epoch 2661/5000] loss: 0.81600\n",
      "[Epoch 2662/5000] loss: 1.04032\n",
      "[Epoch 2663/5000] loss: 1.14463\n",
      "[Epoch 2664/5000] loss: 0.75443\n",
      "[Epoch 2665/5000] loss: 1.77623\n",
      "[Epoch 2666/5000] loss: 1.44555\n",
      "[Epoch 2667/5000] loss: 0.87617\n",
      "[Epoch 2668/5000] loss: 0.74973\n",
      "[Epoch 2669/5000] loss: 1.05160\n",
      "[Epoch 2670/5000] loss: 0.93523\n",
      "[Epoch 2671/5000] loss: 1.08163\n",
      "[Epoch 2672/5000] loss: 1.24455\n",
      "[Epoch 2673/5000] loss: 0.95364\n",
      "[Epoch 2674/5000] loss: 0.79211\n",
      "[Epoch 2675/5000] loss: 0.99198\n",
      "[Epoch 2676/5000] loss: 1.12349\n",
      "[Epoch 2677/5000] loss: 1.09214\n",
      "[Epoch 2678/5000] loss: 0.68017\n",
      "[Epoch 2679/5000] loss: 0.81983\n",
      "[Epoch 2680/5000] loss: 1.40472\n",
      "[Epoch 2681/5000] loss: 1.06489\n",
      "[Epoch 2682/5000] loss: 0.53337\n",
      "[Epoch 2683/5000] loss: 1.06292\n",
      "[Epoch 2684/5000] loss: 1.01221\n",
      "[Epoch 2685/5000] loss: 1.14196\n",
      "[Epoch 2686/5000] loss: 0.51929\n",
      "[Epoch 2687/5000] loss: 0.96659\n",
      "[Epoch 2688/5000] loss: 1.34900\n",
      "[Epoch 2689/5000] loss: 1.41317\n",
      "[Epoch 2690/5000] loss: 0.65459\n",
      "[Epoch 2691/5000] loss: 0.78519\n",
      "[Epoch 2692/5000] loss: 1.09551\n",
      "[Epoch 2693/5000] loss: 0.96012\n",
      "[Epoch 2694/5000] loss: 1.08733\n",
      "[Epoch 2695/5000] loss: 0.83045\n",
      "[Epoch 2696/5000] loss: 1.29046\n",
      "[Epoch 2697/5000] loss: 1.08227\n",
      "[Epoch 2698/5000] loss: 0.87097\n",
      "[Epoch 2699/5000] loss: 1.19996\n",
      "[Epoch 2700/5000] loss: 0.85400\n",
      "[Epoch 2701/5000] loss: 1.15195\n",
      "[Epoch 2702/5000] loss: 1.50389\n",
      "[Epoch 2703/5000] loss: 1.04230\n",
      "[Epoch 2704/5000] loss: 1.16919\n",
      "[Epoch 2705/5000] loss: 0.99349\n",
      "[Epoch 2706/5000] loss: 0.97955\n",
      "[Epoch 2707/5000] loss: 1.19712\n",
      "[Epoch 2708/5000] loss: 0.75695\n",
      "[Epoch 2709/5000] loss: 0.47873\n",
      "[Epoch 2710/5000] loss: 0.99874\n",
      "[Epoch 2711/5000] loss: 1.23008\n",
      "[Epoch 2712/5000] loss: 1.48455\n",
      "[Epoch 2713/5000] loss: 0.89404\n",
      "[Epoch 2714/5000] loss: 1.78027\n",
      "[Epoch 2715/5000] loss: 0.90654\n",
      "[Epoch 2716/5000] loss: 3.25469\n",
      "[Epoch 2717/5000] loss: 1.00823\n",
      "[Epoch 2718/5000] loss: 1.28246\n",
      "[Epoch 2719/5000] loss: 1.16405\n",
      "[Epoch 2720/5000] loss: 1.04078\n",
      "[Epoch 2721/5000] loss: 0.95664\n",
      "[Epoch 2722/5000] loss: 1.13425\n",
      "[Epoch 2723/5000] loss: 0.65063\n",
      "[Epoch 2724/5000] loss: 1.37566\n",
      "[Epoch 2725/5000] loss: 0.75742\n",
      "[Epoch 2726/5000] loss: 0.87680\n",
      "[Epoch 2727/5000] loss: 0.93211\n",
      "[Epoch 2728/5000] loss: 0.76458\n",
      "[Epoch 2729/5000] loss: 0.87928\n",
      "[Epoch 2730/5000] loss: 0.74611\n",
      "[Epoch 2731/5000] loss: 1.31065\n",
      "[Epoch 2732/5000] loss: 0.92941\n",
      "[Epoch 2733/5000] loss: 0.89727\n",
      "[Epoch 2734/5000] loss: 0.80388\n",
      "[Epoch 2735/5000] loss: 1.07708\n",
      "[Epoch 2736/5000] loss: 0.54016\n",
      "[Epoch 2737/5000] loss: 0.99987\n",
      "[Epoch 2738/5000] loss: 0.93750\n",
      "[Epoch 2739/5000] loss: 0.91664\n",
      "[Epoch 2740/5000] loss: 1.25310\n",
      "[Epoch 2741/5000] loss: 0.61411\n",
      "[Epoch 2742/5000] loss: 1.07934\n",
      "[Epoch 2743/5000] loss: 0.89875\n",
      "[Epoch 2744/5000] loss: 1.26737\n",
      "[Epoch 2745/5000] loss: 0.86669\n",
      "[Epoch 2746/5000] loss: 0.96557\n",
      "[Epoch 2747/5000] loss: 1.38907\n",
      "[Epoch 2748/5000] loss: 0.42351\n",
      "[Epoch 2749/5000] loss: 0.64883\n",
      "[Epoch 2750/5000] loss: 0.95280\n",
      "[Epoch 2751/5000] loss: 0.96404\n",
      "[Epoch 2752/5000] loss: 1.63562\n",
      "[Epoch 2753/5000] loss: 0.82940\n",
      "[Epoch 2754/5000] loss: 0.94200\n",
      "[Epoch 2755/5000] loss: 0.87800\n",
      "[Epoch 2756/5000] loss: 1.22380\n",
      "[Epoch 2757/5000] loss: 1.11792\n",
      "[Epoch 2758/5000] loss: 0.80015\n",
      "[Epoch 2759/5000] loss: 1.18194\n",
      "[Epoch 2760/5000] loss: 1.02615\n",
      "[Epoch 2761/5000] loss: 0.62677\n",
      "[Epoch 2762/5000] loss: 0.86924\n",
      "[Epoch 2763/5000] loss: 0.65897\n",
      "[Epoch 2764/5000] loss: 0.39697\n",
      "[Epoch 2765/5000] loss: 1.09739\n",
      "[Epoch 2766/5000] loss: 0.77920\n",
      "[Epoch 2767/5000] loss: 0.99128\n",
      "[Epoch 2768/5000] loss: 1.16268\n",
      "[Epoch 2769/5000] loss: 1.09077\n",
      "[Epoch 2770/5000] loss: 0.60523\n",
      "[Epoch 2771/5000] loss: 1.01506\n",
      "[Epoch 2772/5000] loss: 1.59251\n",
      "[Epoch 2773/5000] loss: 1.23483\n",
      "[Epoch 2774/5000] loss: 1.11627\n",
      "[Epoch 2775/5000] loss: 0.98480\n",
      "[Epoch 2776/5000] loss: 1.35430\n",
      "[Epoch 2777/5000] loss: 1.22258\n",
      "[Epoch 2778/5000] loss: 0.92724\n",
      "[Epoch 2779/5000] loss: 1.06759\n",
      "[Epoch 2780/5000] loss: 1.04936\n",
      "[Epoch 2781/5000] loss: 0.74772\n",
      "[Epoch 2782/5000] loss: 0.88909\n",
      "[Epoch 2783/5000] loss: 1.10872\n",
      "[Epoch 2784/5000] loss: 1.12072\n",
      "[Epoch 2785/5000] loss: 1.17359\n",
      "[Epoch 2786/5000] loss: 1.20146\n",
      "[Epoch 2787/5000] loss: 1.61067\n",
      "[Epoch 2788/5000] loss: 1.09615\n",
      "[Epoch 2789/5000] loss: 0.97031\n",
      "[Epoch 2790/5000] loss: 1.05435\n",
      "[Epoch 2791/5000] loss: 0.86377\n",
      "[Epoch 2792/5000] loss: 0.85315\n",
      "[Epoch 2793/5000] loss: 1.09787\n",
      "[Epoch 2794/5000] loss: 0.86846\n",
      "[Epoch 2795/5000] loss: 0.87420\n",
      "[Epoch 2796/5000] loss: 0.53523\n",
      "[Epoch 2797/5000] loss: 0.92216\n",
      "[Epoch 2798/5000] loss: 0.70288\n",
      "[Epoch 2799/5000] loss: 1.28060\n",
      "[Epoch 2800/5000] loss: 0.92002\n",
      "[Epoch 2801/5000] loss: 0.76002\n",
      "[Epoch 2802/5000] loss: 0.64064\n",
      "[Epoch 2803/5000] loss: 0.90821\n",
      "[Epoch 2804/5000] loss: 0.79804\n",
      "[Epoch 2805/5000] loss: 1.30748\n",
      "[Epoch 2806/5000] loss: 1.67591\n",
      "[Epoch 2807/5000] loss: 1.46978\n",
      "[Epoch 2808/5000] loss: 1.68782\n",
      "[Epoch 2809/5000] loss: 1.38243\n",
      "[Epoch 2810/5000] loss: 0.91780\n",
      "[Epoch 2811/5000] loss: 0.93123\n",
      "[Epoch 2812/5000] loss: 1.50575\n",
      "[Epoch 2813/5000] loss: 1.01023\n",
      "[Epoch 2814/5000] loss: 1.14449\n",
      "[Epoch 2815/5000] loss: 1.19932\n",
      "[Epoch 2816/5000] loss: 0.80781\n",
      "[Epoch 2817/5000] loss: 0.76111\n",
      "[Epoch 2818/5000] loss: 0.46868\n",
      "[Epoch 2819/5000] loss: 0.61042\n",
      "[Epoch 2820/5000] loss: 0.72960\n",
      "[Epoch 2821/5000] loss: 1.03310\n",
      "[Epoch 2822/5000] loss: 1.26154\n",
      "[Epoch 2823/5000] loss: 0.85618\n",
      "[Epoch 2824/5000] loss: 1.17019\n",
      "[Epoch 2825/5000] loss: 1.34572\n",
      "[Epoch 2826/5000] loss: 0.90004\n",
      "[Epoch 2827/5000] loss: 0.51759\n",
      "[Epoch 2828/5000] loss: 0.91024\n",
      "[Epoch 2829/5000] loss: 0.78406\n",
      "[Epoch 2830/5000] loss: 1.21372\n",
      "[Epoch 2831/5000] loss: 2.07905\n",
      "[Epoch 2832/5000] loss: 0.73256\n",
      "[Epoch 2833/5000] loss: 0.64516\n",
      "[Epoch 2834/5000] loss: 0.81761\n",
      "[Epoch 2835/5000] loss: 0.83098\n",
      "[Epoch 2836/5000] loss: 0.97222\n",
      "[Epoch 2837/5000] loss: 0.65931\n",
      "[Epoch 2838/5000] loss: 0.98082\n",
      "[Epoch 2839/5000] loss: 1.61119\n",
      "[Epoch 2840/5000] loss: 0.94502\n",
      "[Epoch 2841/5000] loss: 1.01869\n",
      "[Epoch 2842/5000] loss: 1.12454\n",
      "[Epoch 2843/5000] loss: 1.21830\n",
      "[Epoch 2844/5000] loss: 0.78457\n",
      "[Epoch 2845/5000] loss: 3.24144\n",
      "[Epoch 2846/5000] loss: 1.05552\n",
      "[Epoch 2847/5000] loss: 3.79392\n",
      "[Epoch 2848/5000] loss: 1.82074\n",
      "[Epoch 2849/5000] loss: 0.94444\n",
      "[Epoch 2850/5000] loss: 1.18968\n",
      "[Epoch 2851/5000] loss: 1.45062\n",
      "[Epoch 2852/5000] loss: 0.63656\n",
      "[Epoch 2853/5000] loss: 0.88447\n",
      "[Epoch 2854/5000] loss: 0.78602\n",
      "[Epoch 2855/5000] loss: 1.03488\n",
      "[Epoch 2856/5000] loss: 0.81560\n",
      "[Epoch 2857/5000] loss: 0.85499\n",
      "[Epoch 2858/5000] loss: 0.62937\n",
      "[Epoch 2859/5000] loss: 0.90695\n",
      "[Epoch 2860/5000] loss: 0.98706\n",
      "[Epoch 2861/5000] loss: 1.11589\n",
      "[Epoch 2862/5000] loss: 0.66319\n",
      "[Epoch 2863/5000] loss: 1.19125\n",
      "[Epoch 2864/5000] loss: 0.63665\n",
      "[Epoch 2865/5000] loss: 0.77576\n",
      "[Epoch 2866/5000] loss: 1.14831\n",
      "[Epoch 2867/5000] loss: 0.89527\n",
      "[Epoch 2868/5000] loss: 0.88469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2869/5000] loss: 0.75791\n",
      "[Epoch 2870/5000] loss: 0.71176\n",
      "[Epoch 2871/5000] loss: 1.20758\n",
      "[Epoch 2872/5000] loss: 1.00084\n",
      "[Epoch 2873/5000] loss: 0.65237\n",
      "[Epoch 2874/5000] loss: 0.47271\n",
      "[Epoch 2875/5000] loss: 0.93743\n",
      "[Epoch 2876/5000] loss: 0.95723\n",
      "[Epoch 2877/5000] loss: 0.59380\n",
      "[Epoch 2878/5000] loss: 0.78558\n",
      "[Epoch 2879/5000] loss: 1.60815\n",
      "[Epoch 2880/5000] loss: 1.04359\n",
      "[Epoch 2881/5000] loss: 1.23637\n",
      "[Epoch 2882/5000] loss: 1.94649\n",
      "[Epoch 2883/5000] loss: 0.45085\n",
      "[Epoch 2884/5000] loss: 0.41713\n",
      "[Epoch 2885/5000] loss: 1.10141\n",
      "[Epoch 2886/5000] loss: 3.08024\n",
      "[Epoch 2887/5000] loss: 1.04813\n",
      "[Epoch 2888/5000] loss: 0.96119\n",
      "[Epoch 2889/5000] loss: 1.01078\n",
      "[Epoch 2890/5000] loss: 0.76350\n",
      "[Epoch 2891/5000] loss: 0.56528\n",
      "[Epoch 2892/5000] loss: 0.81727\n",
      "[Epoch 2893/5000] loss: 0.86264\n",
      "[Epoch 2894/5000] loss: 1.46450\n",
      "[Epoch 2895/5000] loss: 1.24586\n",
      "[Epoch 2896/5000] loss: 0.84120\n",
      "[Epoch 2897/5000] loss: 0.61792\n",
      "[Epoch 2898/5000] loss: 3.57097\n",
      "[Epoch 2899/5000] loss: 0.75442\n",
      "[Epoch 2900/5000] loss: 1.21085\n",
      "[Epoch 2901/5000] loss: 0.72895\n",
      "[Epoch 2902/5000] loss: 1.11527\n",
      "[Epoch 2903/5000] loss: 1.00115\n",
      "[Epoch 2904/5000] loss: 0.83464\n",
      "[Epoch 2905/5000] loss: 1.39952\n",
      "[Epoch 2906/5000] loss: 0.63651\n",
      "[Epoch 2907/5000] loss: 0.95619\n",
      "[Epoch 2908/5000] loss: 0.67581\n",
      "[Epoch 2909/5000] loss: 1.20547\n",
      "[Epoch 2910/5000] loss: 0.62399\n",
      "[Epoch 2911/5000] loss: 0.46733\n",
      "[Epoch 2912/5000] loss: 0.94895\n",
      "[Epoch 2913/5000] loss: 0.80502\n",
      "[Epoch 2914/5000] loss: 1.19460\n",
      "[Epoch 2915/5000] loss: 1.07199\n",
      "[Epoch 2916/5000] loss: 1.10003\n",
      "[Epoch 2917/5000] loss: 1.20938\n",
      "[Epoch 2918/5000] loss: 1.08335\n",
      "[Epoch 2919/5000] loss: 1.21286\n",
      "[Epoch 2920/5000] loss: 0.82487\n",
      "[Epoch 2921/5000] loss: 0.77552\n",
      "[Epoch 2922/5000] loss: 1.27111\n",
      "[Epoch 2923/5000] loss: 0.79461\n",
      "[Epoch 2924/5000] loss: 1.00161\n",
      "[Epoch 2925/5000] loss: 0.92253\n",
      "[Epoch 2926/5000] loss: 1.35258\n",
      "[Epoch 2927/5000] loss: 1.60900\n",
      "[Epoch 2928/5000] loss: 0.95205\n",
      "[Epoch 2929/5000] loss: 0.56120\n",
      "[Epoch 2930/5000] loss: 0.76993\n",
      "[Epoch 2931/5000] loss: 1.01333\n",
      "[Epoch 2932/5000] loss: 1.07985\n",
      "[Epoch 2933/5000] loss: 3.08049\n",
      "[Epoch 2934/5000] loss: 0.97517\n",
      "[Epoch 2935/5000] loss: 0.42483\n",
      "[Epoch 2936/5000] loss: 1.12740\n",
      "[Epoch 2937/5000] loss: 0.97681\n",
      "[Epoch 2938/5000] loss: 0.95710\n",
      "[Epoch 2939/5000] loss: 0.82791\n",
      "[Epoch 2940/5000] loss: 0.55879\n",
      "[Epoch 2941/5000] loss: 0.71265\n",
      "[Epoch 2942/5000] loss: 1.16533\n",
      "[Epoch 2943/5000] loss: 1.50989\n",
      "[Epoch 2944/5000] loss: 1.13679\n",
      "[Epoch 2945/5000] loss: 0.27454\n",
      "[Epoch 2946/5000] loss: 0.73008\n",
      "[Epoch 2947/5000] loss: 0.92283\n",
      "[Epoch 2948/5000] loss: 0.72886\n",
      "[Epoch 2949/5000] loss: 0.50001\n",
      "[Epoch 2950/5000] loss: 1.30472\n",
      "[Epoch 2951/5000] loss: 1.01404\n",
      "[Epoch 2952/5000] loss: 0.88969\n",
      "[Epoch 2953/5000] loss: 0.89307\n",
      "[Epoch 2954/5000] loss: 0.64618\n",
      "[Epoch 2955/5000] loss: 0.56218\n",
      "[Epoch 2956/5000] loss: 1.06157\n",
      "[Epoch 2957/5000] loss: 1.35599\n",
      "[Epoch 2958/5000] loss: 1.05725\n",
      "[Epoch 2959/5000] loss: 0.65770\n",
      "[Epoch 2960/5000] loss: 0.78243\n",
      "[Epoch 2961/5000] loss: 0.81063\n",
      "[Epoch 2962/5000] loss: 1.26334\n",
      "[Epoch 2963/5000] loss: 0.74627\n",
      "[Epoch 2964/5000] loss: 0.58262\n",
      "[Epoch 2965/5000] loss: 0.82481\n",
      "[Epoch 2966/5000] loss: 1.41517\n",
      "[Epoch 2967/5000] loss: 1.00761\n",
      "[Epoch 2968/5000] loss: 1.40571\n",
      "[Epoch 2969/5000] loss: 1.42573\n",
      "[Epoch 2970/5000] loss: 1.78111\n",
      "[Epoch 2971/5000] loss: 1.48834\n",
      "[Epoch 2972/5000] loss: 0.30006\n",
      "[Epoch 2973/5000] loss: 1.05501\n",
      "[Epoch 2974/5000] loss: 1.31619\n",
      "[Epoch 2975/5000] loss: 0.93479\n",
      "[Epoch 2976/5000] loss: 1.22275\n",
      "[Epoch 2977/5000] loss: 1.71432\n",
      "[Epoch 2978/5000] loss: 0.80348\n",
      "[Epoch 2979/5000] loss: 1.18532\n",
      "[Epoch 2980/5000] loss: 1.13612\n",
      "[Epoch 2981/5000] loss: 0.88716\n",
      "[Epoch 2982/5000] loss: 0.76548\n",
      "[Epoch 2983/5000] loss: 0.76532\n",
      "[Epoch 2984/5000] loss: 0.74483\n",
      "[Epoch 2985/5000] loss: 0.69845\n",
      "[Epoch 2986/5000] loss: 1.19917\n",
      "[Epoch 2987/5000] loss: 0.74108\n",
      "[Epoch 2988/5000] loss: 0.79668\n",
      "[Epoch 2989/5000] loss: 0.86132\n",
      "[Epoch 2990/5000] loss: 1.05397\n",
      "[Epoch 2991/5000] loss: 0.89711\n",
      "[Epoch 2992/5000] loss: 0.93692\n",
      "[Epoch 2993/5000] loss: 1.42555\n",
      "[Epoch 2994/5000] loss: 0.78589\n",
      "[Epoch 2995/5000] loss: 1.17091\n",
      "[Epoch 2996/5000] loss: 1.11548\n",
      "[Epoch 2997/5000] loss: 0.63533\n",
      "[Epoch 2998/5000] loss: 1.13339\n",
      "[Epoch 2999/5000] loss: 1.19721\n",
      "[Epoch 3000/5000] loss: 0.66271\n",
      "[Epoch 3001/5000] loss: 0.62858\n",
      "[Epoch 3002/5000] loss: 0.67981\n",
      "[Epoch 3003/5000] loss: 1.88719\n",
      "[Epoch 3004/5000] loss: 0.76576\n",
      "[Epoch 3005/5000] loss: 0.72444\n",
      "[Epoch 3006/5000] loss: 0.65167\n",
      "[Epoch 3007/5000] loss: 1.02575\n",
      "[Epoch 3008/5000] loss: 1.03680\n",
      "[Epoch 3009/5000] loss: 0.91749\n",
      "[Epoch 3010/5000] loss: 0.69117\n",
      "[Epoch 3011/5000] loss: 1.02145\n",
      "[Epoch 3012/5000] loss: 1.20433\n",
      "[Epoch 3013/5000] loss: 0.71715\n",
      "[Epoch 3014/5000] loss: 0.86849\n",
      "[Epoch 3015/5000] loss: 1.49443\n",
      "[Epoch 3016/5000] loss: 0.63466\n",
      "[Epoch 3017/5000] loss: 0.56521\n",
      "[Epoch 3018/5000] loss: 0.76284\n",
      "[Epoch 3019/5000] loss: 2.17876\n",
      "[Epoch 3020/5000] loss: 1.23688\n",
      "[Epoch 3021/5000] loss: 1.26815\n",
      "[Epoch 3022/5000] loss: 0.97424\n",
      "[Epoch 3023/5000] loss: 0.86513\n",
      "[Epoch 3024/5000] loss: 0.94280\n",
      "[Epoch 3025/5000] loss: 1.09530\n",
      "[Epoch 3026/5000] loss: 1.15147\n",
      "[Epoch 3027/5000] loss: 1.85749\n",
      "[Epoch 3028/5000] loss: 1.02765\n",
      "[Epoch 3029/5000] loss: 0.64305\n",
      "[Epoch 3030/5000] loss: 0.75936\n",
      "[Epoch 3031/5000] loss: 1.11588\n",
      "[Epoch 3032/5000] loss: 1.21672\n",
      "[Epoch 3033/5000] loss: 1.07043\n",
      "[Epoch 3034/5000] loss: 2.82087\n",
      "[Epoch 3035/5000] loss: 0.82926\n",
      "[Epoch 3036/5000] loss: 0.96025\n",
      "[Epoch 3037/5000] loss: 2.11891\n",
      "[Epoch 3038/5000] loss: 0.54338\n",
      "[Epoch 3039/5000] loss: 0.93912\n",
      "[Epoch 3040/5000] loss: 0.78081\n",
      "[Epoch 3041/5000] loss: 0.81519\n",
      "[Epoch 3042/5000] loss: 3.10136\n",
      "[Epoch 3043/5000] loss: 0.92007\n",
      "[Epoch 3044/5000] loss: 0.69439\n",
      "[Epoch 3045/5000] loss: 0.89003\n",
      "[Epoch 3046/5000] loss: 1.30622\n",
      "[Epoch 3047/5000] loss: 1.59673\n",
      "[Epoch 3048/5000] loss: 0.78134\n",
      "[Epoch 3049/5000] loss: 1.67864\n",
      "[Epoch 3050/5000] loss: 1.08096\n",
      "[Epoch 3051/5000] loss: 1.16248\n",
      "[Epoch 3052/5000] loss: 1.23387\n",
      "[Epoch 3053/5000] loss: 1.55667\n",
      "[Epoch 3054/5000] loss: 0.90776\n",
      "[Epoch 3055/5000] loss: 0.97469\n",
      "[Epoch 3056/5000] loss: 0.63740\n",
      "[Epoch 3057/5000] loss: 0.54887\n",
      "[Epoch 3058/5000] loss: 0.80650\n",
      "[Epoch 3059/5000] loss: 1.33169\n",
      "[Epoch 3060/5000] loss: 0.98817\n",
      "[Epoch 3061/5000] loss: 0.57861\n",
      "[Epoch 3062/5000] loss: 0.63019\n",
      "[Epoch 3063/5000] loss: 0.42545\n",
      "[Epoch 3064/5000] loss: 0.74017\n",
      "[Epoch 3065/5000] loss: 1.35280\n",
      "[Epoch 3066/5000] loss: 1.26305\n",
      "[Epoch 3067/5000] loss: 1.19324\n",
      "[Epoch 3068/5000] loss: 0.65483\n",
      "[Epoch 3069/5000] loss: 0.81052\n",
      "[Epoch 3070/5000] loss: 0.95284\n",
      "[Epoch 3071/5000] loss: 0.81514\n",
      "[Epoch 3072/5000] loss: 1.09459\n",
      "[Epoch 3073/5000] loss: 0.51885\n",
      "[Epoch 3074/5000] loss: 0.94883\n",
      "[Epoch 3075/5000] loss: 1.20140\n",
      "[Epoch 3076/5000] loss: 1.17902\n",
      "[Epoch 3077/5000] loss: 1.28831\n",
      "[Epoch 3078/5000] loss: 1.06531\n",
      "[Epoch 3079/5000] loss: 0.86182\n",
      "[Epoch 3080/5000] loss: 1.26342\n",
      "[Epoch 3081/5000] loss: 0.86574\n",
      "[Epoch 3082/5000] loss: 1.14466\n",
      "[Epoch 3083/5000] loss: 1.02222\n",
      "[Epoch 3084/5000] loss: 3.32982\n",
      "[Epoch 3085/5000] loss: 0.52753\n",
      "[Epoch 3086/5000] loss: 1.02446\n",
      "[Epoch 3087/5000] loss: 0.82849\n",
      "[Epoch 3088/5000] loss: 0.98520\n",
      "[Epoch 3089/5000] loss: 1.11536\n",
      "[Epoch 3090/5000] loss: 0.59273\n",
      "[Epoch 3091/5000] loss: 0.83431\n",
      "[Epoch 3092/5000] loss: 0.84849\n",
      "[Epoch 3093/5000] loss: 0.68395\n",
      "[Epoch 3094/5000] loss: 0.98357\n",
      "[Epoch 3095/5000] loss: 1.22827\n",
      "[Epoch 3096/5000] loss: 0.65425\n",
      "[Epoch 3097/5000] loss: 1.05488\n",
      "[Epoch 3098/5000] loss: 0.96113\n",
      "[Epoch 3099/5000] loss: 1.12519\n",
      "[Epoch 3100/5000] loss: 0.48240\n",
      "[Epoch 3101/5000] loss: 1.12369\n",
      "[Epoch 3102/5000] loss: 1.07476\n",
      "[Epoch 3103/5000] loss: 0.49711\n",
      "[Epoch 3104/5000] loss: 0.42588\n",
      "[Epoch 3105/5000] loss: 1.12238\n",
      "[Epoch 3106/5000] loss: 0.84277\n",
      "[Epoch 3107/5000] loss: 0.85871\n",
      "[Epoch 3108/5000] loss: 1.24528\n",
      "[Epoch 3109/5000] loss: 0.88409\n",
      "[Epoch 3110/5000] loss: 0.72406\n",
      "[Epoch 3111/5000] loss: 1.43947\n",
      "[Epoch 3112/5000] loss: 0.69206\n",
      "[Epoch 3113/5000] loss: 1.48837\n",
      "[Epoch 3114/5000] loss: 0.92706\n",
      "[Epoch 3115/5000] loss: 0.94033\n",
      "[Epoch 3116/5000] loss: 0.66645\n",
      "[Epoch 3117/5000] loss: 0.88541\n",
      "[Epoch 3118/5000] loss: 1.12676\n",
      "[Epoch 3119/5000] loss: 0.63238\n",
      "[Epoch 3120/5000] loss: 0.92370\n",
      "[Epoch 3121/5000] loss: 0.86915\n",
      "[Epoch 3122/5000] loss: 1.19993\n",
      "[Epoch 3123/5000] loss: 1.23608\n",
      "[Epoch 3124/5000] loss: 1.07179\n",
      "[Epoch 3125/5000] loss: 0.92821\n",
      "[Epoch 3126/5000] loss: 1.37427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3127/5000] loss: 1.06191\n",
      "[Epoch 3128/5000] loss: 0.93711\n",
      "[Epoch 3129/5000] loss: 1.40107\n",
      "[Epoch 3130/5000] loss: 3.19713\n",
      "[Epoch 3131/5000] loss: 0.50896\n",
      "[Epoch 3132/5000] loss: 0.45674\n",
      "[Epoch 3133/5000] loss: 0.99221\n",
      "[Epoch 3134/5000] loss: 1.03998\n",
      "[Epoch 3135/5000] loss: 1.05490\n",
      "[Epoch 3136/5000] loss: 0.90531\n",
      "[Epoch 3137/5000] loss: 2.84971\n",
      "[Epoch 3138/5000] loss: 1.20935\n",
      "[Epoch 3139/5000] loss: 0.73211\n",
      "[Epoch 3140/5000] loss: 0.80419\n",
      "[Epoch 3141/5000] loss: 1.15122\n",
      "[Epoch 3142/5000] loss: 1.56040\n",
      "[Epoch 3143/5000] loss: 1.04577\n",
      "[Epoch 3144/5000] loss: 1.00966\n",
      "[Epoch 3145/5000] loss: 0.85790\n",
      "[Epoch 3146/5000] loss: 0.81479\n",
      "[Epoch 3147/5000] loss: 1.21150\n",
      "[Epoch 3148/5000] loss: 0.51143\n",
      "[Epoch 3149/5000] loss: 1.09709\n",
      "[Epoch 3150/5000] loss: 1.27737\n",
      "[Epoch 3151/5000] loss: 0.91608\n",
      "[Epoch 3152/5000] loss: 0.40197\n",
      "[Epoch 3153/5000] loss: 1.18898\n",
      "[Epoch 3154/5000] loss: 0.68636\n",
      "[Epoch 3155/5000] loss: 1.27314\n",
      "[Epoch 3156/5000] loss: 0.82159\n",
      "[Epoch 3157/5000] loss: 0.98346\n",
      "[Epoch 3158/5000] loss: 0.66681\n",
      "[Epoch 3159/5000] loss: 0.73203\n",
      "[Epoch 3160/5000] loss: 1.38991\n",
      "[Epoch 3161/5000] loss: 3.12123\n",
      "[Epoch 3162/5000] loss: 1.14550\n",
      "[Epoch 3163/5000] loss: 0.91831\n",
      "[Epoch 3164/5000] loss: 0.40368\n",
      "[Epoch 3165/5000] loss: 0.71516\n",
      "[Epoch 3166/5000] loss: 0.72369\n",
      "[Epoch 3167/5000] loss: 0.91367\n",
      "[Epoch 3168/5000] loss: 1.22756\n",
      "[Epoch 3169/5000] loss: 0.61790\n",
      "[Epoch 3170/5000] loss: 0.91294\n",
      "[Epoch 3171/5000] loss: 0.78236\n",
      "[Epoch 3172/5000] loss: 0.56740\n",
      "[Epoch 3173/5000] loss: 0.94865\n",
      "[Epoch 3174/5000] loss: 1.25433\n",
      "[Epoch 3175/5000] loss: 1.11670\n",
      "[Epoch 3176/5000] loss: 0.54556\n",
      "[Epoch 3177/5000] loss: 0.67466\n",
      "[Epoch 3178/5000] loss: 0.52168\n",
      "[Epoch 3179/5000] loss: 0.44815\n",
      "[Epoch 3180/5000] loss: 1.09860\n",
      "[Epoch 3181/5000] loss: 0.66423\n",
      "[Epoch 3182/5000] loss: 0.86872\n",
      "[Epoch 3183/5000] loss: 0.75045\n",
      "[Epoch 3184/5000] loss: 0.78378\n",
      "[Epoch 3185/5000] loss: 1.58565\n",
      "[Epoch 3186/5000] loss: 1.04410\n",
      "[Epoch 3187/5000] loss: 1.11366\n",
      "[Epoch 3188/5000] loss: 1.21289\n",
      "[Epoch 3189/5000] loss: 1.21394\n",
      "[Epoch 3190/5000] loss: 0.94922\n",
      "[Epoch 3191/5000] loss: 1.02311\n",
      "[Epoch 3192/5000] loss: 1.04589\n",
      "[Epoch 3193/5000] loss: 0.69607\n",
      "[Epoch 3194/5000] loss: 0.72511\n",
      "[Epoch 3195/5000] loss: 0.98500\n",
      "[Epoch 3196/5000] loss: 1.11281\n",
      "[Epoch 3197/5000] loss: 1.19775\n",
      "[Epoch 3198/5000] loss: 1.06280\n",
      "[Epoch 3199/5000] loss: 0.61966\n",
      "[Epoch 3200/5000] loss: 1.46318\n",
      "[Epoch 3201/5000] loss: 1.01199\n",
      "[Epoch 3202/5000] loss: 3.08179\n",
      "[Epoch 3203/5000] loss: 0.61657\n",
      "[Epoch 3204/5000] loss: 0.87030\n",
      "[Epoch 3205/5000] loss: 0.53034\n",
      "[Epoch 3206/5000] loss: 1.03386\n",
      "[Epoch 3207/5000] loss: 0.86154\n",
      "[Epoch 3208/5000] loss: 0.96518\n",
      "[Epoch 3209/5000] loss: 0.89516\n",
      "[Epoch 3210/5000] loss: 0.89716\n",
      "[Epoch 3211/5000] loss: 0.77563\n",
      "[Epoch 3212/5000] loss: 1.10467\n",
      "[Epoch 3213/5000] loss: 0.82378\n",
      "[Epoch 3214/5000] loss: 0.52620\n",
      "[Epoch 3215/5000] loss: 1.34051\n",
      "[Epoch 3216/5000] loss: 1.26991\n",
      "[Epoch 3217/5000] loss: 1.38935\n",
      "[Epoch 3218/5000] loss: 0.79009\n",
      "[Epoch 3219/5000] loss: 1.21200\n",
      "[Epoch 3220/5000] loss: 0.84454\n",
      "[Epoch 3221/5000] loss: 1.48378\n",
      "[Epoch 3222/5000] loss: 0.54146\n",
      "[Epoch 3223/5000] loss: 1.43503\n",
      "[Epoch 3224/5000] loss: 1.66566\n",
      "[Epoch 3225/5000] loss: 2.92023\n",
      "[Epoch 3226/5000] loss: 1.21571\n",
      "[Epoch 3227/5000] loss: 0.79263\n",
      "[Epoch 3228/5000] loss: 0.97612\n",
      "[Epoch 3229/5000] loss: 1.28597\n",
      "[Epoch 3230/5000] loss: 1.01667\n",
      "[Epoch 3231/5000] loss: 0.37841\n",
      "[Epoch 3232/5000] loss: 0.79550\n",
      "[Epoch 3233/5000] loss: 1.07297\n",
      "[Epoch 3234/5000] loss: 0.56338\n",
      "[Epoch 3235/5000] loss: 0.67773\n",
      "[Epoch 3236/5000] loss: 1.02175\n",
      "[Epoch 3237/5000] loss: 0.72255\n",
      "[Epoch 3238/5000] loss: 0.51920\n",
      "[Epoch 3239/5000] loss: 1.26410\n",
      "[Epoch 3240/5000] loss: 1.04995\n",
      "[Epoch 3241/5000] loss: 0.74920\n",
      "[Epoch 3242/5000] loss: 1.05211\n",
      "[Epoch 3243/5000] loss: 0.63461\n",
      "[Epoch 3244/5000] loss: 0.81847\n",
      "[Epoch 3245/5000] loss: 1.22877\n",
      "[Epoch 3246/5000] loss: 1.04108\n",
      "[Epoch 3247/5000] loss: 1.40471\n",
      "[Epoch 3248/5000] loss: 0.90726\n",
      "[Epoch 3249/5000] loss: 0.97435\n",
      "[Epoch 3250/5000] loss: 0.72898\n",
      "[Epoch 3251/5000] loss: 1.04318\n",
      "[Epoch 3252/5000] loss: 0.68565\n",
      "[Epoch 3253/5000] loss: 1.30339\n",
      "[Epoch 3254/5000] loss: 1.20752\n",
      "[Epoch 3255/5000] loss: 1.04363\n",
      "[Epoch 3256/5000] loss: 0.67225\n",
      "[Epoch 3257/5000] loss: 0.74557\n",
      "[Epoch 3258/5000] loss: 1.31698\n",
      "[Epoch 3259/5000] loss: 1.73581\n",
      "[Epoch 3260/5000] loss: 1.12484\n",
      "[Epoch 3261/5000] loss: 0.77780\n",
      "[Epoch 3262/5000] loss: 0.93467\n",
      "[Epoch 3263/5000] loss: 0.78379\n",
      "[Epoch 3264/5000] loss: 1.76659\n",
      "[Epoch 3265/5000] loss: 1.19024\n",
      "[Epoch 3266/5000] loss: 1.12497\n",
      "[Epoch 3267/5000] loss: 0.97574\n",
      "[Epoch 3268/5000] loss: 1.09786\n",
      "[Epoch 3269/5000] loss: 1.01344\n",
      "[Epoch 3270/5000] loss: 1.26348\n",
      "[Epoch 3271/5000] loss: 0.85782\n",
      "[Epoch 3272/5000] loss: 1.06043\n",
      "[Epoch 3273/5000] loss: 1.24038\n",
      "[Epoch 3274/5000] loss: 1.15311\n",
      "[Epoch 3275/5000] loss: 3.40701\n",
      "[Epoch 3276/5000] loss: 0.77944\n",
      "[Epoch 3277/5000] loss: 0.67068\n",
      "[Epoch 3278/5000] loss: 0.66995\n",
      "[Epoch 3279/5000] loss: 0.82657\n",
      "[Epoch 3280/5000] loss: 0.71470\n",
      "[Epoch 3281/5000] loss: 1.08613\n",
      "[Epoch 3282/5000] loss: 0.62692\n",
      "[Epoch 3283/5000] loss: 1.66283\n",
      "[Epoch 3284/5000] loss: 0.87476\n",
      "[Epoch 3285/5000] loss: 0.69097\n",
      "[Epoch 3286/5000] loss: 1.16919\n",
      "[Epoch 3287/5000] loss: 1.41235\n",
      "[Epoch 3288/5000] loss: 0.76143\n",
      "[Epoch 3289/5000] loss: 0.78278\n",
      "[Epoch 3290/5000] loss: 1.34016\n",
      "[Epoch 3291/5000] loss: 1.07721\n",
      "[Epoch 3292/5000] loss: 0.61787\n",
      "[Epoch 3293/5000] loss: 0.80165\n",
      "[Epoch 3294/5000] loss: 1.45423\n",
      "[Epoch 3295/5000] loss: 0.79983\n",
      "[Epoch 3296/5000] loss: 2.85703\n",
      "[Epoch 3297/5000] loss: 0.82309\n",
      "[Epoch 3298/5000] loss: 1.07836\n",
      "[Epoch 3299/5000] loss: 0.72730\n",
      "[Epoch 3300/5000] loss: 1.14306\n",
      "[Epoch 3301/5000] loss: 1.28238\n",
      "[Epoch 3302/5000] loss: 0.55902\n",
      "[Epoch 3303/5000] loss: 1.96325\n",
      "[Epoch 3304/5000] loss: 1.00298\n",
      "[Epoch 3305/5000] loss: 0.94857\n",
      "[Epoch 3306/5000] loss: 1.09729\n",
      "[Epoch 3307/5000] loss: 0.95782\n",
      "[Epoch 3308/5000] loss: 1.12587\n",
      "[Epoch 3309/5000] loss: 1.18482\n",
      "[Epoch 3310/5000] loss: 0.94332\n",
      "[Epoch 3311/5000] loss: 0.95162\n",
      "[Epoch 3312/5000] loss: 0.83694\n",
      "[Epoch 3313/5000] loss: 0.89766\n",
      "[Epoch 3314/5000] loss: 0.91027\n",
      "[Epoch 3315/5000] loss: 1.36348\n",
      "[Epoch 3316/5000] loss: 1.02200\n",
      "[Epoch 3317/5000] loss: 0.97251\n",
      "[Epoch 3318/5000] loss: 1.30657\n",
      "[Epoch 3319/5000] loss: 0.73526\n",
      "[Epoch 3320/5000] loss: 0.57848\n",
      "[Epoch 3321/5000] loss: 0.56297\n",
      "[Epoch 3322/5000] loss: 0.73982\n",
      "[Epoch 3323/5000] loss: 1.14694\n",
      "[Epoch 3324/5000] loss: 0.98870\n",
      "[Epoch 3325/5000] loss: 1.18551\n",
      "[Epoch 3326/5000] loss: 0.50077\n",
      "[Epoch 3327/5000] loss: 0.75983\n",
      "[Epoch 3328/5000] loss: 1.33016\n",
      "[Epoch 3329/5000] loss: 0.68707\n",
      "[Epoch 3330/5000] loss: 3.68881\n",
      "[Epoch 3331/5000] loss: 0.67290\n",
      "[Epoch 3332/5000] loss: 0.81734\n",
      "[Epoch 3333/5000] loss: 0.56486\n",
      "[Epoch 3334/5000] loss: 1.39705\n",
      "[Epoch 3335/5000] loss: 0.85565\n",
      "[Epoch 3336/5000] loss: 0.78370\n",
      "[Epoch 3337/5000] loss: 0.94252\n",
      "[Epoch 3338/5000] loss: 0.64639\n",
      "[Epoch 3339/5000] loss: 1.25387\n",
      "[Epoch 3340/5000] loss: 1.29709\n",
      "[Epoch 3341/5000] loss: 0.55065\n",
      "[Epoch 3342/5000] loss: 0.49328\n",
      "[Epoch 3343/5000] loss: 0.99273\n",
      "[Epoch 3344/5000] loss: 1.04059\n",
      "[Epoch 3345/5000] loss: 0.62837\n",
      "[Epoch 3346/5000] loss: 1.18050\n",
      "[Epoch 3347/5000] loss: 1.44568\n",
      "[Epoch 3348/5000] loss: 1.12863\n",
      "[Epoch 3349/5000] loss: 0.89722\n",
      "[Epoch 3350/5000] loss: 0.96691\n",
      "[Epoch 3351/5000] loss: 1.09275\n",
      "[Epoch 3352/5000] loss: 1.06265\n",
      "[Epoch 3353/5000] loss: 0.72652\n",
      "[Epoch 3354/5000] loss: 0.45078\n",
      "[Epoch 3355/5000] loss: 1.20074\n",
      "[Epoch 3356/5000] loss: 1.07424\n",
      "[Epoch 3357/5000] loss: 0.64767\n",
      "[Epoch 3358/5000] loss: 1.05486\n",
      "[Epoch 3359/5000] loss: 1.07994\n",
      "[Epoch 3360/5000] loss: 0.72417\n",
      "[Epoch 3361/5000] loss: 1.08090\n",
      "[Epoch 3362/5000] loss: 0.99392\n",
      "[Epoch 3363/5000] loss: 0.93816\n",
      "[Epoch 3364/5000] loss: 1.56352\n",
      "[Epoch 3365/5000] loss: 1.61771\n",
      "[Epoch 3366/5000] loss: 0.63535\n",
      "[Epoch 3367/5000] loss: 1.23973\n",
      "[Epoch 3368/5000] loss: 1.40569\n",
      "[Epoch 3369/5000] loss: 0.54988\n",
      "[Epoch 3370/5000] loss: 1.24800\n",
      "[Epoch 3371/5000] loss: 0.97327\n",
      "[Epoch 3372/5000] loss: 0.85808\n",
      "[Epoch 3373/5000] loss: 0.88266\n",
      "[Epoch 3374/5000] loss: 0.94905\n",
      "[Epoch 3375/5000] loss: 0.68285\n",
      "[Epoch 3376/5000] loss: 0.90247\n",
      "[Epoch 3377/5000] loss: 0.66748\n",
      "[Epoch 3378/5000] loss: 0.74098\n",
      "[Epoch 3379/5000] loss: 2.88883\n",
      "[Epoch 3380/5000] loss: 0.64559\n",
      "[Epoch 3381/5000] loss: 1.21903\n",
      "[Epoch 3382/5000] loss: 1.06738\n",
      "[Epoch 3383/5000] loss: 1.23139\n",
      "[Epoch 3384/5000] loss: 0.98062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3385/5000] loss: 1.55089\n",
      "[Epoch 3386/5000] loss: 0.60203\n",
      "[Epoch 3387/5000] loss: 0.78064\n",
      "[Epoch 3388/5000] loss: 0.88677\n",
      "[Epoch 3389/5000] loss: 1.09344\n",
      "[Epoch 3390/5000] loss: 0.75667\n",
      "[Epoch 3391/5000] loss: 0.57354\n",
      "[Epoch 3392/5000] loss: 1.21624\n",
      "[Epoch 3393/5000] loss: 0.72397\n",
      "[Epoch 3394/5000] loss: 0.99630\n",
      "[Epoch 3395/5000] loss: 1.39879\n",
      "[Epoch 3396/5000] loss: 0.41739\n",
      "[Epoch 3397/5000] loss: 3.03480\n",
      "[Epoch 3398/5000] loss: 0.59539\n",
      "[Epoch 3399/5000] loss: 0.77608\n",
      "[Epoch 3400/5000] loss: 1.02131\n",
      "[Epoch 3401/5000] loss: 1.50662\n",
      "[Epoch 3402/5000] loss: 0.78859\n",
      "[Epoch 3403/5000] loss: 1.26733\n",
      "[Epoch 3404/5000] loss: 1.26440\n",
      "[Epoch 3405/5000] loss: 0.98776\n",
      "[Epoch 3406/5000] loss: 0.78432\n",
      "[Epoch 3407/5000] loss: 1.40362\n",
      "[Epoch 3408/5000] loss: 1.02200\n",
      "[Epoch 3409/5000] loss: 0.80566\n",
      "[Epoch 3410/5000] loss: 0.93086\n",
      "[Epoch 3411/5000] loss: 0.90205\n",
      "[Epoch 3412/5000] loss: 0.83843\n",
      "[Epoch 3413/5000] loss: 0.81238\n",
      "[Epoch 3414/5000] loss: 0.88744\n",
      "[Epoch 3415/5000] loss: 0.84758\n",
      "[Epoch 3416/5000] loss: 0.93721\n",
      "[Epoch 3417/5000] loss: 0.93477\n",
      "[Epoch 3418/5000] loss: 0.69866\n",
      "[Epoch 3419/5000] loss: 0.87692\n",
      "[Epoch 3420/5000] loss: 1.03935\n",
      "[Epoch 3421/5000] loss: 0.81236\n",
      "[Epoch 3422/5000] loss: 1.13195\n",
      "[Epoch 3423/5000] loss: 0.48134\n",
      "[Epoch 3424/5000] loss: 0.80692\n",
      "[Epoch 3425/5000] loss: 0.61621\n",
      "[Epoch 3426/5000] loss: 0.76605\n",
      "[Epoch 3427/5000] loss: 0.94609\n",
      "[Epoch 3428/5000] loss: 0.94922\n",
      "[Epoch 3429/5000] loss: 3.36284\n",
      "[Epoch 3430/5000] loss: 1.12656\n",
      "[Epoch 3431/5000] loss: 0.60855\n",
      "[Epoch 3432/5000] loss: 1.07940\n",
      "[Epoch 3433/5000] loss: 1.11144\n",
      "[Epoch 3434/5000] loss: 0.77712\n",
      "[Epoch 3435/5000] loss: 0.59622\n",
      "[Epoch 3436/5000] loss: 0.67619\n",
      "[Epoch 3437/5000] loss: 1.29344\n",
      "[Epoch 3438/5000] loss: 0.55914\n",
      "[Epoch 3439/5000] loss: 0.78638\n",
      "[Epoch 3440/5000] loss: 0.77547\n",
      "[Epoch 3441/5000] loss: 0.79593\n",
      "[Epoch 3442/5000] loss: 1.62625\n",
      "[Epoch 3443/5000] loss: 3.63008\n",
      "[Epoch 3444/5000] loss: 0.83871\n",
      "[Epoch 3445/5000] loss: 1.02981\n",
      "[Epoch 3446/5000] loss: 1.11192\n",
      "[Epoch 3447/5000] loss: 1.30970\n",
      "[Epoch 3448/5000] loss: 0.72760\n",
      "[Epoch 3449/5000] loss: 1.23102\n",
      "[Epoch 3450/5000] loss: 0.67450\n",
      "[Epoch 3451/5000] loss: 0.58575\n",
      "[Epoch 3452/5000] loss: 0.86966\n",
      "[Epoch 3453/5000] loss: 1.02858\n",
      "[Epoch 3454/5000] loss: 1.25457\n",
      "[Epoch 3455/5000] loss: 0.61901\n",
      "[Epoch 3456/5000] loss: 0.71492\n",
      "[Epoch 3457/5000] loss: 0.67671\n",
      "[Epoch 3458/5000] loss: 0.76958\n",
      "[Epoch 3459/5000] loss: 0.69345\n",
      "[Epoch 3460/5000] loss: 1.58758\n",
      "[Epoch 3461/5000] loss: 0.69955\n",
      "[Epoch 3462/5000] loss: 1.20523\n",
      "[Epoch 3463/5000] loss: 1.32253\n",
      "[Epoch 3464/5000] loss: 0.75390\n",
      "[Epoch 3465/5000] loss: 0.43134\n",
      "[Epoch 3466/5000] loss: 1.07208\n",
      "[Epoch 3467/5000] loss: 1.18855\n",
      "[Epoch 3468/5000] loss: 0.71245\n",
      "[Epoch 3469/5000] loss: 0.95224\n",
      "[Epoch 3470/5000] loss: 0.72206\n",
      "[Epoch 3471/5000] loss: 1.46413\n",
      "[Epoch 3472/5000] loss: 0.73133\n",
      "[Epoch 3473/5000] loss: 0.88992\n",
      "[Epoch 3474/5000] loss: 3.67232\n",
      "[Epoch 3475/5000] loss: 1.40253\n",
      "[Epoch 3476/5000] loss: 0.57991\n",
      "[Epoch 3477/5000] loss: 1.06897\n",
      "[Epoch 3478/5000] loss: 0.88455\n",
      "[Epoch 3479/5000] loss: 0.78684\n",
      "[Epoch 3480/5000] loss: 0.68837\n",
      "[Epoch 3481/5000] loss: 0.81792\n",
      "[Epoch 3482/5000] loss: 1.01293\n",
      "[Epoch 3483/5000] loss: 0.55669\n",
      "[Epoch 3484/5000] loss: 2.92887\n",
      "[Epoch 3485/5000] loss: 1.03269\n",
      "[Epoch 3486/5000] loss: 0.59197\n",
      "[Epoch 3487/5000] loss: 1.32112\n",
      "[Epoch 3488/5000] loss: 1.36631\n",
      "[Epoch 3489/5000] loss: 0.89396\n",
      "[Epoch 3490/5000] loss: 0.93384\n",
      "[Epoch 3491/5000] loss: 1.31027\n",
      "[Epoch 3492/5000] loss: 1.26524\n",
      "[Epoch 3493/5000] loss: 1.16196\n",
      "[Epoch 3494/5000] loss: 0.51726\n",
      "[Epoch 3495/5000] loss: 0.62895\n",
      "[Epoch 3496/5000] loss: 0.63190\n",
      "[Epoch 3497/5000] loss: 0.93078\n",
      "[Epoch 3498/5000] loss: 0.90466\n",
      "[Epoch 3499/5000] loss: 0.90170\n",
      "[Epoch 3500/5000] loss: 0.72817\n",
      "[Epoch 3501/5000] loss: 1.17764\n",
      "[Epoch 3502/5000] loss: 1.20245\n",
      "[Epoch 3503/5000] loss: 0.79440\n",
      "[Epoch 3504/5000] loss: 1.32251\n",
      "[Epoch 3505/5000] loss: 0.32066\n",
      "[Epoch 3506/5000] loss: 1.24891\n",
      "[Epoch 3507/5000] loss: 0.63628\n",
      "[Epoch 3508/5000] loss: 1.06718\n",
      "[Epoch 3509/5000] loss: 0.68593\n",
      "[Epoch 3510/5000] loss: 0.87749\n",
      "[Epoch 3511/5000] loss: 0.72822\n",
      "[Epoch 3512/5000] loss: 0.81678\n",
      "[Epoch 3513/5000] loss: 1.12863\n",
      "[Epoch 3514/5000] loss: 1.44168\n",
      "[Epoch 3515/5000] loss: 3.38606\n",
      "[Epoch 3516/5000] loss: 0.78887\n",
      "[Epoch 3517/5000] loss: 0.82567\n",
      "[Epoch 3518/5000] loss: 1.21210\n",
      "[Epoch 3519/5000] loss: 1.25449\n",
      "[Epoch 3520/5000] loss: 0.83746\n",
      "[Epoch 3521/5000] loss: 1.54511\n",
      "[Epoch 3522/5000] loss: 0.80574\n",
      "[Epoch 3523/5000] loss: 1.47122\n",
      "[Epoch 3524/5000] loss: 0.90605\n",
      "[Epoch 3525/5000] loss: 0.87122\n",
      "[Epoch 3526/5000] loss: 1.03884\n",
      "[Epoch 3527/5000] loss: 1.46152\n",
      "[Epoch 3528/5000] loss: 1.07622\n",
      "[Epoch 3529/5000] loss: 0.90752\n",
      "[Epoch 3530/5000] loss: 1.01536\n",
      "[Epoch 3531/5000] loss: 0.82385\n",
      "[Epoch 3532/5000] loss: 1.15173\n",
      "[Epoch 3533/5000] loss: 1.06494\n",
      "[Epoch 3534/5000] loss: 1.27461\n",
      "[Epoch 3535/5000] loss: 1.20445\n",
      "[Epoch 3536/5000] loss: 3.38532\n",
      "[Epoch 3537/5000] loss: 0.54710\n",
      "[Epoch 3538/5000] loss: 0.98664\n",
      "[Epoch 3539/5000] loss: 0.81595\n",
      "[Epoch 3540/5000] loss: 1.52201\n",
      "[Epoch 3541/5000] loss: 1.27152\n",
      "[Epoch 3542/5000] loss: 1.07599\n",
      "[Epoch 3543/5000] loss: 1.24534\n",
      "[Epoch 3544/5000] loss: 1.10490\n",
      "[Epoch 3545/5000] loss: 0.81531\n",
      "[Epoch 3546/5000] loss: 0.63785\n",
      "[Epoch 3547/5000] loss: 0.66118\n",
      "[Epoch 3548/5000] loss: 1.37754\n",
      "[Epoch 3549/5000] loss: 1.17557\n",
      "[Epoch 3550/5000] loss: 0.31531\n",
      "[Epoch 3551/5000] loss: 2.96138\n",
      "[Epoch 3552/5000] loss: 0.52609\n",
      "[Epoch 3553/5000] loss: 1.20949\n",
      "[Epoch 3554/5000] loss: 0.37913\n",
      "[Epoch 3555/5000] loss: 0.79009\n",
      "[Epoch 3556/5000] loss: 1.11270\n",
      "[Epoch 3557/5000] loss: 0.68931\n",
      "[Epoch 3558/5000] loss: 0.75844\n",
      "[Epoch 3559/5000] loss: 0.86285\n",
      "[Epoch 3560/5000] loss: 0.73679\n",
      "[Epoch 3561/5000] loss: 0.73855\n",
      "[Epoch 3562/5000] loss: 0.75502\n",
      "[Epoch 3563/5000] loss: 0.59010\n",
      "[Epoch 3564/5000] loss: 1.74616\n",
      "[Epoch 3565/5000] loss: 0.94891\n",
      "[Epoch 3566/5000] loss: 1.12928\n",
      "[Epoch 3567/5000] loss: 1.37851\n",
      "[Epoch 3568/5000] loss: 1.44285\n",
      "[Epoch 3569/5000] loss: 0.91695\n",
      "[Epoch 3570/5000] loss: 0.92368\n",
      "[Epoch 3571/5000] loss: 0.57089\n",
      "[Epoch 3572/5000] loss: 0.69852\n",
      "[Epoch 3573/5000] loss: 0.59729\n",
      "[Epoch 3574/5000] loss: 0.68441\n",
      "[Epoch 3575/5000] loss: 0.78816\n",
      "[Epoch 3576/5000] loss: 1.02020\n",
      "[Epoch 3577/5000] loss: 0.82916\n",
      "[Epoch 3578/5000] loss: 1.34888\n",
      "[Epoch 3579/5000] loss: 0.55792\n",
      "[Epoch 3580/5000] loss: 3.36214\n",
      "[Epoch 3581/5000] loss: 0.99447\n",
      "[Epoch 3582/5000] loss: 0.76510\n",
      "[Epoch 3583/5000] loss: 1.60022\n",
      "[Epoch 3584/5000] loss: 0.94812\n",
      "[Epoch 3585/5000] loss: 0.83629\n",
      "[Epoch 3586/5000] loss: 1.12774\n",
      "[Epoch 3587/5000] loss: 1.11124\n",
      "[Epoch 3588/5000] loss: 0.91710\n",
      "[Epoch 3589/5000] loss: 1.08045\n",
      "[Epoch 3590/5000] loss: 1.33947\n",
      "[Epoch 3591/5000] loss: 0.71080\n",
      "[Epoch 3592/5000] loss: 0.82693\n",
      "[Epoch 3593/5000] loss: 1.08697\n",
      "[Epoch 3594/5000] loss: 1.14275\n",
      "[Epoch 3595/5000] loss: 0.91738\n",
      "[Epoch 3596/5000] loss: 0.70162\n",
      "[Epoch 3597/5000] loss: 0.91048\n",
      "[Epoch 3598/5000] loss: 0.95328\n",
      "[Epoch 3599/5000] loss: 1.10309\n",
      "[Epoch 3600/5000] loss: 1.34428\n",
      "[Epoch 3601/5000] loss: 1.02599\n",
      "[Epoch 3602/5000] loss: 0.83572\n",
      "[Epoch 3603/5000] loss: 1.06920\n",
      "[Epoch 3604/5000] loss: 1.01103\n",
      "[Epoch 3605/5000] loss: 0.51820\n",
      "[Epoch 3606/5000] loss: 1.05210\n",
      "[Epoch 3607/5000] loss: 1.09812\n",
      "[Epoch 3608/5000] loss: 1.11872\n",
      "[Epoch 3609/5000] loss: 0.82254\n",
      "[Epoch 3610/5000] loss: 1.00918\n",
      "[Epoch 3611/5000] loss: 3.13361\n",
      "[Epoch 3612/5000] loss: 0.49372\n",
      "[Epoch 3613/5000] loss: 0.71096\n",
      "[Epoch 3614/5000] loss: 0.88688\n",
      "[Epoch 3615/5000] loss: 0.62489\n",
      "[Epoch 3616/5000] loss: 1.34175\n",
      "[Epoch 3617/5000] loss: 1.08763\n",
      "[Epoch 3618/5000] loss: 0.54850\n",
      "[Epoch 3619/5000] loss: 1.01918\n",
      "[Epoch 3620/5000] loss: 0.68942\n",
      "[Epoch 3621/5000] loss: 0.77704\n",
      "[Epoch 3622/5000] loss: 1.01487\n",
      "[Epoch 3623/5000] loss: 1.43550\n",
      "[Epoch 3624/5000] loss: 1.19396\n",
      "[Epoch 3625/5000] loss: 0.87425\n",
      "[Epoch 3626/5000] loss: 0.60574\n",
      "[Epoch 3627/5000] loss: 0.76503\n",
      "[Epoch 3628/5000] loss: 0.84973\n",
      "[Epoch 3629/5000] loss: 0.71321\n",
      "[Epoch 3630/5000] loss: 0.47684\n",
      "[Epoch 3631/5000] loss: 1.70822\n",
      "[Epoch 3632/5000] loss: 0.58814\n",
      "[Epoch 3633/5000] loss: 0.79529\n",
      "[Epoch 3634/5000] loss: 1.01301\n",
      "[Epoch 3635/5000] loss: 2.99810\n",
      "[Epoch 3636/5000] loss: 1.12898\n",
      "[Epoch 3637/5000] loss: 0.71370\n",
      "[Epoch 3638/5000] loss: 1.61908\n",
      "[Epoch 3639/5000] loss: 0.62381\n",
      "[Epoch 3640/5000] loss: 0.59769\n",
      "[Epoch 3641/5000] loss: 0.91943\n",
      "[Epoch 3642/5000] loss: 0.43129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3643/5000] loss: 0.84350\n",
      "[Epoch 3644/5000] loss: 1.00439\n",
      "[Epoch 3645/5000] loss: 0.88021\n",
      "[Epoch 3646/5000] loss: 0.78978\n",
      "[Epoch 3647/5000] loss: 0.98946\n",
      "[Epoch 3648/5000] loss: 0.89198\n",
      "[Epoch 3649/5000] loss: 0.94497\n",
      "[Epoch 3650/5000] loss: 1.37786\n",
      "[Epoch 3651/5000] loss: 1.01036\n",
      "[Epoch 3652/5000] loss: 0.60960\n",
      "[Epoch 3653/5000] loss: 0.55359\n",
      "[Epoch 3654/5000] loss: 0.90446\n",
      "[Epoch 3655/5000] loss: 1.28511\n",
      "[Epoch 3656/5000] loss: 0.42573\n",
      "[Epoch 3657/5000] loss: 0.73569\n",
      "[Epoch 3658/5000] loss: 1.30844\n",
      "[Epoch 3659/5000] loss: 0.71449\n",
      "[Epoch 3660/5000] loss: 1.02092\n",
      "[Epoch 3661/5000] loss: 0.55276\n",
      "[Epoch 3662/5000] loss: 0.80383\n",
      "[Epoch 3663/5000] loss: 0.92419\n",
      "[Epoch 3664/5000] loss: 0.89236\n",
      "[Epoch 3665/5000] loss: 1.03244\n",
      "[Epoch 3666/5000] loss: 1.33978\n",
      "[Epoch 3667/5000] loss: 1.10998\n",
      "[Epoch 3668/5000] loss: 1.04942\n",
      "[Epoch 3669/5000] loss: 3.09103\n",
      "[Epoch 3670/5000] loss: 1.18268\n",
      "[Epoch 3671/5000] loss: 1.00800\n",
      "[Epoch 3672/5000] loss: 0.82050\n",
      "[Epoch 3673/5000] loss: 0.88404\n",
      "[Epoch 3674/5000] loss: 1.32864\n",
      "[Epoch 3675/5000] loss: 1.30267\n",
      "[Epoch 3676/5000] loss: 0.78218\n",
      "[Epoch 3677/5000] loss: 0.92342\n",
      "[Epoch 3678/5000] loss: 1.12048\n",
      "[Epoch 3679/5000] loss: 1.54421\n",
      "[Epoch 3680/5000] loss: 1.08717\n",
      "[Epoch 3681/5000] loss: 0.64905\n",
      "[Epoch 3682/5000] loss: 1.07490\n",
      "[Epoch 3683/5000] loss: 1.05755\n",
      "[Epoch 3684/5000] loss: 1.27968\n",
      "[Epoch 3685/5000] loss: 1.03690\n",
      "[Epoch 3686/5000] loss: 1.41155\n",
      "[Epoch 3687/5000] loss: 0.94587\n",
      "[Epoch 3688/5000] loss: 0.72805\n",
      "[Epoch 3689/5000] loss: 0.66338\n",
      "[Epoch 3690/5000] loss: 0.98485\n",
      "[Epoch 3691/5000] loss: 0.93210\n",
      "[Epoch 3692/5000] loss: 0.99650\n",
      "[Epoch 3693/5000] loss: 1.11529\n",
      "[Epoch 3694/5000] loss: 1.87421\n",
      "[Epoch 3695/5000] loss: 1.44403\n",
      "[Epoch 3696/5000] loss: 0.58875\n",
      "[Epoch 3697/5000] loss: 0.38613\n",
      "[Epoch 3698/5000] loss: 0.87224\n",
      "[Epoch 3699/5000] loss: 0.71556\n",
      "[Epoch 3700/5000] loss: 0.87447\n",
      "[Epoch 3701/5000] loss: 0.65785\n",
      "[Epoch 3702/5000] loss: 1.39530\n",
      "[Epoch 3703/5000] loss: 0.73752\n",
      "[Epoch 3704/5000] loss: 0.77118\n",
      "[Epoch 3705/5000] loss: 0.93010\n",
      "[Epoch 3706/5000] loss: 1.34785\n",
      "[Epoch 3707/5000] loss: 1.14432\n",
      "[Epoch 3708/5000] loss: 0.69987\n",
      "[Epoch 3709/5000] loss: 0.62245\n",
      "[Epoch 3710/5000] loss: 0.73086\n",
      "[Epoch 3711/5000] loss: 0.84513\n",
      "[Epoch 3712/5000] loss: 1.30222\n",
      "[Epoch 3713/5000] loss: 0.65896\n",
      "[Epoch 3714/5000] loss: 3.82194\n",
      "[Epoch 3715/5000] loss: 0.90725\n",
      "[Epoch 3716/5000] loss: 0.69563\n",
      "[Epoch 3717/5000] loss: 1.35836\n",
      "[Epoch 3718/5000] loss: 0.90248\n",
      "[Epoch 3719/5000] loss: 1.10238\n",
      "[Epoch 3720/5000] loss: 1.56512\n",
      "[Epoch 3721/5000] loss: 0.67182\n",
      "[Epoch 3722/5000] loss: 0.99344\n",
      "[Epoch 3723/5000] loss: 1.10441\n",
      "[Epoch 3724/5000] loss: 0.77339\n",
      "[Epoch 3725/5000] loss: 1.47392\n",
      "[Epoch 3726/5000] loss: 1.45108\n",
      "[Epoch 3727/5000] loss: 1.56686\n",
      "[Epoch 3728/5000] loss: 2.07711\n",
      "[Epoch 3729/5000] loss: 1.05351\n",
      "[Epoch 3730/5000] loss: 0.89944\n",
      "[Epoch 3731/5000] loss: 0.86889\n",
      "[Epoch 3732/5000] loss: 0.87763\n",
      "[Epoch 3733/5000] loss: 0.91073\n",
      "[Epoch 3734/5000] loss: 1.17500\n",
      "[Epoch 3735/5000] loss: 0.96475\n",
      "[Epoch 3736/5000] loss: 1.60854\n",
      "[Epoch 3737/5000] loss: 0.44273\n",
      "[Epoch 3738/5000] loss: 0.92404\n",
      "[Epoch 3739/5000] loss: 0.73720\n",
      "[Epoch 3740/5000] loss: 0.89196\n",
      "[Epoch 3741/5000] loss: 1.59885\n",
      "[Epoch 3742/5000] loss: 1.29504\n",
      "[Epoch 3743/5000] loss: 0.68071\n",
      "[Epoch 3744/5000] loss: 0.69089\n",
      "[Epoch 3745/5000] loss: 0.72737\n",
      "[Epoch 3746/5000] loss: 0.80971\n",
      "[Epoch 3747/5000] loss: 0.75895\n",
      "[Epoch 3748/5000] loss: 0.79994\n",
      "[Epoch 3749/5000] loss: 0.61125\n",
      "[Epoch 3750/5000] loss: 0.67277\n",
      "[Epoch 3751/5000] loss: 1.73777\n",
      "[Epoch 3752/5000] loss: 0.92606\n",
      "[Epoch 3753/5000] loss: 0.80187\n",
      "[Epoch 3754/5000] loss: 1.38964\n",
      "[Epoch 3755/5000] loss: 0.89962\n",
      "[Epoch 3756/5000] loss: 0.76539\n",
      "[Epoch 3757/5000] loss: 0.85689\n",
      "[Epoch 3758/5000] loss: 0.44280\n",
      "[Epoch 3759/5000] loss: 0.61259\n",
      "[Epoch 3760/5000] loss: 0.58706\n",
      "[Epoch 3761/5000] loss: 0.80825\n",
      "[Epoch 3762/5000] loss: 1.03510\n",
      "[Epoch 3763/5000] loss: 1.05509\n",
      "[Epoch 3764/5000] loss: 0.80368\n",
      "[Epoch 3765/5000] loss: 0.80496\n",
      "[Epoch 3766/5000] loss: 0.57714\n",
      "[Epoch 3767/5000] loss: 0.97481\n",
      "[Epoch 3768/5000] loss: 0.87285\n",
      "[Epoch 3769/5000] loss: 0.75505\n",
      "[Epoch 3770/5000] loss: 0.98836\n",
      "[Epoch 3771/5000] loss: 1.07491\n",
      "[Epoch 3772/5000] loss: 1.20098\n",
      "[Epoch 3773/5000] loss: 1.09882\n",
      "[Epoch 3774/5000] loss: 0.77465\n",
      "[Epoch 3775/5000] loss: 0.73311\n",
      "[Epoch 3776/5000] loss: 0.94914\n",
      "[Epoch 3777/5000] loss: 0.95618\n",
      "[Epoch 3778/5000] loss: 0.85356\n",
      "[Epoch 3779/5000] loss: 0.72578\n",
      "[Epoch 3780/5000] loss: 0.41290\n",
      "[Epoch 3781/5000] loss: 0.88482\n",
      "[Epoch 3782/5000] loss: 1.08780\n",
      "[Epoch 3783/5000] loss: 1.20736\n",
      "[Epoch 3784/5000] loss: 0.88071\n",
      "[Epoch 3785/5000] loss: 0.75883\n",
      "[Epoch 3786/5000] loss: 0.84179\n",
      "[Epoch 3787/5000] loss: 0.93481\n",
      "[Epoch 3788/5000] loss: 1.21340\n",
      "[Epoch 3789/5000] loss: 0.78443\n",
      "[Epoch 3790/5000] loss: 3.07929\n",
      "[Epoch 3791/5000] loss: 0.96781\n",
      "[Epoch 3792/5000] loss: 0.97973\n",
      "[Epoch 3793/5000] loss: 0.65186\n",
      "[Epoch 3794/5000] loss: 1.19272\n",
      "[Epoch 3795/5000] loss: 3.12876\n",
      "[Epoch 3796/5000] loss: 1.09982\n",
      "[Epoch 3797/5000] loss: 0.87559\n",
      "[Epoch 3798/5000] loss: 1.95146\n",
      "[Epoch 3799/5000] loss: 1.12317\n",
      "[Epoch 3800/5000] loss: 0.78151\n",
      "[Epoch 3801/5000] loss: 0.87074\n",
      "[Epoch 3802/5000] loss: 0.44260\n",
      "[Epoch 3803/5000] loss: 1.20246\n",
      "[Epoch 3804/5000] loss: 1.16392\n",
      "[Epoch 3805/5000] loss: 0.75578\n",
      "[Epoch 3806/5000] loss: 1.37559\n",
      "[Epoch 3807/5000] loss: 1.54272\n",
      "[Epoch 3808/5000] loss: 0.95701\n",
      "[Epoch 3809/5000] loss: 1.01967\n",
      "[Epoch 3810/5000] loss: 1.02487\n",
      "[Epoch 3811/5000] loss: 1.05132\n",
      "[Epoch 3812/5000] loss: 1.05060\n",
      "[Epoch 3813/5000] loss: 0.75333\n",
      "[Epoch 3814/5000] loss: 1.69554\n",
      "[Epoch 3815/5000] loss: 1.01888\n",
      "[Epoch 3816/5000] loss: 1.71839\n",
      "[Epoch 3817/5000] loss: 1.01447\n",
      "[Epoch 3818/5000] loss: 1.15578\n",
      "[Epoch 3819/5000] loss: 1.27275\n",
      "[Epoch 3820/5000] loss: 3.08731\n",
      "[Epoch 3821/5000] loss: 0.88214\n",
      "[Epoch 3822/5000] loss: 0.92930\n",
      "[Epoch 3823/5000] loss: 0.93155\n",
      "[Epoch 3824/5000] loss: 0.93271\n",
      "[Epoch 3825/5000] loss: 1.07926\n",
      "[Epoch 3826/5000] loss: 0.65307\n",
      "[Epoch 3827/5000] loss: 0.93523\n",
      "[Epoch 3828/5000] loss: 0.67260\n",
      "[Epoch 3829/5000] loss: 0.65930\n",
      "[Epoch 3830/5000] loss: 0.82883\n",
      "[Epoch 3831/5000] loss: 3.43034\n",
      "[Epoch 3832/5000] loss: 0.49723\n",
      "[Epoch 3833/5000] loss: 0.90133\n",
      "[Epoch 3834/5000] loss: 0.70289\n",
      "[Epoch 3835/5000] loss: 1.01662\n",
      "[Epoch 3836/5000] loss: 1.29712\n",
      "[Epoch 3837/5000] loss: 1.03069\n",
      "[Epoch 3838/5000] loss: 0.61321\n",
      "[Epoch 3839/5000] loss: 0.85122\n",
      "[Epoch 3840/5000] loss: 0.69037\n",
      "[Epoch 3841/5000] loss: 0.37248\n",
      "[Epoch 3842/5000] loss: 1.30783\n",
      "[Epoch 3843/5000] loss: 1.15684\n",
      "[Epoch 3844/5000] loss: 0.99694\n",
      "[Epoch 3845/5000] loss: 1.12769\n",
      "[Epoch 3846/5000] loss: 0.82721\n",
      "[Epoch 3847/5000] loss: 0.62065\n",
      "[Epoch 3848/5000] loss: 0.98908\n",
      "[Epoch 3849/5000] loss: 0.90608\n",
      "[Epoch 3850/5000] loss: 0.91404\n",
      "[Epoch 3851/5000] loss: 0.56529\n",
      "[Epoch 3852/5000] loss: 0.91595\n",
      "[Epoch 3853/5000] loss: 3.45588\n",
      "[Epoch 3854/5000] loss: 0.64428\n",
      "[Epoch 3855/5000] loss: 0.67478\n",
      "[Epoch 3856/5000] loss: 0.84039\n",
      "[Epoch 3857/5000] loss: 0.87913\n",
      "[Epoch 3858/5000] loss: 0.84216\n",
      "[Epoch 3859/5000] loss: 0.79802\n",
      "[Epoch 3860/5000] loss: 0.75652\n",
      "[Epoch 3861/5000] loss: 0.67617\n",
      "[Epoch 3862/5000] loss: 1.03934\n",
      "[Epoch 3863/5000] loss: 1.12134\n",
      "[Epoch 3864/5000] loss: 1.67741\n",
      "[Epoch 3865/5000] loss: 0.32019\n",
      "[Epoch 3866/5000] loss: 1.21106\n",
      "[Epoch 3867/5000] loss: 1.15292\n",
      "[Epoch 3868/5000] loss: 0.48089\n",
      "[Epoch 3869/5000] loss: 1.02539\n",
      "[Epoch 3870/5000] loss: 0.87476\n",
      "[Epoch 3871/5000] loss: 0.95437\n",
      "[Epoch 3872/5000] loss: 0.76801\n",
      "[Epoch 3873/5000] loss: 1.01450\n",
      "[Epoch 3874/5000] loss: 0.70721\n",
      "[Epoch 3875/5000] loss: 1.22470\n",
      "[Epoch 3876/5000] loss: 1.14431\n",
      "[Epoch 3877/5000] loss: 0.59314\n",
      "[Epoch 3878/5000] loss: 1.24330\n",
      "[Epoch 3879/5000] loss: 1.22692\n",
      "[Epoch 3880/5000] loss: 0.76809\n",
      "[Epoch 3881/5000] loss: 0.99306\n",
      "[Epoch 3882/5000] loss: 0.99879\n",
      "[Epoch 3883/5000] loss: 1.25582\n",
      "[Epoch 3884/5000] loss: 1.53456\n",
      "[Epoch 3885/5000] loss: 1.28418\n",
      "[Epoch 3886/5000] loss: 0.75409\n",
      "[Epoch 3887/5000] loss: 1.15327\n",
      "[Epoch 3888/5000] loss: 1.14532\n",
      "[Epoch 3889/5000] loss: 1.05139\n",
      "[Epoch 3890/5000] loss: 0.56880\n",
      "[Epoch 3891/5000] loss: 0.68269\n",
      "[Epoch 3892/5000] loss: 1.40958\n",
      "[Epoch 3893/5000] loss: 0.89739\n",
      "[Epoch 3894/5000] loss: 0.79170\n",
      "[Epoch 3895/5000] loss: 1.49574\n",
      "[Epoch 3896/5000] loss: 0.91963\n",
      "[Epoch 3897/5000] loss: 0.70754\n",
      "[Epoch 3898/5000] loss: 0.79526\n",
      "[Epoch 3899/5000] loss: 1.61769\n",
      "[Epoch 3900/5000] loss: 1.01790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3901/5000] loss: 0.70402\n",
      "[Epoch 3902/5000] loss: 1.26430\n",
      "[Epoch 3903/5000] loss: 1.10509\n",
      "[Epoch 3904/5000] loss: 3.04701\n",
      "[Epoch 3905/5000] loss: 3.42857\n",
      "[Epoch 3906/5000] loss: 1.64901\n",
      "[Epoch 3907/5000] loss: 0.98100\n",
      "[Epoch 3908/5000] loss: 0.96723\n",
      "[Epoch 3909/5000] loss: 1.40544\n",
      "[Epoch 3910/5000] loss: 0.96822\n",
      "[Epoch 3911/5000] loss: 0.41553\n",
      "[Epoch 3912/5000] loss: 0.82354\n",
      "[Epoch 3913/5000] loss: 0.35276\n",
      "[Epoch 3914/5000] loss: 0.73207\n",
      "[Epoch 3915/5000] loss: 0.98745\n",
      "[Epoch 3916/5000] loss: 0.55474\n",
      "[Epoch 3917/5000] loss: 0.87231\n",
      "[Epoch 3918/5000] loss: 1.07361\n",
      "[Epoch 3919/5000] loss: 1.48136\n",
      "[Epoch 3920/5000] loss: 1.05549\n",
      "[Epoch 3921/5000] loss: 1.38471\n",
      "[Epoch 3922/5000] loss: 0.80411\n",
      "[Epoch 3923/5000] loss: 1.39793\n",
      "[Epoch 3924/5000] loss: 0.86993\n",
      "[Epoch 3925/5000] loss: 1.21954\n",
      "[Epoch 3926/5000] loss: 1.19083\n",
      "[Epoch 3927/5000] loss: 0.63252\n",
      "[Epoch 3928/5000] loss: 0.74559\n",
      "[Epoch 3929/5000] loss: 1.09223\n",
      "[Epoch 3930/5000] loss: 1.28784\n",
      "[Epoch 3931/5000] loss: 1.21420\n",
      "[Epoch 3932/5000] loss: 1.14618\n",
      "[Epoch 3933/5000] loss: 0.83697\n",
      "[Epoch 3934/5000] loss: 0.63466\n",
      "[Epoch 3935/5000] loss: 0.69519\n",
      "[Epoch 3936/5000] loss: 0.63970\n",
      "[Epoch 3937/5000] loss: 1.26985\n",
      "[Epoch 3938/5000] loss: 0.81421\n",
      "[Epoch 3939/5000] loss: 1.05549\n",
      "[Epoch 3940/5000] loss: 0.87773\n",
      "[Epoch 3941/5000] loss: 0.86260\n",
      "[Epoch 3942/5000] loss: 1.04654\n",
      "[Epoch 3943/5000] loss: 1.16986\n",
      "[Epoch 3944/5000] loss: 1.02171\n",
      "[Epoch 3945/5000] loss: 1.01800\n",
      "[Epoch 3946/5000] loss: 1.35071\n",
      "[Epoch 3947/5000] loss: 0.92498\n",
      "[Epoch 3948/5000] loss: 0.71524\n",
      "[Epoch 3949/5000] loss: 0.82426\n",
      "[Epoch 3950/5000] loss: 0.77773\n",
      "[Epoch 3951/5000] loss: 1.03311\n",
      "[Epoch 3952/5000] loss: 0.96871\n",
      "[Epoch 3953/5000] loss: 1.22719\n",
      "[Epoch 3954/5000] loss: 0.83257\n",
      "[Epoch 3955/5000] loss: 0.63164\n",
      "[Epoch 3956/5000] loss: 0.71540\n",
      "[Epoch 3957/5000] loss: 1.47586\n",
      "[Epoch 3958/5000] loss: 1.08577\n",
      "[Epoch 3959/5000] loss: 1.27287\n",
      "[Epoch 3960/5000] loss: 0.96364\n",
      "[Epoch 3961/5000] loss: 0.71394\n",
      "[Epoch 3962/5000] loss: 0.63055\n",
      "[Epoch 3963/5000] loss: 1.02665\n",
      "[Epoch 3964/5000] loss: 0.93009\n",
      "[Epoch 3965/5000] loss: 0.96518\n",
      "[Epoch 3966/5000] loss: 0.49036\n",
      "[Epoch 3967/5000] loss: 0.82448\n",
      "[Epoch 3968/5000] loss: 1.55511\n",
      "[Epoch 3969/5000] loss: 0.72969\n",
      "[Epoch 3970/5000] loss: 1.03652\n",
      "[Epoch 3971/5000] loss: 0.49340\n",
      "[Epoch 3972/5000] loss: 1.10534\n",
      "[Epoch 3973/5000] loss: 0.68431\n",
      "[Epoch 3974/5000] loss: 0.63344\n",
      "[Epoch 3975/5000] loss: 1.07263\n",
      "[Epoch 3976/5000] loss: 0.63827\n",
      "[Epoch 3977/5000] loss: 0.80630\n",
      "[Epoch 3978/5000] loss: 0.72175\n",
      "[Epoch 3979/5000] loss: 0.33097\n",
      "[Epoch 3980/5000] loss: 0.81440\n",
      "[Epoch 3981/5000] loss: 1.29608\n",
      "[Epoch 3982/5000] loss: 1.66377\n",
      "[Epoch 3983/5000] loss: 1.06410\n",
      "[Epoch 3984/5000] loss: 1.10523\n",
      "[Epoch 3985/5000] loss: 0.63038\n",
      "[Epoch 3986/5000] loss: 1.21024\n",
      "[Epoch 3987/5000] loss: 0.94147\n",
      "[Epoch 3988/5000] loss: 1.42218\n",
      "[Epoch 3989/5000] loss: 0.74810\n",
      "[Epoch 3990/5000] loss: 0.71474\n",
      "[Epoch 3991/5000] loss: 0.62974\n",
      "[Epoch 3992/5000] loss: 0.91604\n",
      "[Epoch 3993/5000] loss: 0.56022\n",
      "[Epoch 3994/5000] loss: 0.53376\n",
      "[Epoch 3995/5000] loss: 0.91614\n",
      "[Epoch 3996/5000] loss: 0.92586\n",
      "[Epoch 3997/5000] loss: 0.74987\n",
      "[Epoch 3998/5000] loss: 1.02140\n",
      "[Epoch 3999/5000] loss: 0.96671\n",
      "[Epoch 4000/5000] loss: 1.03399\n",
      "[Epoch 4001/5000] loss: 0.66488\n",
      "[Epoch 4002/5000] loss: 0.75178\n",
      "[Epoch 4003/5000] loss: 1.02167\n",
      "[Epoch 4004/5000] loss: 0.75790\n",
      "[Epoch 4005/5000] loss: 0.71674\n",
      "[Epoch 4006/5000] loss: 0.91306\n",
      "[Epoch 4007/5000] loss: 0.51191\n",
      "[Epoch 4008/5000] loss: 1.08156\n",
      "[Epoch 4009/5000] loss: 0.96304\n",
      "[Epoch 4010/5000] loss: 0.93423\n",
      "[Epoch 4011/5000] loss: 0.49307\n",
      "[Epoch 4012/5000] loss: 0.78433\n",
      "[Epoch 4013/5000] loss: 0.85241\n",
      "[Epoch 4014/5000] loss: 0.92542\n",
      "[Epoch 4015/5000] loss: 1.19957\n",
      "[Epoch 4016/5000] loss: 1.41303\n",
      "[Epoch 4017/5000] loss: 0.75151\n",
      "[Epoch 4018/5000] loss: 0.62053\n",
      "[Epoch 4019/5000] loss: 0.83733\n",
      "[Epoch 4020/5000] loss: 0.76482\n",
      "[Epoch 4021/5000] loss: 0.90482\n",
      "[Epoch 4022/5000] loss: 3.32069\n",
      "[Epoch 4023/5000] loss: 0.82548\n",
      "[Epoch 4024/5000] loss: 0.80719\n",
      "[Epoch 4025/5000] loss: 0.85414\n",
      "[Epoch 4026/5000] loss: 1.01221\n",
      "[Epoch 4027/5000] loss: 0.22869\n",
      "[Epoch 4028/5000] loss: 0.60403\n",
      "[Epoch 4029/5000] loss: 0.77830\n",
      "[Epoch 4030/5000] loss: 0.80280\n",
      "[Epoch 4031/5000] loss: 0.80757\n",
      "[Epoch 4032/5000] loss: 1.00006\n",
      "[Epoch 4033/5000] loss: 0.81433\n",
      "[Epoch 4034/5000] loss: 1.11650\n",
      "[Epoch 4035/5000] loss: 1.00368\n",
      "[Epoch 4036/5000] loss: 1.11769\n",
      "[Epoch 4037/5000] loss: 1.27336\n",
      "[Epoch 4038/5000] loss: 0.99026\n",
      "[Epoch 4039/5000] loss: 1.20456\n",
      "[Epoch 4040/5000] loss: 1.15632\n",
      "[Epoch 4041/5000] loss: 0.95278\n",
      "[Epoch 4042/5000] loss: 0.85359\n",
      "[Epoch 4043/5000] loss: 1.26543\n",
      "[Epoch 4044/5000] loss: 0.67108\n",
      "[Epoch 4045/5000] loss: 0.83074\n",
      "[Epoch 4046/5000] loss: 1.12966\n",
      "[Epoch 4047/5000] loss: 0.49822\n",
      "[Epoch 4048/5000] loss: 0.82473\n",
      "[Epoch 4049/5000] loss: 0.78362\n",
      "[Epoch 4050/5000] loss: 0.93647\n",
      "[Epoch 4051/5000] loss: 1.19062\n",
      "[Epoch 4052/5000] loss: 0.77392\n",
      "[Epoch 4053/5000] loss: 0.72484\n",
      "[Epoch 4054/5000] loss: 1.06113\n",
      "[Epoch 4055/5000] loss: 0.99929\n",
      "[Epoch 4056/5000] loss: 0.78316\n",
      "[Epoch 4057/5000] loss: 1.18971\n",
      "[Epoch 4058/5000] loss: 0.89576\n",
      "[Epoch 4059/5000] loss: 0.86010\n",
      "[Epoch 4060/5000] loss: 1.01118\n",
      "[Epoch 4061/5000] loss: 0.50789\n",
      "[Epoch 4062/5000] loss: 1.14361\n",
      "[Epoch 4063/5000] loss: 0.64519\n",
      "[Epoch 4064/5000] loss: 1.02738\n",
      "[Epoch 4065/5000] loss: 1.10332\n",
      "[Epoch 4066/5000] loss: 0.86119\n",
      "[Epoch 4067/5000] loss: 0.96546\n",
      "[Epoch 4068/5000] loss: 1.18792\n",
      "[Epoch 4069/5000] loss: 1.16352\n",
      "[Epoch 4070/5000] loss: 0.78093\n",
      "[Epoch 4071/5000] loss: 0.88262\n",
      "[Epoch 4072/5000] loss: 1.41166\n",
      "[Epoch 4073/5000] loss: 1.28238\n",
      "[Epoch 4074/5000] loss: 1.01406\n",
      "[Epoch 4075/5000] loss: 0.54484\n",
      "[Epoch 4076/5000] loss: 1.39821\n",
      "[Epoch 4077/5000] loss: 2.99518\n",
      "[Epoch 4078/5000] loss: 0.82262\n",
      "[Epoch 4079/5000] loss: 1.27298\n",
      "[Epoch 4080/5000] loss: 0.68456\n",
      "[Epoch 4081/5000] loss: 1.07282\n",
      "[Epoch 4082/5000] loss: 0.46070\n",
      "[Epoch 4083/5000] loss: 0.58307\n",
      "[Epoch 4084/5000] loss: 1.22662\n",
      "[Epoch 4085/5000] loss: 0.66702\n",
      "[Epoch 4086/5000] loss: 0.59331\n",
      "[Epoch 4087/5000] loss: 1.57481\n",
      "[Epoch 4088/5000] loss: 1.09386\n",
      "[Epoch 4089/5000] loss: 0.57508\n",
      "[Epoch 4090/5000] loss: 1.08094\n",
      "[Epoch 4091/5000] loss: 0.58824\n",
      "[Epoch 4092/5000] loss: 1.37466\n",
      "[Epoch 4093/5000] loss: 1.05428\n",
      "[Epoch 4094/5000] loss: 0.88751\n",
      "[Epoch 4095/5000] loss: 1.02018\n",
      "[Epoch 4096/5000] loss: 1.41202\n",
      "[Epoch 4097/5000] loss: 0.88768\n",
      "[Epoch 4098/5000] loss: 0.74554\n",
      "[Epoch 4099/5000] loss: 1.12164\n",
      "[Epoch 4100/5000] loss: 0.66816\n",
      "[Epoch 4101/5000] loss: 1.09385\n",
      "[Epoch 4102/5000] loss: 1.24063\n",
      "[Epoch 4103/5000] loss: 1.15725\n",
      "[Epoch 4104/5000] loss: 1.05678\n",
      "[Epoch 4105/5000] loss: 1.10998\n",
      "[Epoch 4106/5000] loss: 1.42605\n",
      "[Epoch 4107/5000] loss: 1.62437\n",
      "[Epoch 4108/5000] loss: 0.75586\n",
      "[Epoch 4109/5000] loss: 0.87580\n",
      "[Epoch 4110/5000] loss: 0.57719\n",
      "[Epoch 4111/5000] loss: 0.42821\n",
      "[Epoch 4112/5000] loss: 1.04975\n",
      "[Epoch 4113/5000] loss: 0.99694\n",
      "[Epoch 4114/5000] loss: 0.62064\n",
      "[Epoch 4115/5000] loss: 0.73582\n",
      "[Epoch 4116/5000] loss: 0.95878\n",
      "[Epoch 4117/5000] loss: 0.61731\n",
      "[Epoch 4118/5000] loss: 1.25375\n",
      "[Epoch 4119/5000] loss: 0.56655\n",
      "[Epoch 4120/5000] loss: 1.15459\n",
      "[Epoch 4121/5000] loss: 0.82003\n",
      "[Epoch 4122/5000] loss: 0.76321\n",
      "[Epoch 4123/5000] loss: 0.76165\n",
      "[Epoch 4124/5000] loss: 1.19173\n",
      "[Epoch 4125/5000] loss: 1.04078\n",
      "[Epoch 4126/5000] loss: 0.54184\n",
      "[Epoch 4127/5000] loss: 0.74869\n",
      "[Epoch 4128/5000] loss: 1.89696\n",
      "[Epoch 4129/5000] loss: 0.74534\n",
      "[Epoch 4130/5000] loss: 1.00740\n",
      "[Epoch 4131/5000] loss: 0.81845\n",
      "[Epoch 4132/5000] loss: 0.59930\n",
      "[Epoch 4133/5000] loss: 0.71812\n",
      "[Epoch 4134/5000] loss: 1.02601\n",
      "[Epoch 4135/5000] loss: 1.05348\n",
      "[Epoch 4136/5000] loss: 0.74246\n",
      "[Epoch 4137/5000] loss: 1.12181\n",
      "[Epoch 4138/5000] loss: 0.78148\n",
      "[Epoch 4139/5000] loss: 1.41613\n",
      "[Epoch 4140/5000] loss: 0.78970\n",
      "[Epoch 4141/5000] loss: 0.46115\n",
      "[Epoch 4142/5000] loss: 0.57037\n",
      "[Epoch 4143/5000] loss: 0.67586\n",
      "[Epoch 4144/5000] loss: 0.98523\n",
      "[Epoch 4145/5000] loss: 0.79933\n",
      "[Epoch 4146/5000] loss: 0.68267\n",
      "[Epoch 4147/5000] loss: 0.90641\n",
      "[Epoch 4148/5000] loss: 0.94228\n",
      "[Epoch 4149/5000] loss: 1.07186\n",
      "[Epoch 4150/5000] loss: 1.21380\n",
      "[Epoch 4151/5000] loss: 1.05579\n",
      "[Epoch 4152/5000] loss: 3.59720\n",
      "[Epoch 4153/5000] loss: 0.82187\n",
      "[Epoch 4154/5000] loss: 0.72713\n",
      "[Epoch 4155/5000] loss: 0.90213\n",
      "[Epoch 4156/5000] loss: 0.81180\n",
      "[Epoch 4157/5000] loss: 1.40708\n",
      "[Epoch 4158/5000] loss: 0.45276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4159/5000] loss: 0.74699\n",
      "[Epoch 4160/5000] loss: 0.65721\n",
      "[Epoch 4161/5000] loss: 3.45643\n",
      "[Epoch 4162/5000] loss: 0.45051\n",
      "[Epoch 4163/5000] loss: 1.08545\n",
      "[Epoch 4164/5000] loss: 0.96037\n",
      "[Epoch 4165/5000] loss: 1.18044\n",
      "[Epoch 4166/5000] loss: 0.79642\n",
      "[Epoch 4167/5000] loss: 1.23567\n",
      "[Epoch 4168/5000] loss: 0.82900\n",
      "[Epoch 4169/5000] loss: 0.90904\n",
      "[Epoch 4170/5000] loss: 1.17654\n",
      "[Epoch 4171/5000] loss: 1.47886\n",
      "[Epoch 4172/5000] loss: 0.64729\n",
      "[Epoch 4173/5000] loss: 0.53370\n",
      "[Epoch 4174/5000] loss: 0.43236\n",
      "[Epoch 4175/5000] loss: 0.73135\n",
      "[Epoch 4176/5000] loss: 0.73786\n",
      "[Epoch 4177/5000] loss: 0.72576\n",
      "[Epoch 4178/5000] loss: 0.65929\n",
      "[Epoch 4179/5000] loss: 0.73966\n",
      "[Epoch 4180/5000] loss: 0.58667\n",
      "[Epoch 4181/5000] loss: 1.41932\n",
      "[Epoch 4182/5000] loss: 0.45890\n",
      "[Epoch 4183/5000] loss: 0.80284\n",
      "[Epoch 4184/5000] loss: 1.02147\n",
      "[Epoch 4185/5000] loss: 1.24904\n",
      "[Epoch 4186/5000] loss: 1.00911\n",
      "[Epoch 4187/5000] loss: 0.47376\n",
      "[Epoch 4188/5000] loss: 1.32308\n",
      "[Epoch 4189/5000] loss: 1.40852\n",
      "[Epoch 4190/5000] loss: 0.42211\n",
      "[Epoch 4191/5000] loss: 1.35974\n",
      "[Epoch 4192/5000] loss: 0.77864\n",
      "[Epoch 4193/5000] loss: 0.90949\n",
      "[Epoch 4194/5000] loss: 1.40700\n",
      "[Epoch 4195/5000] loss: 0.69966\n",
      "[Epoch 4196/5000] loss: 1.42806\n",
      "[Epoch 4197/5000] loss: 0.58149\n",
      "[Epoch 4198/5000] loss: 0.48671\n",
      "[Epoch 4199/5000] loss: 0.70549\n",
      "[Epoch 4200/5000] loss: 0.92339\n",
      "[Epoch 4201/5000] loss: 1.03457\n",
      "[Epoch 4202/5000] loss: 1.14060\n",
      "[Epoch 4203/5000] loss: 0.53905\n",
      "[Epoch 4204/5000] loss: 1.13902\n",
      "[Epoch 4205/5000] loss: 0.92663\n",
      "[Epoch 4206/5000] loss: 1.16492\n",
      "[Epoch 4207/5000] loss: 1.03441\n",
      "[Epoch 4208/5000] loss: 0.97441\n",
      "[Epoch 4209/5000] loss: 1.17386\n",
      "[Epoch 4210/5000] loss: 0.88956\n",
      "[Epoch 4211/5000] loss: 1.39535\n",
      "[Epoch 4212/5000] loss: 1.09987\n",
      "[Epoch 4213/5000] loss: 0.90731\n",
      "[Epoch 4214/5000] loss: 0.69936\n",
      "[Epoch 4215/5000] loss: 1.25355\n",
      "[Epoch 4216/5000] loss: 1.39334\n",
      "[Epoch 4217/5000] loss: 0.92978\n",
      "[Epoch 4218/5000] loss: 1.05004\n",
      "[Epoch 4219/5000] loss: 0.73898\n",
      "[Epoch 4220/5000] loss: 0.80296\n",
      "[Epoch 4221/5000] loss: 1.35674\n",
      "[Epoch 4222/5000] loss: 0.74777\n",
      "[Epoch 4223/5000] loss: 0.65430\n",
      "[Epoch 4224/5000] loss: 1.07567\n",
      "[Epoch 4225/5000] loss: 1.09807\n",
      "[Epoch 4226/5000] loss: 0.76291\n",
      "[Epoch 4227/5000] loss: 0.50543\n",
      "[Epoch 4228/5000] loss: 0.81585\n",
      "[Epoch 4229/5000] loss: 1.34623\n",
      "[Epoch 4230/5000] loss: 1.38770\n",
      "[Epoch 4231/5000] loss: 0.68167\n",
      "[Epoch 4232/5000] loss: 1.02212\n",
      "[Epoch 4233/5000] loss: 0.86169\n",
      "[Epoch 4234/5000] loss: 0.95243\n",
      "[Epoch 4235/5000] loss: 0.63998\n",
      "[Epoch 4236/5000] loss: 0.76025\n",
      "[Epoch 4237/5000] loss: 1.10334\n",
      "[Epoch 4238/5000] loss: 0.76862\n",
      "[Epoch 4239/5000] loss: 1.00216\n",
      "[Epoch 4240/5000] loss: 0.66803\n",
      "[Epoch 4241/5000] loss: 1.22311\n",
      "[Epoch 4242/5000] loss: 0.91385\n",
      "[Epoch 4243/5000] loss: 0.88273\n",
      "[Epoch 4244/5000] loss: 1.70233\n",
      "[Epoch 4245/5000] loss: 0.71861\n",
      "[Epoch 4246/5000] loss: 1.44116\n",
      "[Epoch 4247/5000] loss: 1.20183\n",
      "[Epoch 4248/5000] loss: 0.76642\n",
      "[Epoch 4249/5000] loss: 1.03070\n",
      "[Epoch 4250/5000] loss: 1.41366\n",
      "[Epoch 4251/5000] loss: 0.85437\n",
      "[Epoch 4252/5000] loss: 1.47767\n",
      "[Epoch 4253/5000] loss: 0.67498\n",
      "[Epoch 4254/5000] loss: 1.00435\n",
      "[Epoch 4255/5000] loss: 0.60620\n",
      "[Epoch 4256/5000] loss: 0.89332\n",
      "[Epoch 4257/5000] loss: 0.53073\n",
      "[Epoch 4258/5000] loss: 0.75687\n",
      "[Epoch 4259/5000] loss: 0.89941\n",
      "[Epoch 4260/5000] loss: 0.87319\n",
      "[Epoch 4261/5000] loss: 0.76092\n",
      "[Epoch 4262/5000] loss: 1.05879\n",
      "[Epoch 4263/5000] loss: 0.94174\n",
      "[Epoch 4264/5000] loss: 0.57979\n",
      "[Epoch 4265/5000] loss: 1.01717\n",
      "[Epoch 4266/5000] loss: 0.97853\n",
      "[Epoch 4267/5000] loss: 0.89192\n",
      "[Epoch 4268/5000] loss: 0.66532\n",
      "[Epoch 4269/5000] loss: 0.74456\n",
      "[Epoch 4270/5000] loss: 0.71352\n",
      "[Epoch 4271/5000] loss: 1.29093\n",
      "[Epoch 4272/5000] loss: 0.72488\n",
      "[Epoch 4273/5000] loss: 0.78765\n",
      "[Epoch 4274/5000] loss: 0.87722\n",
      "[Epoch 4275/5000] loss: 1.20851\n",
      "[Epoch 4276/5000] loss: 0.34525\n",
      "[Epoch 4277/5000] loss: 1.09404\n",
      "[Epoch 4278/5000] loss: 1.19331\n",
      "[Epoch 4279/5000] loss: 1.25128\n",
      "[Epoch 4280/5000] loss: 1.14073\n",
      "[Epoch 4281/5000] loss: 0.88400\n",
      "[Epoch 4282/5000] loss: 1.50651\n",
      "[Epoch 4283/5000] loss: 1.16899\n",
      "[Epoch 4284/5000] loss: 1.49154\n",
      "[Epoch 4285/5000] loss: 0.77527\n",
      "[Epoch 4286/5000] loss: 0.64505\n",
      "[Epoch 4287/5000] loss: 0.51375\n",
      "[Epoch 4288/5000] loss: 0.72862\n",
      "[Epoch 4289/5000] loss: 1.07776\n",
      "[Epoch 4290/5000] loss: 0.81260\n",
      "[Epoch 4291/5000] loss: 1.06663\n",
      "[Epoch 4292/5000] loss: 1.06987\n",
      "[Epoch 4293/5000] loss: 0.82939\n",
      "[Epoch 4294/5000] loss: 0.58423\n",
      "[Epoch 4295/5000] loss: 1.29977\n",
      "[Epoch 4296/5000] loss: 0.88113\n",
      "[Epoch 4297/5000] loss: 1.27976\n",
      "[Epoch 4298/5000] loss: 1.19695\n",
      "[Epoch 4299/5000] loss: 1.01918\n",
      "[Epoch 4300/5000] loss: 3.43027\n",
      "[Epoch 4301/5000] loss: 0.56267\n",
      "[Epoch 4302/5000] loss: 0.81332\n",
      "[Epoch 4303/5000] loss: 1.14735\n",
      "[Epoch 4304/5000] loss: 0.94972\n",
      "[Epoch 4305/5000] loss: 1.06939\n",
      "[Epoch 4306/5000] loss: 0.77994\n",
      "[Epoch 4307/5000] loss: 4.13630\n",
      "[Epoch 4308/5000] loss: 0.64339\n",
      "[Epoch 4309/5000] loss: 1.04395\n",
      "[Epoch 4310/5000] loss: 1.18363\n",
      "[Epoch 4311/5000] loss: 1.14720\n",
      "[Epoch 4312/5000] loss: 1.11480\n",
      "[Epoch 4313/5000] loss: 0.83036\n",
      "[Epoch 4314/5000] loss: 1.34841\n",
      "[Epoch 4315/5000] loss: 0.50158\n",
      "[Epoch 4316/5000] loss: 0.86225\n",
      "[Epoch 4317/5000] loss: 1.23636\n",
      "[Epoch 4318/5000] loss: 0.76515\n",
      "[Epoch 4319/5000] loss: 1.23339\n",
      "[Epoch 4320/5000] loss: 0.82486\n",
      "[Epoch 4321/5000] loss: 0.60731\n",
      "[Epoch 4322/5000] loss: 0.97753\n",
      "[Epoch 4323/5000] loss: 0.99462\n",
      "[Epoch 4324/5000] loss: 0.93066\n",
      "[Epoch 4325/5000] loss: 1.15573\n",
      "[Epoch 4326/5000] loss: 0.82280\n",
      "[Epoch 4327/5000] loss: 1.08628\n",
      "[Epoch 4328/5000] loss: 0.78886\n",
      "[Epoch 4329/5000] loss: 1.00648\n",
      "[Epoch 4330/5000] loss: 0.82955\n",
      "[Epoch 4331/5000] loss: 1.41270\n",
      "[Epoch 4332/5000] loss: 0.99785\n",
      "[Epoch 4333/5000] loss: 0.78861\n",
      "[Epoch 4334/5000] loss: 0.77724\n",
      "[Epoch 4335/5000] loss: 1.53619\n",
      "[Epoch 4336/5000] loss: 0.51170\n",
      "[Epoch 4337/5000] loss: 0.79968\n",
      "[Epoch 4338/5000] loss: 0.62891\n",
      "[Epoch 4339/5000] loss: 0.81586\n",
      "[Epoch 4340/5000] loss: 1.32503\n",
      "[Epoch 4341/5000] loss: 0.79517\n",
      "[Epoch 4342/5000] loss: 0.74707\n",
      "[Epoch 4343/5000] loss: 3.40406\n",
      "[Epoch 4344/5000] loss: 0.80141\n",
      "[Epoch 4345/5000] loss: 1.04490\n",
      "[Epoch 4346/5000] loss: 1.17861\n",
      "[Epoch 4347/5000] loss: 0.70106\n",
      "[Epoch 4348/5000] loss: 1.37884\n",
      "[Epoch 4349/5000] loss: 1.23903\n",
      "[Epoch 4350/5000] loss: 0.69737\n",
      "[Epoch 4351/5000] loss: 0.73986\n",
      "[Epoch 4352/5000] loss: 0.68750\n",
      "[Epoch 4353/5000] loss: 1.65045\n",
      "[Epoch 4354/5000] loss: 3.19799\n",
      "[Epoch 4355/5000] loss: 3.32497\n",
      "[Epoch 4356/5000] loss: 0.61467\n",
      "[Epoch 4357/5000] loss: 1.08699\n",
      "[Epoch 4358/5000] loss: 0.79071\n",
      "[Epoch 4359/5000] loss: 1.40531\n",
      "[Epoch 4360/5000] loss: 0.85107\n",
      "[Epoch 4361/5000] loss: 0.69326\n",
      "[Epoch 4362/5000] loss: 1.01233\n",
      "[Epoch 4363/5000] loss: 1.75270\n",
      "[Epoch 4364/5000] loss: 0.64610\n",
      "[Epoch 4365/5000] loss: 0.83008\n",
      "[Epoch 4366/5000] loss: 3.08553\n",
      "[Epoch 4367/5000] loss: 0.33169\n",
      "[Epoch 4368/5000] loss: 1.07616\n",
      "[Epoch 4369/5000] loss: 1.64202\n",
      "[Epoch 4370/5000] loss: 0.77750\n",
      "[Epoch 4371/5000] loss: 0.82794\n",
      "[Epoch 4372/5000] loss: 1.14618\n",
      "[Epoch 4373/5000] loss: 0.56538\n",
      "[Epoch 4374/5000] loss: 0.95555\n",
      "[Epoch 4375/5000] loss: 0.92855\n",
      "[Epoch 4376/5000] loss: 1.12699\n",
      "[Epoch 4377/5000] loss: 1.20026\n",
      "[Epoch 4378/5000] loss: 0.75795\n",
      "[Epoch 4379/5000] loss: 0.77993\n",
      "[Epoch 4380/5000] loss: 0.82945\n",
      "[Epoch 4381/5000] loss: 0.98179\n",
      "[Epoch 4382/5000] loss: 0.82851\n",
      "[Epoch 4383/5000] loss: 0.78090\n",
      "[Epoch 4384/5000] loss: 1.20519\n",
      "[Epoch 4385/5000] loss: 0.86072\n",
      "[Epoch 4386/5000] loss: 1.19171\n",
      "[Epoch 4387/5000] loss: 1.06644\n",
      "[Epoch 4388/5000] loss: 0.66373\n",
      "[Epoch 4389/5000] loss: 1.43540\n",
      "[Epoch 4390/5000] loss: 0.66220\n",
      "[Epoch 4391/5000] loss: 1.00367\n",
      "[Epoch 4392/5000] loss: 0.73937\n",
      "[Epoch 4393/5000] loss: 1.28054\n",
      "[Epoch 4394/5000] loss: 1.02485\n",
      "[Epoch 4395/5000] loss: 1.14689\n",
      "[Epoch 4396/5000] loss: 1.06041\n",
      "[Epoch 4397/5000] loss: 1.21044\n",
      "[Epoch 4398/5000] loss: 0.24524\n",
      "[Epoch 4399/5000] loss: 0.88008\n",
      "[Epoch 4400/5000] loss: 1.05311\n",
      "[Epoch 4401/5000] loss: 0.72074\n",
      "[Epoch 4402/5000] loss: 0.61352\n",
      "[Epoch 4403/5000] loss: 0.85275\n",
      "[Epoch 4404/5000] loss: 0.79668\n",
      "[Epoch 4405/5000] loss: 1.05687\n",
      "[Epoch 4406/5000] loss: 0.61060\n",
      "[Epoch 4407/5000] loss: 1.09717\n",
      "[Epoch 4408/5000] loss: 0.79776\n",
      "[Epoch 4409/5000] loss: 1.38716\n",
      "[Epoch 4410/5000] loss: 0.74609\n",
      "[Epoch 4411/5000] loss: 0.80336\n",
      "[Epoch 4412/5000] loss: 1.17452\n",
      "[Epoch 4413/5000] loss: 0.62156\n",
      "[Epoch 4414/5000] loss: 1.28935\n",
      "[Epoch 4415/5000] loss: 0.72143\n",
      "[Epoch 4416/5000] loss: 0.55016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4417/5000] loss: 0.49340\n",
      "[Epoch 4418/5000] loss: 0.95296\n",
      "[Epoch 4419/5000] loss: 1.25440\n",
      "[Epoch 4420/5000] loss: 1.36049\n",
      "[Epoch 4421/5000] loss: 1.27763\n",
      "[Epoch 4422/5000] loss: 0.93530\n",
      "[Epoch 4423/5000] loss: 1.13714\n",
      "[Epoch 4424/5000] loss: 0.94383\n",
      "[Epoch 4425/5000] loss: 1.06662\n",
      "[Epoch 4426/5000] loss: 1.55748\n",
      "[Epoch 4427/5000] loss: 0.97254\n",
      "[Epoch 4428/5000] loss: 0.87671\n",
      "[Epoch 4429/5000] loss: 1.13239\n",
      "[Epoch 4430/5000] loss: 0.55489\n",
      "[Epoch 4431/5000] loss: 1.27720\n",
      "[Epoch 4432/5000] loss: 1.40845\n",
      "[Epoch 4433/5000] loss: 0.73926\n",
      "[Epoch 4434/5000] loss: 1.01159\n",
      "[Epoch 4435/5000] loss: 0.75406\n",
      "[Epoch 4436/5000] loss: 0.81526\n",
      "[Epoch 4437/5000] loss: 0.77265\n",
      "[Epoch 4438/5000] loss: 0.77138\n",
      "[Epoch 4439/5000] loss: 0.85887\n",
      "[Epoch 4440/5000] loss: 3.26766\n",
      "[Epoch 4441/5000] loss: 1.12701\n",
      "[Epoch 4442/5000] loss: 0.97744\n",
      "[Epoch 4443/5000] loss: 1.03185\n",
      "[Epoch 4444/5000] loss: 1.09811\n",
      "[Epoch 4445/5000] loss: 0.74570\n",
      "[Epoch 4446/5000] loss: 0.83461\n",
      "[Epoch 4447/5000] loss: 0.96102\n",
      "[Epoch 4448/5000] loss: 1.35752\n",
      "[Epoch 4449/5000] loss: 0.90379\n",
      "[Epoch 4450/5000] loss: 0.81893\n",
      "[Epoch 4451/5000] loss: 1.35269\n",
      "[Epoch 4452/5000] loss: 0.47493\n",
      "[Epoch 4453/5000] loss: 1.18823\n",
      "[Epoch 4454/5000] loss: 1.16538\n",
      "[Epoch 4455/5000] loss: 0.66943\n",
      "[Epoch 4456/5000] loss: 0.74426\n",
      "[Epoch 4457/5000] loss: 1.01026\n",
      "[Epoch 4458/5000] loss: 1.19890\n",
      "[Epoch 4459/5000] loss: 0.77319\n",
      "[Epoch 4460/5000] loss: 0.63989\n",
      "[Epoch 4461/5000] loss: 0.94344\n",
      "[Epoch 4462/5000] loss: 0.77710\n",
      "[Epoch 4463/5000] loss: 1.11757\n",
      "[Epoch 4464/5000] loss: 1.07056\n",
      "[Epoch 4465/5000] loss: 0.59483\n",
      "[Epoch 4466/5000] loss: 3.60803\n",
      "[Epoch 4467/5000] loss: 1.08652\n",
      "[Epoch 4468/5000] loss: 0.87619\n",
      "[Epoch 4469/5000] loss: 0.66723\n",
      "[Epoch 4470/5000] loss: 0.88737\n",
      "[Epoch 4471/5000] loss: 0.65508\n",
      "[Epoch 4472/5000] loss: 1.13579\n",
      "[Epoch 4473/5000] loss: 0.81678\n",
      "[Epoch 4474/5000] loss: 0.99369\n",
      "[Epoch 4475/5000] loss: 0.81809\n",
      "[Epoch 4476/5000] loss: 0.80087\n",
      "[Epoch 4477/5000] loss: 1.02277\n",
      "[Epoch 4478/5000] loss: 0.73626\n",
      "[Epoch 4479/5000] loss: 1.00487\n",
      "[Epoch 4480/5000] loss: 0.78108\n",
      "[Epoch 4481/5000] loss: 0.78242\n",
      "[Epoch 4482/5000] loss: 1.54759\n",
      "[Epoch 4483/5000] loss: 1.14023\n",
      "[Epoch 4484/5000] loss: 1.32269\n",
      "[Epoch 4485/5000] loss: 1.31346\n",
      "[Epoch 4486/5000] loss: 0.93811\n",
      "[Epoch 4487/5000] loss: 0.80903\n",
      "[Epoch 4488/5000] loss: 0.94193\n",
      "[Epoch 4489/5000] loss: 1.39308\n",
      "[Epoch 4490/5000] loss: 0.65508\n",
      "[Epoch 4491/5000] loss: 1.19091\n",
      "[Epoch 4492/5000] loss: 0.94108\n",
      "[Epoch 4493/5000] loss: 1.14108\n",
      "[Epoch 4494/5000] loss: 1.11059\n",
      "[Epoch 4495/5000] loss: 1.23273\n",
      "[Epoch 4496/5000] loss: 0.74490\n",
      "[Epoch 4497/5000] loss: 0.78703\n",
      "[Epoch 4498/5000] loss: 1.15487\n",
      "[Epoch 4499/5000] loss: 0.92896\n",
      "[Epoch 4500/5000] loss: 0.82543\n",
      "[Epoch 4501/5000] loss: 0.66544\n",
      "[Epoch 4502/5000] loss: 0.72790\n",
      "[Epoch 4503/5000] loss: 1.22494\n",
      "[Epoch 4504/5000] loss: 1.44036\n",
      "[Epoch 4505/5000] loss: 0.66201\n",
      "[Epoch 4506/5000] loss: 0.83034\n",
      "[Epoch 4507/5000] loss: 1.10991\n",
      "[Epoch 4508/5000] loss: 1.07050\n",
      "[Epoch 4509/5000] loss: 1.33186\n",
      "[Epoch 4510/5000] loss: 1.69947\n",
      "[Epoch 4511/5000] loss: 0.95791\n",
      "[Epoch 4512/5000] loss: 0.59644\n",
      "[Epoch 4513/5000] loss: 0.88337\n",
      "[Epoch 4514/5000] loss: 0.68304\n",
      "[Epoch 4515/5000] loss: 0.54036\n",
      "[Epoch 4516/5000] loss: 1.22868\n",
      "[Epoch 4517/5000] loss: 0.91225\n",
      "[Epoch 4518/5000] loss: 0.76243\n",
      "[Epoch 4519/5000] loss: 0.68769\n",
      "[Epoch 4520/5000] loss: 0.43329\n",
      "[Epoch 4521/5000] loss: 1.11955\n",
      "[Epoch 4522/5000] loss: 1.04806\n",
      "[Epoch 4523/5000] loss: 1.21920\n",
      "[Epoch 4524/5000] loss: 1.10385\n",
      "[Epoch 4525/5000] loss: 0.68052\n",
      "[Epoch 4526/5000] loss: 1.00112\n",
      "[Epoch 4527/5000] loss: 1.12844\n",
      "[Epoch 4528/5000] loss: 1.01699\n",
      "[Epoch 4529/5000] loss: 1.04009\n",
      "[Epoch 4530/5000] loss: 1.23117\n",
      "[Epoch 4531/5000] loss: 0.62108\n",
      "[Epoch 4532/5000] loss: 1.02395\n",
      "[Epoch 4533/5000] loss: 0.72588\n",
      "[Epoch 4534/5000] loss: 0.77597\n",
      "[Epoch 4535/5000] loss: 1.10275\n",
      "[Epoch 4536/5000] loss: 1.54709\n",
      "[Epoch 4537/5000] loss: 0.93118\n",
      "[Epoch 4538/5000] loss: 0.68659\n",
      "[Epoch 4539/5000] loss: 0.99059\n",
      "[Epoch 4540/5000] loss: 1.12898\n",
      "[Epoch 4541/5000] loss: 1.15681\n",
      "[Epoch 4542/5000] loss: 0.57085\n",
      "[Epoch 4543/5000] loss: 1.09287\n",
      "[Epoch 4544/5000] loss: 1.15550\n",
      "[Epoch 4545/5000] loss: 1.13746\n",
      "[Epoch 4546/5000] loss: 0.72049\n",
      "[Epoch 4547/5000] loss: 3.65123\n",
      "[Epoch 4548/5000] loss: 1.04630\n",
      "[Epoch 4549/5000] loss: 0.85140\n",
      "[Epoch 4550/5000] loss: 0.70976\n",
      "[Epoch 4551/5000] loss: 0.54651\n",
      "[Epoch 4552/5000] loss: 0.95305\n",
      "[Epoch 4553/5000] loss: 0.88142\n",
      "[Epoch 4554/5000] loss: 0.90145\n",
      "[Epoch 4555/5000] loss: 1.38508\n",
      "[Epoch 4556/5000] loss: 1.03528\n",
      "[Epoch 4557/5000] loss: 0.88596\n",
      "[Epoch 4558/5000] loss: 0.72334\n",
      "[Epoch 4559/5000] loss: 0.63617\n",
      "[Epoch 4560/5000] loss: 1.04146\n",
      "[Epoch 4561/5000] loss: 1.26498\n",
      "[Epoch 4562/5000] loss: 0.73689\n",
      "[Epoch 4563/5000] loss: 0.70127\n",
      "[Epoch 4564/5000] loss: 0.72777\n",
      "[Epoch 4565/5000] loss: 1.36276\n",
      "[Epoch 4566/5000] loss: 0.84109\n",
      "[Epoch 4567/5000] loss: 1.31982\n",
      "[Epoch 4568/5000] loss: 0.88629\n",
      "[Epoch 4569/5000] loss: 0.63159\n",
      "[Epoch 4570/5000] loss: 0.80948\n",
      "[Epoch 4571/5000] loss: 1.24952\n",
      "[Epoch 4572/5000] loss: 1.60740\n",
      "[Epoch 4573/5000] loss: 1.37047\n",
      "[Epoch 4574/5000] loss: 1.27190\n",
      "[Epoch 4575/5000] loss: 0.86760\n",
      "[Epoch 4576/5000] loss: 0.66562\n",
      "[Epoch 4577/5000] loss: 1.25654\n",
      "[Epoch 4578/5000] loss: 1.14356\n",
      "[Epoch 4579/5000] loss: 1.28408\n",
      "[Epoch 4580/5000] loss: 0.71821\n",
      "[Epoch 4581/5000] loss: 0.84681\n",
      "[Epoch 4582/5000] loss: 1.26694\n",
      "[Epoch 4583/5000] loss: 0.81043\n",
      "[Epoch 4584/5000] loss: 0.49082\n",
      "[Epoch 4585/5000] loss: 0.65200\n",
      "[Epoch 4586/5000] loss: 0.62609\n",
      "[Epoch 4587/5000] loss: 1.06806\n",
      "[Epoch 4588/5000] loss: 1.10333\n",
      "[Epoch 4589/5000] loss: 0.88767\n",
      "[Epoch 4590/5000] loss: 1.31766\n",
      "[Epoch 4591/5000] loss: 1.41332\n",
      "[Epoch 4592/5000] loss: 0.81821\n",
      "[Epoch 4593/5000] loss: 0.78016\n",
      "[Epoch 4594/5000] loss: 0.79136\n",
      "[Epoch 4595/5000] loss: 0.87908\n",
      "[Epoch 4596/5000] loss: 0.68849\n",
      "[Epoch 4597/5000] loss: 3.21741\n",
      "[Epoch 4598/5000] loss: 0.51396\n",
      "[Epoch 4599/5000] loss: 1.19412\n",
      "[Epoch 4600/5000] loss: 0.90478\n",
      "[Epoch 4601/5000] loss: 1.06551\n",
      "[Epoch 4602/5000] loss: 0.68741\n",
      "[Epoch 4603/5000] loss: 0.61859\n",
      "[Epoch 4604/5000] loss: 1.24910\n",
      "[Epoch 4605/5000] loss: 0.86053\n",
      "[Epoch 4606/5000] loss: 0.79941\n",
      "[Epoch 4607/5000] loss: 0.45113\n",
      "[Epoch 4608/5000] loss: 1.05750\n",
      "[Epoch 4609/5000] loss: 0.68060\n",
      "[Epoch 4610/5000] loss: 1.25316\n",
      "[Epoch 4611/5000] loss: 1.14997\n",
      "[Epoch 4612/5000] loss: 1.07491\n",
      "[Epoch 4613/5000] loss: 0.81896\n",
      "[Epoch 4614/5000] loss: 0.91305\n",
      "[Epoch 4615/5000] loss: 0.66316\n",
      "[Epoch 4616/5000] loss: 0.52641\n",
      "[Epoch 4617/5000] loss: 2.12688\n",
      "[Epoch 4618/5000] loss: 0.73075\n",
      "[Epoch 4619/5000] loss: 0.68869\n",
      "[Epoch 4620/5000] loss: 0.45501\n",
      "[Epoch 4621/5000] loss: 0.91001\n",
      "[Epoch 4622/5000] loss: 1.04227\n",
      "[Epoch 4623/5000] loss: 1.05154\n",
      "[Epoch 4624/5000] loss: 0.60542\n",
      "[Epoch 4625/5000] loss: 1.11683\n",
      "[Epoch 4626/5000] loss: 0.82830\n",
      "[Epoch 4627/5000] loss: 1.21869\n",
      "[Epoch 4628/5000] loss: 1.06527\n",
      "[Epoch 4629/5000] loss: 0.92330\n",
      "[Epoch 4630/5000] loss: 1.04398\n",
      "[Epoch 4631/5000] loss: 0.94164\n",
      "[Epoch 4632/5000] loss: 0.75579\n",
      "[Epoch 4633/5000] loss: 0.58558\n",
      "[Epoch 4634/5000] loss: 0.57983\n",
      "[Epoch 4635/5000] loss: 1.52769\n",
      "[Epoch 4636/5000] loss: 0.86102\n",
      "[Epoch 4637/5000] loss: 0.81560\n",
      "[Epoch 4638/5000] loss: 0.87847\n",
      "[Epoch 4639/5000] loss: 0.54971\n",
      "[Epoch 4640/5000] loss: 1.56460\n",
      "[Epoch 4641/5000] loss: 0.84538\n",
      "[Epoch 4642/5000] loss: 1.04304\n",
      "[Epoch 4643/5000] loss: 1.28642\n",
      "[Epoch 4644/5000] loss: 1.13233\n",
      "[Epoch 4645/5000] loss: 0.98237\n",
      "[Epoch 4646/5000] loss: 0.75460\n",
      "[Epoch 4647/5000] loss: 0.99047\n",
      "[Epoch 4648/5000] loss: 0.76745\n",
      "[Epoch 4649/5000] loss: 1.03393\n",
      "[Epoch 4650/5000] loss: 0.72548\n",
      "[Epoch 4651/5000] loss: 0.55863\n",
      "[Epoch 4652/5000] loss: 1.54211\n",
      "[Epoch 4653/5000] loss: 0.55077\n",
      "[Epoch 4654/5000] loss: 0.57588\n",
      "[Epoch 4655/5000] loss: 3.15326\n",
      "[Epoch 4656/5000] loss: 1.06622\n",
      "[Epoch 4657/5000] loss: 0.65514\n",
      "[Epoch 4658/5000] loss: 0.98086\n",
      "[Epoch 4659/5000] loss: 0.35078\n",
      "[Epoch 4660/5000] loss: 0.82963\n",
      "[Epoch 4661/5000] loss: 0.57306\n",
      "[Epoch 4662/5000] loss: 1.17941\n",
      "[Epoch 4663/5000] loss: 1.19819\n",
      "[Epoch 4664/5000] loss: 0.95156\n",
      "[Epoch 4665/5000] loss: 0.64909\n",
      "[Epoch 4666/5000] loss: 0.79084\n",
      "[Epoch 4667/5000] loss: 0.98107\n",
      "[Epoch 4668/5000] loss: 0.68266\n",
      "[Epoch 4669/5000] loss: 1.18131\n",
      "[Epoch 4670/5000] loss: 1.35581\n",
      "[Epoch 4671/5000] loss: 0.71145\n",
      "[Epoch 4672/5000] loss: 0.54643\n",
      "[Epoch 4673/5000] loss: 0.91882\n",
      "[Epoch 4674/5000] loss: 0.78154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4675/5000] loss: 0.96762\n",
      "[Epoch 4676/5000] loss: 1.25549\n",
      "[Epoch 4677/5000] loss: 1.13957\n",
      "[Epoch 4678/5000] loss: 0.67721\n",
      "[Epoch 4679/5000] loss: 3.10704\n",
      "[Epoch 4680/5000] loss: 0.29313\n",
      "[Epoch 4681/5000] loss: 0.79442\n",
      "[Epoch 4682/5000] loss: 0.97837\n",
      "[Epoch 4683/5000] loss: 0.97429\n",
      "[Epoch 4684/5000] loss: 0.47552\n",
      "[Epoch 4685/5000] loss: 0.78089\n",
      "[Epoch 4686/5000] loss: 0.93454\n",
      "[Epoch 4687/5000] loss: 0.78754\n",
      "[Epoch 4688/5000] loss: 1.15591\n",
      "[Epoch 4689/5000] loss: 0.83522\n",
      "[Epoch 4690/5000] loss: 0.83009\n",
      "[Epoch 4691/5000] loss: 0.99206\n",
      "[Epoch 4692/5000] loss: 1.02235\n",
      "[Epoch 4693/5000] loss: 1.18580\n",
      "[Epoch 4694/5000] loss: 0.98469\n",
      "[Epoch 4695/5000] loss: 0.92828\n",
      "[Epoch 4696/5000] loss: 0.86522\n",
      "[Epoch 4697/5000] loss: 0.56579\n",
      "[Epoch 4698/5000] loss: 0.67395\n",
      "[Epoch 4699/5000] loss: 0.71266\n",
      "[Epoch 4700/5000] loss: 1.26301\n",
      "[Epoch 4701/5000] loss: 0.65024\n",
      "[Epoch 4702/5000] loss: 0.92397\n",
      "[Epoch 4703/5000] loss: 1.19314\n",
      "[Epoch 4704/5000] loss: 1.27382\n",
      "[Epoch 4705/5000] loss: 0.45541\n",
      "[Epoch 4706/5000] loss: 0.86548\n",
      "[Epoch 4707/5000] loss: 0.86627\n",
      "[Epoch 4708/5000] loss: 0.37797\n",
      "[Epoch 4709/5000] loss: 0.86085\n",
      "[Epoch 4710/5000] loss: 0.72061\n",
      "[Epoch 4711/5000] loss: 1.05676\n",
      "[Epoch 4712/5000] loss: 0.65068\n",
      "[Epoch 4713/5000] loss: 0.88476\n",
      "[Epoch 4714/5000] loss: 0.62641\n",
      "[Epoch 4715/5000] loss: 0.67621\n",
      "[Epoch 4716/5000] loss: 1.01566\n",
      "[Epoch 4717/5000] loss: 0.60770\n",
      "[Epoch 4718/5000] loss: 0.97966\n",
      "[Epoch 4719/5000] loss: 1.07571\n",
      "[Epoch 4720/5000] loss: 0.64263\n",
      "[Epoch 4721/5000] loss: 1.13164\n",
      "[Epoch 4722/5000] loss: 0.73498\n",
      "[Epoch 4723/5000] loss: 1.31187\n",
      "[Epoch 4724/5000] loss: 0.85481\n",
      "[Epoch 4725/5000] loss: 0.81272\n",
      "[Epoch 4726/5000] loss: 0.88599\n",
      "[Epoch 4727/5000] loss: 1.39849\n",
      "[Epoch 4728/5000] loss: 1.81967\n",
      "[Epoch 4729/5000] loss: 1.43382\n",
      "[Epoch 4730/5000] loss: 0.75686\n",
      "[Epoch 4731/5000] loss: 1.26688\n",
      "[Epoch 4732/5000] loss: 1.11068\n",
      "[Epoch 4733/5000] loss: 0.66346\n",
      "[Epoch 4734/5000] loss: 1.29713\n",
      "[Epoch 4735/5000] loss: 0.78908\n",
      "[Epoch 4736/5000] loss: 0.76684\n",
      "[Epoch 4737/5000] loss: 1.73753\n",
      "[Epoch 4738/5000] loss: 1.54664\n",
      "[Epoch 4739/5000] loss: 0.85585\n",
      "[Epoch 4740/5000] loss: 1.13530\n",
      "[Epoch 4741/5000] loss: 0.92063\n",
      "[Epoch 4742/5000] loss: 1.67160\n",
      "[Epoch 4743/5000] loss: 0.72301\n",
      "[Epoch 4744/5000] loss: 0.89788\n",
      "[Epoch 4745/5000] loss: 3.18979\n",
      "[Epoch 4746/5000] loss: 0.54517\n",
      "[Epoch 4747/5000] loss: 0.66156\n",
      "[Epoch 4748/5000] loss: 1.08110\n",
      "[Epoch 4749/5000] loss: 1.15440\n",
      "[Epoch 4750/5000] loss: 0.95587\n",
      "[Epoch 4751/5000] loss: 0.66118\n",
      "[Epoch 4752/5000] loss: 0.53423\n",
      "[Epoch 4753/5000] loss: 0.64666\n",
      "[Epoch 4754/5000] loss: 0.37609\n",
      "[Epoch 4755/5000] loss: 0.99870\n",
      "[Epoch 4756/5000] loss: 0.96898\n",
      "[Epoch 4757/5000] loss: 0.97253\n",
      "[Epoch 4758/5000] loss: 0.91752\n",
      "[Epoch 4759/5000] loss: 1.14339\n",
      "[Epoch 4760/5000] loss: 0.54018\n",
      "[Epoch 4761/5000] loss: 0.58225\n",
      "[Epoch 4762/5000] loss: 0.98910\n",
      "[Epoch 4763/5000] loss: 0.82110\n",
      "[Epoch 4764/5000] loss: 3.28173\n",
      "[Epoch 4765/5000] loss: 1.40103\n",
      "[Epoch 4766/5000] loss: 1.58264\n",
      "[Epoch 4767/5000] loss: 0.73883\n",
      "[Epoch 4768/5000] loss: 0.53689\n",
      "[Epoch 4769/5000] loss: 1.08404\n",
      "[Epoch 4770/5000] loss: 1.09016\n",
      "[Epoch 4771/5000] loss: 1.17973\n",
      "[Epoch 4772/5000] loss: 1.04026\n",
      "[Epoch 4773/5000] loss: 0.99080\n",
      "[Epoch 4774/5000] loss: 0.90101\n",
      "[Epoch 4775/5000] loss: 0.67051\n",
      "[Epoch 4776/5000] loss: 0.74531\n",
      "[Epoch 4777/5000] loss: 0.46991\n",
      "[Epoch 4778/5000] loss: 0.76569\n",
      "[Epoch 4779/5000] loss: 1.01767\n",
      "[Epoch 4780/5000] loss: 1.02579\n",
      "[Epoch 4781/5000] loss: 0.52416\n",
      "[Epoch 4782/5000] loss: 0.54788\n",
      "[Epoch 4783/5000] loss: 1.37437\n",
      "[Epoch 4784/5000] loss: 0.93607\n",
      "[Epoch 4785/5000] loss: 0.73328\n",
      "[Epoch 4786/5000] loss: 1.61063\n",
      "[Epoch 4787/5000] loss: 0.90385\n",
      "[Epoch 4788/5000] loss: 0.86201\n",
      "[Epoch 4789/5000] loss: 1.07945\n",
      "[Epoch 4790/5000] loss: 0.76205\n",
      "[Epoch 4791/5000] loss: 0.57083\n",
      "[Epoch 4792/5000] loss: 1.17762\n",
      "[Epoch 4793/5000] loss: 1.13616\n",
      "[Epoch 4794/5000] loss: 0.46741\n",
      "[Epoch 4795/5000] loss: 0.78638\n",
      "[Epoch 4796/5000] loss: 0.82387\n",
      "[Epoch 4797/5000] loss: 0.91126\n",
      "[Epoch 4798/5000] loss: 3.12157\n",
      "[Epoch 4799/5000] loss: 1.30881\n",
      "[Epoch 4800/5000] loss: 0.71924\n",
      "[Epoch 4801/5000] loss: 1.32109\n",
      "[Epoch 4802/5000] loss: 0.57609\n",
      "[Epoch 4803/5000] loss: 1.16700\n",
      "[Epoch 4804/5000] loss: 1.08770\n",
      "[Epoch 4805/5000] loss: 0.85972\n",
      "[Epoch 4806/5000] loss: 1.10932\n",
      "[Epoch 4807/5000] loss: 1.73606\n",
      "[Epoch 4808/5000] loss: 0.62724\n",
      "[Epoch 4809/5000] loss: 1.05255\n",
      "[Epoch 4810/5000] loss: 0.92663\n",
      "[Epoch 4811/5000] loss: 0.89414\n",
      "[Epoch 4812/5000] loss: 0.73098\n",
      "[Epoch 4813/5000] loss: 1.57582\n",
      "[Epoch 4814/5000] loss: 0.54163\n",
      "[Epoch 4815/5000] loss: 0.83406\n",
      "[Epoch 4816/5000] loss: 0.87958\n",
      "[Epoch 4817/5000] loss: 0.83477\n",
      "[Epoch 4818/5000] loss: 0.56596\n",
      "[Epoch 4819/5000] loss: 0.95441\n",
      "[Epoch 4820/5000] loss: 3.35432\n",
      "[Epoch 4821/5000] loss: 0.95367\n",
      "[Epoch 4822/5000] loss: 1.08926\n",
      "[Epoch 4823/5000] loss: 1.06406\n",
      "[Epoch 4824/5000] loss: 1.25141\n",
      "[Epoch 4825/5000] loss: 1.09308\n",
      "[Epoch 4826/5000] loss: 1.12346\n",
      "[Epoch 4827/5000] loss: 3.09642\n",
      "[Epoch 4828/5000] loss: 1.19158\n",
      "[Epoch 4829/5000] loss: 1.03020\n",
      "[Epoch 4830/5000] loss: 1.12244\n",
      "[Epoch 4831/5000] loss: 1.05371\n",
      "[Epoch 4832/5000] loss: 0.63118\n",
      "[Epoch 4833/5000] loss: 1.20566\n",
      "[Epoch 4834/5000] loss: 1.06002\n",
      "[Epoch 4835/5000] loss: 1.08444\n",
      "[Epoch 4836/5000] loss: 0.66576\n",
      "[Epoch 4837/5000] loss: 1.23948\n",
      "[Epoch 4838/5000] loss: 0.76541\n",
      "[Epoch 4839/5000] loss: 0.36000\n",
      "[Epoch 4840/5000] loss: 0.95161\n",
      "[Epoch 4841/5000] loss: 0.96339\n",
      "[Epoch 4842/5000] loss: 0.86165\n",
      "[Epoch 4843/5000] loss: 1.30900\n",
      "[Epoch 4844/5000] loss: 1.08314\n",
      "[Epoch 4845/5000] loss: 0.99361\n",
      "[Epoch 4846/5000] loss: 1.17038\n",
      "[Epoch 4847/5000] loss: 1.30248\n",
      "[Epoch 4848/5000] loss: 0.80841\n",
      "[Epoch 4849/5000] loss: 1.64124\n",
      "[Epoch 4850/5000] loss: 1.23599\n",
      "[Epoch 4851/5000] loss: 0.97995\n",
      "[Epoch 4852/5000] loss: 0.83469\n",
      "[Epoch 4853/5000] loss: 3.74549\n",
      "[Epoch 4854/5000] loss: 1.21809\n",
      "[Epoch 4855/5000] loss: 1.21259\n",
      "[Epoch 4856/5000] loss: 0.86807\n",
      "[Epoch 4857/5000] loss: 3.14287\n",
      "[Epoch 4858/5000] loss: 1.05586\n",
      "[Epoch 4859/5000] loss: 0.81764\n",
      "[Epoch 4860/5000] loss: 1.07843\n",
      "[Epoch 4861/5000] loss: 0.99041\n",
      "[Epoch 4862/5000] loss: 1.03156\n",
      "[Epoch 4863/5000] loss: 0.51371\n",
      "[Epoch 4864/5000] loss: 0.70534\n",
      "[Epoch 4865/5000] loss: 1.09967\n",
      "[Epoch 4866/5000] loss: 1.16141\n",
      "[Epoch 4867/5000] loss: 0.97390\n",
      "[Epoch 4868/5000] loss: 1.13381\n",
      "[Epoch 4869/5000] loss: 0.83861\n",
      "[Epoch 4870/5000] loss: 1.31065\n",
      "[Epoch 4871/5000] loss: 1.27628\n",
      "[Epoch 4872/5000] loss: 1.55122\n",
      "[Epoch 4873/5000] loss: 0.71236\n",
      "[Epoch 4874/5000] loss: 1.26147\n",
      "[Epoch 4875/5000] loss: 1.32941\n",
      "[Epoch 4876/5000] loss: 0.77662\n",
      "[Epoch 4877/5000] loss: 1.00960\n",
      "[Epoch 4878/5000] loss: 1.42119\n",
      "[Epoch 4879/5000] loss: 0.81813\n",
      "[Epoch 4880/5000] loss: 0.86397\n",
      "[Epoch 4881/5000] loss: 3.43265\n",
      "[Epoch 4882/5000] loss: 1.14639\n",
      "[Epoch 4883/5000] loss: 1.15684\n",
      "[Epoch 4884/5000] loss: 0.97830\n",
      "[Epoch 4885/5000] loss: 1.06299\n",
      "[Epoch 4886/5000] loss: 0.62844\n",
      "[Epoch 4887/5000] loss: 1.23438\n",
      "[Epoch 4888/5000] loss: 0.67826\n",
      "[Epoch 4889/5000] loss: 0.72446\n",
      "[Epoch 4890/5000] loss: 1.26355\n",
      "[Epoch 4891/5000] loss: 1.10247\n",
      "[Epoch 4892/5000] loss: 0.76682\n",
      "[Epoch 4893/5000] loss: 1.64753\n",
      "[Epoch 4894/5000] loss: 0.66201\n",
      "[Epoch 4895/5000] loss: 0.86580\n",
      "[Epoch 4896/5000] loss: 1.26306\n",
      "[Epoch 4897/5000] loss: 0.95288\n",
      "[Epoch 4898/5000] loss: 1.20107\n",
      "[Epoch 4899/5000] loss: 1.36146\n",
      "[Epoch 4900/5000] loss: 0.97828\n",
      "[Epoch 4901/5000] loss: 0.84972\n",
      "[Epoch 4902/5000] loss: 1.00534\n",
      "[Epoch 4903/5000] loss: 1.82157\n",
      "[Epoch 4904/5000] loss: 0.56826\n",
      "[Epoch 4905/5000] loss: 1.01024\n",
      "[Epoch 4906/5000] loss: 0.87711\n",
      "[Epoch 4907/5000] loss: 1.14747\n",
      "[Epoch 4908/5000] loss: 1.30828\n",
      "[Epoch 4909/5000] loss: 1.13894\n",
      "[Epoch 4910/5000] loss: 0.70863\n",
      "[Epoch 4911/5000] loss: 0.76275\n",
      "[Epoch 4912/5000] loss: 0.86475\n",
      "[Epoch 4913/5000] loss: 1.27452\n",
      "[Epoch 4914/5000] loss: 1.10536\n",
      "[Epoch 4915/5000] loss: 0.94114\n",
      "[Epoch 4916/5000] loss: 0.97956\n",
      "[Epoch 4917/5000] loss: 0.89762\n",
      "[Epoch 4918/5000] loss: 0.79572\n",
      "[Epoch 4919/5000] loss: 0.56413\n",
      "[Epoch 4920/5000] loss: 1.63981\n",
      "[Epoch 4921/5000] loss: 0.48571\n",
      "[Epoch 4922/5000] loss: 0.80049\n",
      "[Epoch 4923/5000] loss: 1.01029\n",
      "[Epoch 4924/5000] loss: 1.01841\n",
      "[Epoch 4925/5000] loss: 1.23280\n",
      "[Epoch 4926/5000] loss: 1.07161\n",
      "[Epoch 4927/5000] loss: 1.03620\n",
      "[Epoch 4928/5000] loss: 1.14273\n",
      "[Epoch 4929/5000] loss: 0.78035\n",
      "[Epoch 4930/5000] loss: 0.71513\n",
      "[Epoch 4931/5000] loss: 1.18420\n",
      "[Epoch 4932/5000] loss: 2.85452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4933/5000] loss: 1.12815\n",
      "[Epoch 4934/5000] loss: 0.96548\n",
      "[Epoch 4935/5000] loss: 0.83553\n",
      "[Epoch 4936/5000] loss: 1.12934\n",
      "[Epoch 4937/5000] loss: 0.98718\n",
      "[Epoch 4938/5000] loss: 0.74969\n",
      "[Epoch 4939/5000] loss: 0.91326\n",
      "[Epoch 4940/5000] loss: 0.82504\n",
      "[Epoch 4941/5000] loss: 0.84025\n",
      "[Epoch 4942/5000] loss: 1.01718\n",
      "[Epoch 4943/5000] loss: 1.28484\n",
      "[Epoch 4944/5000] loss: 0.37323\n",
      "[Epoch 4945/5000] loss: 1.18165\n",
      "[Epoch 4946/5000] loss: 0.91226\n",
      "[Epoch 4947/5000] loss: 0.84507\n",
      "[Epoch 4948/5000] loss: 0.97112\n",
      "[Epoch 4949/5000] loss: 0.72018\n",
      "[Epoch 4950/5000] loss: 0.99275\n",
      "[Epoch 4951/5000] loss: 1.17357\n",
      "[Epoch 4952/5000] loss: 0.80441\n",
      "[Epoch 4953/5000] loss: 0.90894\n",
      "[Epoch 4954/5000] loss: 0.48300\n",
      "[Epoch 4955/5000] loss: 0.74943\n",
      "[Epoch 4956/5000] loss: 1.40896\n",
      "[Epoch 4957/5000] loss: 0.66242\n",
      "[Epoch 4958/5000] loss: 0.58977\n",
      "[Epoch 4959/5000] loss: 1.34833\n",
      "[Epoch 4960/5000] loss: 0.53969\n",
      "[Epoch 4961/5000] loss: 0.57573\n",
      "[Epoch 4962/5000] loss: 0.88708\n",
      "[Epoch 4963/5000] loss: 0.86921\n",
      "[Epoch 4964/5000] loss: 1.15101\n",
      "[Epoch 4965/5000] loss: 1.37392\n",
      "[Epoch 4966/5000] loss: 0.79088\n",
      "[Epoch 4967/5000] loss: 0.87365\n",
      "[Epoch 4968/5000] loss: 1.02975\n",
      "[Epoch 4969/5000] loss: 0.88048\n",
      "[Epoch 4970/5000] loss: 1.36238\n",
      "[Epoch 4971/5000] loss: 0.81737\n",
      "[Epoch 4972/5000] loss: 1.31359\n",
      "[Epoch 4973/5000] loss: 0.69176\n",
      "[Epoch 4974/5000] loss: 0.97065\n",
      "[Epoch 4975/5000] loss: 1.59158\n",
      "[Epoch 4976/5000] loss: 0.98524\n",
      "[Epoch 4977/5000] loss: 1.04324\n",
      "[Epoch 4978/5000] loss: 0.64151\n",
      "[Epoch 4979/5000] loss: 0.57841\n",
      "[Epoch 4980/5000] loss: 1.33622\n",
      "[Epoch 4981/5000] loss: 1.04085\n",
      "[Epoch 4982/5000] loss: 0.98995\n",
      "[Epoch 4983/5000] loss: 0.71886\n",
      "[Epoch 4984/5000] loss: 0.53103\n",
      "[Epoch 4985/5000] loss: 0.60471\n",
      "[Epoch 4986/5000] loss: 3.27520\n",
      "[Epoch 4987/5000] loss: 1.02876\n",
      "[Epoch 4988/5000] loss: 0.57576\n",
      "[Epoch 4989/5000] loss: 0.78869\n",
      "[Epoch 4990/5000] loss: 1.17091\n",
      "[Epoch 4991/5000] loss: 1.05499\n",
      "[Epoch 4992/5000] loss: 1.29972\n",
      "[Epoch 4993/5000] loss: 1.01579\n",
      "[Epoch 4994/5000] loss: 1.05794\n",
      "[Epoch 4995/5000] loss: 3.40278\n",
      "[Epoch 4996/5000] loss: 0.64546\n",
      "[Epoch 4997/5000] loss: 1.04010\n",
      "[Epoch 4998/5000] loss: 0.91186\n",
      "[Epoch 4999/5000] loss: 1.40921\n",
      "[Epoch 5000/5000] loss: 1.19509\n",
      "Finished Training\n",
      "OrderedDict([('layer1H.weight', tensor([[ 0.0925,  0.1250, -0.0760,  ...,  0.1569,  0.2035, -0.0569],\n",
      "        [ 0.0633,  0.0814, -0.1410,  ...,  0.2430, -0.0147,  0.1029],\n",
      "        [-0.0092, -0.0877, -0.0823,  ..., -0.0145,  0.0289, -0.0720],\n",
      "        ...,\n",
      "        [ 0.1231, -0.1536, -0.1114,  ..., -0.0945, -0.1581, -0.1970],\n",
      "        [-0.1636, -0.1880, -0.0071,  ...,  0.0082,  0.0148,  0.0187],\n",
      "        [ 0.1763, -0.0077,  0.0266,  ..., -0.1902,  0.1864, -0.1070]])), ('layer1H.bias', tensor([-0.0138,  0.0338, -0.0329, -0.0484, -0.0390, -0.0262,  0.0290,  0.0419,\n",
      "        -0.0028,  0.0244])), ('layer2H.weight', tensor([[-0.2861,  0.2011, -0.3203, -0.0494,  0.3788, -0.3702,  0.4878, -0.2295,\n",
      "         -0.0574,  0.2841],\n",
      "        [ 0.0828, -0.0498,  0.5398,  0.3793, -0.4686,  0.0370,  0.5063,  0.3356,\n",
      "          0.0390, -0.3497],\n",
      "        [ 0.3248, -0.3961, -0.2243,  0.0625, -0.3611,  0.2172, -0.2148,  0.5301,\n",
      "         -0.0873,  0.3714],\n",
      "        [ 0.0197, -0.1345,  0.3678, -0.5131, -0.1106, -0.0886, -0.2586, -0.5293,\n",
      "          0.1294,  0.1308],\n",
      "        [-0.3512, -0.3474, -0.3722,  0.5132,  0.3205,  0.0880,  0.4072, -0.5113,\n",
      "          0.3914, -0.2386],\n",
      "        [-0.0327,  0.1870, -0.5647, -0.2476,  0.2151, -0.4721,  0.1310,  0.4062,\n",
      "          0.0396,  0.1949],\n",
      "        [ 0.2350, -0.0458,  0.4640,  0.0567, -0.1098,  0.2404, -0.3526, -0.2060,\n",
      "         -0.2773,  0.4638],\n",
      "        [-0.5039,  0.4267, -0.1931,  0.2278,  0.2720,  0.2717,  0.4691, -0.3153,\n",
      "         -0.0258,  0.4500],\n",
      "        [ 0.3417, -0.0647, -0.1591,  0.1959, -0.1251,  0.1775,  0.0803,  0.0458,\n",
      "         -0.0471, -0.1990],\n",
      "        [-0.2717, -0.1052, -0.2486,  0.2122, -0.1838, -0.3766,  0.2171, -0.4739,\n",
      "          0.3588, -0.1088]])), ('layer2H.bias', tensor([ 0.0109, -0.0362,  0.0041, -0.0074, -0.0147, -0.0232, -0.0068, -0.0082,\n",
      "        -0.0176, -0.0054])), ('layer_outH.weight', tensor([[-1.0107e-01, -1.3016e-04,  2.3687e-01,  4.6768e-01, -2.0726e-01,\n",
      "          1.0624e-01,  1.5684e-01,  2.2644e-01, -4.0838e-04,  3.8182e-02]])), ('layer_outH.bias', tensor([-0.0013])), ('layer1C.weight', tensor([[-0.0860, -0.0826,  0.0428,  ...,  0.2037, -0.2018, -0.1278],\n",
      "        [ 0.1831,  0.0580, -0.2118,  ...,  0.2953,  0.1862,  0.0187],\n",
      "        [ 0.1046, -0.1770, -0.0376,  ..., -0.0933, -0.0730, -0.0250],\n",
      "        ...,\n",
      "        [ 0.1746, -0.2129,  0.0728,  ..., -0.1313, -0.0329, -0.0820],\n",
      "        [-0.0606,  0.1364, -0.0568,  ...,  0.0553,  0.1765, -0.1167],\n",
      "        [ 0.0513,  0.1048,  0.0820,  ...,  0.0956, -0.1395, -0.0013]])), ('layer1C.bias', tensor([ 0.0179,  0.0372,  0.0056, -0.0245,  0.0223, -0.0204,  0.0280,  0.0367,\n",
      "         0.0141, -0.0078])), ('layer2C.weight', tensor([[-0.1914,  0.0782, -0.5433,  0.1595, -0.1519, -0.0643,  0.0218,  0.1633,\n",
      "          0.0781, -0.0079],\n",
      "        [-0.3160, -0.3374,  0.5329, -0.1649,  0.0082, -0.4527,  0.2320,  0.1977,\n",
      "          0.3392,  0.1561],\n",
      "        [ 0.1463, -0.2997, -0.3610,  0.4890, -0.0251,  0.2616,  0.4475,  0.0866,\n",
      "         -0.5097,  0.1988],\n",
      "        [-0.3361, -0.2919,  0.3210,  0.2398,  0.5198,  0.4984,  0.1147, -0.4721,\n",
      "          0.2827,  0.3829],\n",
      "        [-0.3883, -0.0711,  0.5243, -0.4265,  0.5563, -0.5679,  0.5554,  0.0569,\n",
      "          0.1347,  0.0950],\n",
      "        [-0.4784, -0.2792,  0.3434, -0.2090, -0.4672, -0.0213, -0.5126,  0.1469,\n",
      "         -0.4370, -0.0863],\n",
      "        [-0.1319,  0.5282, -0.3442, -0.2686,  0.3473,  0.0115,  0.5552, -0.2290,\n",
      "         -0.5339,  0.0072],\n",
      "        [-0.0530, -0.0310,  0.1962, -0.3955,  0.0135, -0.2596,  0.3398, -0.3517,\n",
      "          0.3649,  0.3013],\n",
      "        [ 0.1340,  0.5204, -0.1915,  0.3422, -0.4530,  0.4447,  0.5064,  0.1308,\n",
      "          0.0755, -0.0233],\n",
      "        [ 0.0362, -0.4501, -0.4592, -0.0396,  0.0263, -0.1807, -0.2134, -0.1198,\n",
      "         -0.4013, -0.0700]])), ('layer2C.bias', tensor([-0.0019,  0.0116, -0.0089, -0.0171,  0.0344, -0.0162,  0.0029,  0.0128,\n",
      "         0.0259, -0.0149])), ('layer_outC.weight', tensor([[-5.0317e-01, -1.2000e-01, -6.3475e-01, -6.7706e-01, -6.6493e-01,\n",
      "          3.6351e-01,  3.5211e-01, -4.0933e-01,  3.2653e-04,  3.9429e-01]])), ('layer_outC.bias', tensor([-0.0013])), ('layer1O.weight', tensor([[ 0.0256,  0.0671, -0.1138,  ...,  0.1885, -0.1067, -0.1117],\n",
      "        [ 0.1377,  0.0298,  0.0040,  ..., -0.1965, -0.1560,  0.0367],\n",
      "        [-0.0810,  0.0956, -0.1922,  ..., -0.0860, -0.1147, -0.0825],\n",
      "        ...,\n",
      "        [-0.0814, -0.0236,  0.1402,  ..., -0.2164,  0.0461,  0.0890],\n",
      "        [-0.1780,  0.0626, -0.0116,  ...,  0.0431,  0.2067, -0.0027],\n",
      "        [-0.1011, -0.1792, -0.1907,  ..., -0.2415, -0.0952, -0.1802]])), ('layer1O.bias', tensor([ 0.0101,  0.0280,  0.0423, -0.0138,  0.0097,  0.0097,  0.0206,  0.0294,\n",
      "        -0.0181,  0.0185])), ('layer2O.weight', tensor([[-0.3204, -0.3390, -0.3182, -0.0125, -0.4558,  0.1515,  0.2566,  0.5500,\n",
      "          0.4371,  0.1988],\n",
      "        [ 0.4848, -0.1588, -0.3537,  0.4303, -0.4110, -0.3312,  0.2116,  0.1529,\n",
      "         -0.1452,  0.3982],\n",
      "        [-0.1297,  0.5308, -0.4831,  0.3403,  0.2773,  0.5467, -0.0566,  0.5003,\n",
      "          0.2149, -0.2093],\n",
      "        [-0.4731, -0.2691, -0.4420,  0.4719, -0.4418, -0.2138,  0.0060, -0.5032,\n",
      "         -0.3949,  0.3166],\n",
      "        [ 0.4512,  0.4317,  0.3724, -0.2797, -0.1851, -0.3700, -0.4703,  0.5443,\n",
      "         -0.3502,  0.2733],\n",
      "        [ 0.3030,  0.2792,  0.0431, -0.5136,  0.2509,  0.1420,  0.2149, -0.1335,\n",
      "          0.0503, -0.3076],\n",
      "        [-0.3745,  0.0157, -0.1526,  0.1997, -0.2235,  0.5245,  0.5003,  0.3443,\n",
      "          0.2700,  0.3471],\n",
      "        [-0.5276, -0.0449, -0.4320, -0.5062, -0.2759,  0.3259, -0.3638, -0.0390,\n",
      "         -0.2183,  0.2614],\n",
      "        [ 0.1273,  0.3208,  0.3739, -0.0477,  0.4887,  0.3372,  0.4623,  0.4558,\n",
      "         -0.1015,  0.0219],\n",
      "        [-0.3004, -0.1204, -0.4912,  0.4231, -0.1774, -0.2711,  0.5120, -0.3105,\n",
      "         -0.4544,  0.1691]])), ('layer2O.bias', tensor([ 0.0075,  0.0059,  0.0010, -0.0102,  0.0283, -0.0054,  0.0010,  0.0030,\n",
      "         0.0093,  0.0370])), ('layer_outO.weight', tensor([[ 9.3965e-04, -3.7129e-01, -3.5427e-01, -7.1380e-01, -3.4689e-01,\n",
      "         -2.9211e-01,  3.3198e-01, -6.0301e-05, -4.2037e-01, -1.1647e-02]])), ('layer_outO.bias', tensor([-0.0013]))])\n"
     ]
    }
   ],
   "source": [
    "# パラメーター（重みやバイアス）の初期化を行う関数の定義\n",
    "def init_parameters(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(layer.weight) # 重みを「一様分布のランダム値」に初期化\n",
    "        layer.bias.data.fill_(0.0)            # バイアスを「0」に初期化\n",
    "\n",
    "# 学習の前にパラメーター（重みやバイアス）を初期化する\n",
    "model.apply(init_parameters)\n",
    "\n",
    "# 定数（学習／評価時に必要となるもの）\n",
    "EPOCHS = 5000     # エポック数： 2000\n",
    "\n",
    "# 変数（学習／評価時に必要となるもの）\n",
    "avg_loss = 0.0           # 「訓練」用の平均「損失値」\n",
    "#avg_acc = 0.0            # 「訓練」用の平均「正解率」\n",
    "avg_val_loss = 0.0       # 「評価」用の平均「損失値」\n",
    "#avg_val_acc = 0.0        # 「評価」用の平均「正解率」\n",
    "\n",
    "# 損失の履歴を保存するための変数\n",
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # forループ内で使う変数と、エポックごとの値リセット\n",
    "    total_loss = 0.0     # 「訓練」時における累計「損失値」\n",
    "    #total_acc = 0.0      # 「訓練」時における累計「正解数」\n",
    "    total_val_loss = 0.0 # 「評価」時における累計「損失値」\n",
    "    #total_val_acc = 0.0  # 「評価」時における累計「正解数」\n",
    "    total_train = 0      # 「訓練」時における累計「データ数」\n",
    "    total_valid = 0      # 「評価」時における累計「データ数」\n",
    "\n",
    "    for train_X, train_y in loader_train:\n",
    "        # 【重要】1ミニバッチ分の「訓練」を実行\n",
    "        #loss, acc = train_step(train_X, train_y)\n",
    "        loss = train_step(train_X, train_y)\n",
    "        #print(loss)\n",
    "\n",
    "        # 取得した損失値と正解率を累計値側に足していく\n",
    "        total_loss += loss          # 訓練用の累計損失値\n",
    "        #total_acc += acc            # 訓練用の累計正解数\n",
    "        total_train += len(train_y) # 訓練データの累計数\n",
    "            \n",
    "    for valid_X, valid_y in loader_valid:\n",
    "        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n",
    "        #val_loss, val_acc = valid_step(valid_X, valid_y)\n",
    "        val_loss = valid_step(valid_X, valid_y)\n",
    "\n",
    "        # 取得した損失値と正解率を累計値側に足していく\n",
    "        total_val_loss += val_loss  # 評価用の累計損失値\n",
    "        #total_val_acc += val_acc    # 評価用の累計正解数\n",
    "        total_valid += len(valid_y) # 訓練データの累計数\n",
    "\n",
    "    # ミニバッチ単位で累計してきた損失値や正解率の平均を取る\n",
    "    n = epoch + 1                             # 処理済みのエポック数\n",
    "    avg_loss = total_loss / n                 # 訓練用の平均損失値\n",
    "    #avg_acc = total_acc / total_train         # 訓練用の平均正解率\n",
    "    avg_val_loss = total_val_loss / n         # 訓練用の平均損失値\n",
    "    #avg_val_acc = total_val_acc / total_valid # 訓練用の平均正解率\n",
    "\n",
    "    # グラフ描画のために損失の履歴を保存する\n",
    "    train_history.append(loss)\n",
    "    valid_history.append(val_loss)\n",
    "\n",
    "    # 損失や正解率などの情報を表示\n",
    "    print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n",
    "         f' loss: {loss:.5f}') \n",
    "    #print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n",
    "    #      f' loss: {avg_loss:.5f}, acc: {0.000:.5f}' \\\n",
    "    #      f' val_loss: {avg_val_loss:.5f}, val_acc: {0.000:.5f}')\n",
    "\n",
    "print('Finished Training')\n",
    "print(model.state_dict())  # 学習後のパラメーターの情報を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 学習結果（損失）のグラフを描画\n",
    "epochs = len(train_history)\n",
    "plt.plot(range(epochs)[2000:], train_history[2000:], marker='.', label='loss (Training data)')\n",
    "plt.plot(range(epochs)[2000:], valid_history[2000:], marker='.', label='loss (Validation data)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_test_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-89d6f51ba601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_test_y' is not defined"
     ]
    }
   ],
   "source": [
    "criterion(pred_test_y, test_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_y = model(descs_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([901])) that is different to the input size (torch.Size([901, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0154, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(demo_y, true_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X.requires_grad_(True)\n",
    "pred_test_y = model(test_X) \n",
    "#x= pred_test_y.detach().numpy() * train_y_std + train_y_mean\n",
    "#y= test_y.detach().numpy() * train_y_std + train_y_mean\n",
    "x= pred_test_y.detach().numpy() \n",
    "y= test_y.detach().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.60526940981015"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f47151bfef0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_y = model(descs_X) \n",
    "x= demo_y.detach().numpy() * train_y_std + train_y_mean\n",
    "y= true_y.detach().numpy() * train_y_std + train_y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamazaki/miniconda3/envs/openmm/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([901])) that is different to the input size (torch.Size([901, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0154, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(demo_y, true_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f47151054e0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "plt.scatter(demo_y.detach().numpy() ,true_y.detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chebyshev記述子で分布関数を正しく表現できているか確認する \n",
    "dist_i = dist[atom_ind]\n",
    "neighbors_list = np.argwhere(dist_i<=Rcut).reshape(-1)\n",
    "neighbors_list = neighbors_list[neighbors_list !=atom_ind]\n",
    "distances = dist_i[neighbors_list]\n",
    "i = atom_ind \n",
    "indices = [(j,i,k) for j in neighbors_list if j!=i for k in neighbors_list if (k!=i) and (k!=j)]\n",
    "angles = mol1.get_angles(indices,mic=True)\n",
    "\n",
    "#重みの設定\n",
    "weight_RDF = fc(distances,Rcut)\n",
    "weight_ADFj = fc(dist_i[np.array(indices)[:,0]],Rcut)\n",
    "weight_ADFk = fc(dist_i[np.array(indices)[:,2]],Rcut)\n",
    "\n",
    "#動径分布関数の計算と表示\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "RDF_i= np.histogram(distances, bins=1000, weights=weight_RDF, density=True,range=(0,Rcut))\n",
    "x=(RDF_i[1][:-1]+RDF_i[1][1:])/2\n",
    "y=RDF_i[0]\n",
    "dr = x[1]-x[0]\n",
    "sigma = 0.1 \n",
    "ngs = int(sigma / dr) \n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "blurred_y = gaussian_filter(y, sigma=ngs)\n",
    "plt.plot(x,blurred_y)\n",
    "plt.show()\n",
    "rdf0x = x[:]\n",
    "rdf0y = blurred_y[:]\n",
    "\n",
    "#角度分布関数の計算と表示\n",
    "ADF_i= np.histogram(angles, bins=1000, weights=weight_ADFj*weight_ADFk, density=True,range=(0,180))\n",
    "x=(ADF_i[1][:-1]+ADF_i[1][1:])/2\n",
    "y=ADF_i[0]\n",
    "dr = x[1]-x[0]\n",
    "sigma = 1 \n",
    "ngs = int(sigma / dr) \n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "blurred_y = gaussian_filter(y, sigma=ngs)\n",
    "plt.plot(x,blurred_y)\n",
    "plt.show()\n",
    "\n",
    "adf0x = x[:]\n",
    "adf0y = blurred_y[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#原子種の重みを考慮する\n",
    "\n",
    "#動径分布関数の計算と表示\n",
    "wRDF_i= np.histogram(distances, bins=1000, weights=weight_RDF*w[neighbors_list], density=True,range=(0,Rcut))\n",
    "wADF_i= np.histogram(angles, bins=1000, weights=weight_ADFj*weight_ADFk*w[np.array(indices)[:,0]]*w[np.array(indices)[:,2]], density=True,range=(0,180))\n",
    "x=(wRDF_i[1][:-1]+wRDF_i[1][1:])/2\n",
    "y=wRDF_i[0]\n",
    "dr = x[1]-x[0]\n",
    "sigma = 0.2 \n",
    "ngs = int(sigma / dr) \n",
    "blurred_y = gaussian_filter(y, sigma=ngs)\n",
    "plt.plot(x,blurred_y)\n",
    "plt.show()\n",
    "\n",
    "rdf0wx = x[:]\n",
    "rdf0wy = blurred_y[:]\n",
    "\n",
    "#角度分布関数の計算と表示\n",
    "x=(wADF_i[1][:-1]+wADF_i[1][1:])/2\n",
    "y=wADF_i[0]\n",
    "dr = x[1]-x[0]\n",
    "sigma = 2 \n",
    "ngs = int(sigma / dr) \n",
    "blurred_y = gaussian_filter(y, sigma=ngs)\n",
    "plt.plot(x,blurred_y)\n",
    "plt.show()\n",
    "\n",
    "adf0wx = x[:]\n",
    "adf0wy = blurred_y[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chebyshevの展開式を用いる\n",
    "dd = 0.2\n",
    "r = np.linspace(0+dd,Rcut-dd,1000)\n",
    "\n",
    "#Chebyshev関数\n",
    "funcT = []\n",
    "x = 2.0*r/Rcut - 1.0 \n",
    "for n2 in range(N2):\n",
    "    if n2==0 :\n",
    "        funcT.append(np.ones(x.shape))\n",
    "    if n2==1 :\n",
    "        funcT.append(x) \n",
    "    if n2>1 :\n",
    "        funcT.append(2.0*x*funcT[n2-1]-funcT[n2-2])\n",
    "\n",
    "RDF   =np.zeros(r.shape)\n",
    "RDFw  =np.zeros(r.shape)\n",
    "for n2 in range(N2):\n",
    "    if n2==0 :\n",
    "        k=0.5\n",
    "    else :\n",
    "        k=1.0 \n",
    "    phi = k*funcT[n2]/(2.0*np.pi*np.sqrt(r/Rcut-r*r/(Rcut*Rcut)))\n",
    "    RDF  += c2[n2]*phi\n",
    "    #RDF  += phi\n",
    "    RDFw += c2w[n2]*phi\n",
    "\n",
    "dd = 10\n",
    "th = np.linspace(0+dd,180-dd,1000)\n",
    "\n",
    "#Chebyshev記述子を作る（角度分布σ3について）\n",
    "th_rad = np.pi*th/180.0\n",
    "\n",
    "#Chebyshev関数\n",
    "funcT = []\n",
    "x = 2.0*th_rad/np.pi - 1.0 \n",
    "for n3 in range(N3):\n",
    "    if n3==0 :\n",
    "        funcT.append(np.ones(x.shape))\n",
    "    if n3==1 :\n",
    "        funcT.append(x) \n",
    "    if n3>1 :\n",
    "        funcT.append(2.0*x*funcT[n3-1]-funcT[n3-2])\n",
    "\n",
    "ADF  =np.zeros(th.shape)  \n",
    "ADFw =np.zeros(th.shape)  \n",
    "for n3 in range(N3):\n",
    "    if n3==0 :\n",
    "        k=0.5 \n",
    "    else :\n",
    "        k=1.0 \n",
    "    phi = k*funcT[n3]/(2.0*np.pi*np.sqrt(th_rad/np.pi-th_rad*th_rad/(np.pi*np.pi)))\n",
    "    ADF  += c3[n3]*phi\n",
    "    ADFw += c3w[n3]*phi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#動径分布関数の比較\n",
    "plt.plot(r,RDF)\n",
    "plt.plot(rdf0x,rdf0y*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#動径分布関数の比較(原子種の重みを付与)\n",
    "plt.plot(r,-RDFw) #なぜかマイナスをつけないと合わない。\n",
    "plt.plot(rdf0wx,rdf0wy*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#角度分布関数の比較\n",
    "plt.plot(th,ADF)\n",
    "plt.plot(adf0x,adf0y*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#角度分布関数の比較（原子種の重みを付与）\n",
    "plt.plot(th,-ADFw)\n",
    "plt.plot(adf0wx,adf0wy*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
    "\n",
    "# 定数（モデル定義時に必要となるもの）\n",
    "INPUT_FEATURES = 2  # 入力（特徴）の数： 2\n",
    "OUTPUT_NEURONS = 1  # ニューロンの数： 1\n",
    "\n",
    "# 変数（モデル定義時に必要となるもの）\n",
    "activation = torch.nn.Tanh()  # 活性化関数： tanh関数\n",
    "\n",
    "# 「torch.nn.Moduleクラスのサブクラス化」によるモデルの定義\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # 層（layer：レイヤー）を定義\n",
    "        self.layer1 = nn.Linear(  # Linearは「全結合層」を指す\n",
    "            INPUT_FEATURES,       # データ（特徴）の入力ユニット数\n",
    "            OUTPUT_NEURONS)       # 出力結果への出力ユニット数\n",
    "\n",
    "    def forward(self, input):\n",
    "        # フォワードパスを定義\n",
    "        output = activation(self.layer1(input))  # 活性化関数は変数として定義\n",
    "        # 「出力＝活性化関数（第n層（入力））」の形式で記述する。\n",
    "        # 層（layer）を重ねる場合は、同様の記述を続ければよい（第3回）。\n",
    "        # 「出力（output）」は次の層（layer）への「入力（input）」に使う。\n",
    "        # 慣例では入力も出力も「x」と同じ変数名で記述する（よって以下では「x」と書く）\n",
    "        return output\n",
    "\n",
    "# モデル（NeuralNetworkクラス）のインスタンス化\n",
    "model = NeuralNetwork()\n",
    "model   # モデルの内容を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメーター（ニューロンへの入力で必要となるもの）の定義\n",
    "weight_array = nn.Parameter(\n",
    "    torch.tensor([[ 0.6,\n",
    "                   -0.2]]))  # 重み\n",
    "bias_array = nn.Parameter(\n",
    "    torch.tensor([  0.8 ]))  # バイアス\n",
    "\n",
    "# 重みとバイアスの初期値設定\n",
    "model.layer1.weight = weight_array\n",
    "model.layer1.bias = bias_array\n",
    "\n",
    "# torch.nn.Module全体の状態を辞書形式で取得\n",
    "params = model.state_dict()\n",
    "#params = list(model.parameters()) # このように取得することも可能\n",
    "params\n",
    "# 出力例：\n",
    "# OrderedDict([('layer1.weight', tensor([[ 0.6000, -0.2000]])),\n",
    "#              ('layer1.bias', tensor([0.8000]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = torch.tensor([[1.0, 2.0]])  # 入力する座標データ（1.0、2.0）\n",
    "print(X_data)\n",
    "# tensor([[1., 2.]]) ……などと表示される\n",
    "\n",
    "y_pred = model(X_data)  # このモデルに、データを入力して、出力を得る（＝予測：predict）\n",
    "print(y_pred)\n",
    "# tensor([[0.7616]], grad_fn=<TanhBackward>) ……などと表示される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# テンソルの新規作成\n",
    "x = torch.empty(2, 3) # 2行×3列のテンソル（未初期化状態）を生成\n",
    "x = torch.rand(2, 3)  # 2行×3列のテンソル（ランダムに初期化）を生成\n",
    "x = torch.zeros(2, 3, dtype=torch.float) # 2行×3列のテンソル（0で初期化、torch.float型）を生成\n",
    "x = torch.ones(2, 3, dtype=torch.float)  # 2行×3列のテンソル（1で初期化、torch.float型）を生成\n",
    "x = torch.tensor([[0.0, 0.1, 0.2],\n",
    "                  [1.0, 1.1, 1.2]])      # 1行×2列のテンソルをPythonリスト値から作成\n",
    "\n",
    "# 既存のテンソルを使った新規作成\n",
    "# 「new_*()」パターン\n",
    "y = x.new_ones(2, 3)   # 2行×3列のテンソル（1で初期化、既存のテンソルと「同じデータ型」）を生成\n",
    "# 「*_like()」パターン # 既存のテンソルと「同じサイズ」のテンソル（1で初期化、torch.int型）を生成\n",
    "y = torch.ones_like(x, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソルサイズの取得\n",
    "x.size()               # 「torch.Size([2, 3])」のように、2行3列と出力される\n",
    "x.shape                # NumPy風の記述も可能。出力は上と同じ\n",
    "len(x)   # 行数（＝データ数）を取得する際も、NumPy風に記述することが可能\n",
    "x.ndim   # テンソルの次元数を取得する際も、NumPy風に記述が可能\n",
    "\n",
    "# テンソルのサイズ変更／形状変更\n",
    "z = x.view(3, 2)       # 3行2列に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソルの計算操作\n",
    "x + y                  # 演算子を使う方法\n",
    "torch.add(x, y)        # 関数を使う方法\n",
    "torch.add(x, y, out=x) # outパラメーターで出力先の変数を指定可能\n",
    "x.add_(y)              # 「*_()」パターン。xを置き換えて出力する例（上の行と同じ処理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデクシングやスライシング（NumPyのような添え字を使用可能）\n",
    "print(x)         # 元は、2行3列のテンソル\n",
    "x[0, 1]          # 1行2列目（※0スタート）を取得\n",
    "x[:2, 1:]        # 先頭～2行（＝0行目と1行目）×2列～末尾（＝2列目と3列目）の2行2列が抽出される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソルの1つの要素値を、Pythonの数値に変換\n",
    "x[0, 1].item()   # 1行2列目（※0スタート）の要素値をPythonの数値で取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchテンソルを、NumPy多次元配列に変換\n",
    "b = x.numpy()    # 「numpy()」を呼び出すだけ。以下は注意点（メモリ位置の共有）\n",
    "\n",
    "# ※PyTorchテンソル側の値を変えると、NumPy多次元配列値「b」も変化する（トラックされる）\n",
    "print ('PyTorch計算→NumPy反映：')\n",
    "print(b); x.add_(y); print(b)           # PyTorch側の計算はNumPy側に反映\n",
    "print ('NumPy計算→PyTorch反映：')\n",
    "print(x); np.add(b, b, out=b); print(x) # NumPy側の計算はPyTorch側に反映\n",
    "\n",
    "# -----------------------------------------\n",
    "# NumPy多次元配列を、PyTorchテンソルに変換\n",
    "c = np.ones((2, 3), dtype=np.float64) # 2行3列の多次元配列値（1で初期化）を生成\n",
    "d = torch.from_numpy(c)  # 「torch.from_numpy()」を呼び出すだけ\n",
    "\n",
    "# ※NumPy多次元配列値を変えると、PyTorchテンソル「d」も変化する（トラックされる）\n",
    "print ('NumPy計算→PyTorch反映：')\n",
    "print(d); np.add(c, c, out=c); print(d)  # NumPy側の計算はPyTorch側に反映\n",
    "print ('PyTorch計算→NumPy反映：')\n",
    "print(c); d.add_(y); print(c)            # PyTorch側の計算はNumPy側に反映"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ型の変換（※変換後のテンソルには、NumPyの計算は反映されない）\n",
    "e = d.float()  # 「torch.float64」から「torch.float32」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIAのGPUである「CUDA」（GPU）デバイス環境が利用可能な場合、\n",
    "# GPUを使ってテンソルの計算を行うこともできる\n",
    "if torch.cuda.is_available():              # CUDA（GPU）が利用可能な場合\n",
    "    print('CUDA（GPU）が利用できる環境')\n",
    "    print(f'CUDAデバイス数： {torch.cuda.device_count()}')\n",
    "    print(f'現在のCUDAデバイス番号： {torch.cuda.current_device()}')  # ※0スタート\n",
    "    print(f'1番目のCUDAデバイス名： {torch.cuda.get_device_name(0)}') # 例「Tesla T4」  \n",
    "\n",
    "    device = torch.device(\"cuda\")          # デフォルトのCUDAデバイスオブジェクトを取得\n",
    "    device0 = torch.device(\"cuda:0\")       # 1番目（※0スタート）のCUDAデバイスを取得\n",
    "\n",
    "    # テンソル計算でのGPUの使い方は主に3つ：\n",
    "    g = torch.ones(2, 3, device=device)    # （1）テンソル生成時のパラメーター指定\n",
    "    g = x.to(device)                       # （2）既存テンソルのデバイス変更\n",
    "    g = x.cuda(device)                     # （3）既存テンソルの「CUDA（GPU）」利用\n",
    "    f = x.cpu()                            # （3'）既存テンソルの「CPU」利用\n",
    "\n",
    "    # ※（2）の使い方で、GPUは「.to(\"cuda\")」、CPUは「.to(\"cpu\")」と書いてもよい\n",
    "    g = x.to(\"cuda\")\n",
    "    f = x.to(\"cpu\")\n",
    "\n",
    "    # ※（3）の引数は省略することも可能\n",
    "    g = x.cuda()\n",
    "\n",
    "    # 「torch.nn.Module」オブジェクト（model）全体でのGPU／CPUの切り替え\n",
    "    model.cuda()  # モデルの全パラメーターとバッファーを「CUDA（GPU）」に移行する\n",
    "    model.cpu()   # モデルの全パラメーターとバッファーを「CPU」に移行する\n",
    "else:\n",
    "    print('CUDA（GPU）が利用できない環境')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 座標点データを生成するライブラリのインストール\n",
    "!pip install playground-data\n",
    "\n",
    "# playground-dataライブラリのplygdataパッケージを「pg」という別名でインポート\n",
    "import plygdata as pg\n",
    "\n",
    "# 設定値を定数として定義\n",
    "PROBLEM_DATA_TYPE = pg.DatasetType.ClassifyCircleData # 問題種別：「分類（Classification）」、データ種別：「円（CircleData）」を選択\n",
    "TRAINING_DATA_RATIO = 0.5  # データの何％を訓練【Training】用に？ (残りは精度検証【Validation】用) ： 50％\n",
    "DATA_NOISE = 0.0           # ノイズ： 0％\n",
    "\n",
    "# 定義済みの定数を引数に指定して、データを生成する\n",
    "data_list = pg.generate_data(PROBLEM_DATA_TYPE, DATA_NOISE)\n",
    "\n",
    "# データを「訓練用」と「精度検証用」を指定の比率で分割し、さらにそれぞれを「データ（X）」と「教師ラベル（y）」に分ける\n",
    "X_train, y_train, X_valid, y_valid = pg.split_data(data_list, training_size=TRAINING_DATA_RATIO)\n",
    "\n",
    "# データ分割後の各変数の内容例として、それぞれ5件ずつ出力\n",
    "print('X_train:'); print(X_train[:5])\n",
    "print('y_train:'); print(y_train[:5])\n",
    "print('X_valid:'); print(X_valid[:5])\n",
    "print('y_valid:'); print(y_valid[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ関連のユーティリティクラスをインポート\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "\n",
    "# 定数（学習方法設計時に必要となるもの）\n",
    "BATCH_SIZE = 15  # バッチサイズ： 15（Playgroundの選択肢は「1」～「30」）\n",
    "\n",
    "# NumPy多次元配列からテンソルに変換し、データ型はfloatに変換する\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).float()\n",
    "t_X_valid = torch.from_numpy(X_valid).float()\n",
    "t_y_valid = torch.from_numpy(y_valid).float()\n",
    "\n",
    "# 「データ（X）」と「教師ラベル（y）」を、1つの「データセット（dataset）」にまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)  # 訓練用\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)  # 精度検証用\n",
    "\n",
    "# ミニバッチを扱うための「データローダー（loader）」（訓練用と精度検証用）を作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
    "\n",
    "# 離散化を行う関数\n",
    "def discretize(proba):\n",
    "    '''\n",
    "    実数の確率値を「1」か「-1」の2クラス分類値に離散化する。\n",
    "    閾値は「0.0以上」か「未満」か。データ型は「torch.float」を想定。\n",
    " \n",
    "    Examples:\n",
    "        >>> proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)\n",
    "        >>> binary = discretize(proba)\n",
    "    '''\n",
    "    threshold = torch.Tensor([0.0]) # -1か1かを分ける閾値を作成\n",
    "    discretized = (proba >= threshold).float() # 閾値未満で0、以上で1に変換\n",
    "    return discretized * 2 - 1.0 # 2倍して-1.0することで、0／1を-1.0／1.0にスケール変換\n",
    "\n",
    "# discretize関数をモデルで簡単に使用できるようにするため、\n",
    "# PyTorchの「torch.nn.Module」を継承したクラスラッパーも作成した\n",
    "class Discretize(nn.Module):\n",
    "    '''\n",
    "    実数の確率値を「1」か「-1」の2クラス分類値に離散化する。\n",
    "    閾値は「0.0以上」か「未満」か。データ型は「torch.float」を想定。\n",
    " \n",
    "    Examples:\n",
    "        >>> d = Discretize()\n",
    "        >>> proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)\n",
    "        >>> binary = d(proba)\n",
    "    '''        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # forward()メソッドは、基本クラス「torch.nn.Module」の__call__メソッドからも呼び出されるため、\n",
    "    # Discretizeオブジェクトを関数のように使える（例えば上記の「d(proba)」）\n",
    "    def forward(self, proba):\n",
    "        return discretize(proba) # 上記の関数を呼び出すだけ\n",
    "\n",
    "# 関数の利用をテスト\n",
    "proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)  # 確率値の例\n",
    "binary = discretize(proba)  # 2クラス分類（binary classification）値に離散化\n",
    "binary  # tensor([-1.,  1.,  1.]) …… などと表示される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
    "\n",
    "# 定数（モデル定義時に必要となるもの）\n",
    "INPUT_FEATURES = 2      # 入力（特徴）の数： 2\n",
    "LAYER1_NEURONS = 3      # ニューロンの数： 3\n",
    "LAYER2_NEURONS = 3      # ニューロンの数： 3\n",
    "OUTPUT_RESULTS = 1      # 出力結果の数： 1\n",
    "\n",
    "# 変数（モデル定義時に必要となるもの）\n",
    "activation1 = torch.nn.Tanh()  # 活性化関数（隠れ層用）： tanh関数（変更可能）\n",
    "activation2 = torch.nn.Tanh()  # 活性化関数（隠れ層用）： tanh関数（変更可能）\n",
    "acti_out = torch.nn.Tanh()     # 活性化関数（出力層用）： tanh関数（固定）\n",
    "\n",
    "# torch.nn.Moduleによるモデルの定義\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # 隠れ層：1つ目のレイヤー（layer）\n",
    "        self.layer1 = nn.Linear(\n",
    "            INPUT_FEATURES,                # 入力ユニット数（＝入力層）\n",
    "            LAYER1_NEURONS)                # 次のレイヤーの出力ユニット数\n",
    "\n",
    "        # 隠れ層：2つ目のレイヤー（layer）\n",
    "        self.layer2 = nn.Linear(\n",
    "            LAYER1_NEURONS,                # 入力ユニット数\n",
    "            LAYER2_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
    "\n",
    "        # 出力層\n",
    "        self.layer_out = nn.Linear(\n",
    "            LAYER2_NEURONS,                # 入力ユニット数\n",
    "            OUTPUT_RESULTS)                # 出力結果への出力ユニット数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # フォワードパスを定義\n",
    "        # 「出力＝活性化関数（第n層（入力））」の形式で記述する\n",
    "        x = activation1(self.layer1(x))  # 活性化関数は変数として定義\n",
    "        x = activation2(self.layer2(x))  # 同上\n",
    "        x = acti_out(self.layer_out(x))  # ※活性化関数は「tanh」固定\n",
    "        return x\n",
    "\n",
    "# モデル（NeuralNetworkクラス）のインスタンス化\n",
    "model = NeuralNetwork()\n",
    "model   # モデルの内容を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim   # 「最適化」モジュールの別名定義\n",
    "\n",
    "# 定数（学習方法設計時に必要となるもの）\n",
    "LEARNING_RATE = 0.03   # 学習率： 0.03\n",
    "REGULARIZATION = 0.03  # 正則化率： 0.03\n",
    "\n",
    "# オプティマイザを作成（パラメーターと学習率も指定）\n",
    "optimizer = optim.SGD(           # 最適化アルゴリズムに「SGD」を選択\n",
    "    model.parameters(),          # 最適化で更新対象のパラメーター（重みやバイアス）\n",
    "    lr=LEARNING_RATE,            # 更新時の学習率\n",
    "    weight_decay=REGULARIZATION) # L2正則化（※不要な場合は0か省略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数（学習方法設計時に必要となるもの）\n",
    "criterion = nn.MSELoss()  # 損失関数：平均二乗誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_X, train_y):\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "\n",
    "    # フォワードプロパゲーションで出力結果を取得\n",
    "    #train_X                # 入力データ\n",
    "    pred_y = model(train_X) # 出力結果\n",
    "    #train_y                # 正解ラベル\n",
    "\n",
    "    # 出力結果と正解ラベルから損失を計算し、勾配を求める\n",
    "    optimizer.zero_grad()   # 勾配を0で初期化（※累積してしまうため要注意）\n",
    "    loss = criterion(pred_y, train_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
    "    loss.backward()   # 逆伝播の処理として勾配を計算（自動微分）\n",
    "\n",
    "    # 勾配を使ってパラメーター（重みとバイアス）を更新\n",
    "    optimizer.step()  # 指定されたデータ分の最適化を実施\n",
    "\n",
    "    # 正解数を算出\n",
    "    with torch.no_grad(): # 勾配は計算しないモードにする\n",
    "        discr_y = discretize(pred_y)         # 確率値から「-1」／「1」に変換\n",
    "        acc = (discr_y == train_y).sum()     # 正解数を計算する\n",
    "\n",
    "    # 損失と正解数をタプルで返す\n",
    "    return (loss.item(), acc.item())  # ※item()=Pythonの数値\n",
    "\n",
    "def valid_step(valid_X, valid_y):\n",
    "    # 評価モードに設定（※dropoutなどの挙動が評価用になる）\n",
    "    model.eval()\n",
    "    \n",
    "    # フォワードプロパゲーションで出力結果を取得\n",
    "    #valid_X                # 入力データ\n",
    "    pred_y = model(valid_X) # 出力結果\n",
    "    #valid_y                # 正解ラベル\n",
    "\n",
    "    # 出力結果と正解ラベルから損失を計算\n",
    "    loss = criterion(pred_y, valid_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
    "    # ※評価時は勾配を計算しない\n",
    "\n",
    "    # 正解数を算出\n",
    "    with torch.no_grad(): # 勾配は計算しないモードにする\n",
    "        discr_y = discretize(pred_y)     # 確率値から「-1」／「1」に変換\n",
    "        acc = (discr_y == valid_y).sum() # 正解数を合計する\n",
    "\n",
    "    # 損失と正解数をタプルで返す\n",
    "    return (loss.item(), acc.item())  # ※item()=Pythonの数値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメーター（重みやバイアス）の初期化を行う関数の定義\n",
    "def init_parameters(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(layer.weight) # 重みを「一様分布のランダム値」に初期化\n",
    "        layer.bias.data.fill_(0.0)            # バイアスを「0」に初期化\n",
    "\n",
    "# 学習の前にパラメーター（重みやバイアス）を初期化する\n",
    "model.apply(init_parameters)\n",
    "\n",
    "# 定数（学習／評価時に必要となるもの）\n",
    "EPOCHS = 100             # エポック数： 100\n",
    "\n",
    "# 変数（学習／評価時に必要となるもの）\n",
    "avg_loss = 0.0           # 「訓練」用の平均「損失値」\n",
    "avg_acc = 0.0            # 「訓練」用の平均「正解率」\n",
    "avg_val_loss = 0.0       # 「評価」用の平均「損失値」\n",
    "avg_val_acc = 0.0        # 「評価」用の平均「正解率」\n",
    "\n",
    "# 損失の履歴を保存するための変数\n",
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # forループ内で使う変数と、エポックごとの値リセット\n",
    "    total_loss = 0.0     # 「訓練」時における累計「損失値」\n",
    "    total_acc = 0.0      # 「訓練」時における累計「正解数」\n",
    "    total_val_loss = 0.0 # 「評価」時における累計「損失値」\n",
    "    total_val_acc = 0.0  # 「評価」時における累計「正解数」\n",
    "    total_train = 0      # 「訓練」時における累計「データ数」\n",
    "    total_valid = 0      # 「評価」時における累計「データ数」\n",
    "\n",
    "    for train_X, train_y in loader_train:\n",
    "        # 【重要】1ミニバッチ分の「訓練」を実行\n",
    "        loss, acc = train_step(train_X, train_y)\n",
    "\n",
    "        # 取得した損失値と正解率を累計値側に足していく\n",
    "        total_loss += loss          # 訓練用の累計損失値\n",
    "        total_acc += acc            # 訓練用の累計正解数\n",
    "        total_train += len(train_y) # 訓練データの累計数\n",
    "            \n",
    "    for valid_X, valid_y in loader_valid:\n",
    "        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n",
    "        val_loss, val_acc = valid_step(valid_X, valid_y)\n",
    "\n",
    "        # 取得した損失値と正解率を累計値側に足していく\n",
    "        total_val_loss += val_loss  # 評価用の累計損失値\n",
    "        total_val_acc += val_acc    # 評価用の累計正解数\n",
    "        total_valid += len(valid_y) # 訓練データの累計数\n",
    "\n",
    "    # ミニバッチ単位で累計してきた損失値や正解率の平均を取る\n",
    "    n = epoch + 1                             # 処理済みのエポック数\n",
    "    avg_loss = total_loss / n                 # 訓練用の平均損失値\n",
    "    avg_acc = total_acc / total_train         # 訓練用の平均正解率\n",
    "    avg_val_loss = total_val_loss / n         # 訓練用の平均損失値\n",
    "    avg_val_acc = total_val_acc / total_valid # 訓練用の平均正解率\n",
    "\n",
    "    # グラフ描画のために損失の履歴を保存する\n",
    "    train_history.append(avg_loss)\n",
    "    valid_history.append(avg_val_loss)\n",
    "\n",
    "    # 損失や正解率などの情報を表示\n",
    "    print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n",
    "          f' loss: {avg_loss:.5f}, acc: {avg_acc:.5f}' \\\n",
    "          f' val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f}')\n",
    "\n",
    "print('Finished Training')\n",
    "print(model.state_dict())  # 学習後のパラメーターの情報を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 学習結果（損失）のグラフを描画\n",
    "epochs = len(train_history)\n",
    "plt.plot(range(epochs), train_history, marker='.', label='loss (Training data)')\n",
    "plt.plot(range(epochs), valid_history, marker='.', label='loss (Validation data)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1,3-propanediolの電荷\n",
    "chags =[\n",
    "-0.597800,\n",
    " 0.128900,\n",
    "-0.158400,\n",
    " 0.128900,   \n",
    "-0.597800, \n",
    " 0.395500,      \n",
    " 0.050700,   \n",
    " 0.050700,     \n",
    " 0.051200,  \n",
    " 0.051200,  \n",
    " 0.050700,    \n",
    " 0.050700,    \n",
    " 0.395500]     \n",
    "\n",
    "#電荷素量、長さ、デバイの単位\n",
    "qe = 1.60217663e-19 #C \n",
    "debye = 3.33564e-30 #Cm \n",
    "ang = 1.0e-10 #Ang. Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#原子座標と電荷の値を用いて直接、UnitCellの双極子モーメントを計算する\n",
    "import numpy as np\n",
    "dx=0;dy=0;dz=0\n",
    "num_at = len(chags)\n",
    "for i,r in enumerate(mol1.get_positions()) :\n",
    "    dx += chags[i%num_at] * r[0] ; dy += chags[i%num_at] * r[1]; dz += chags[i%num_at] * r[2]\n",
    "\n",
    "print(\"Total < M_x > = {} Debye\".format(dx*ang*qe/debye))\n",
    "print(\"Total < M_y > = {} Debye\".format(dy*ang*qe/debye))\n",
    "print(\"Total < M_z > = {} Debye\".format(dz*ang*qe/debye))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "gromacs_home = \"~/usr/local/gromacs2018/bin/\"\n",
    "!echo \"System\" > anal.txt \n",
    "os.environ['OMP_NUM_THREADS'] = '1'    \n",
    "commands = gromacs_home+\"gmx_mpi dipoles -s eq.tpr -f eq.gro < anal.txt\"\n",
    "proc = subprocess.run(commands, shell=True, stdout=PIPE, stderr=PIPE,encoding='utf-8')\n",
    "output = proc.stdout\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【結論】  \n",
    "gromacsのdipolesでは、周期境界をはみ出した分子の部分について特にUnitCell内に戻すような処理を行わず、  \n",
    "はみ出した値のまま素朴に計算しているようである。  \n",
    "現在弊社の誘電スペクトルの計算ではgmx dipolesを特にオプションなど検討せずに使用しているが、  \n",
    "これで問題がないのかどうかについては改めて検討する。  \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "dipoles_gromacs_IR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
